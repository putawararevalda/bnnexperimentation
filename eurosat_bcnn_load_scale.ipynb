{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3284bd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import torch\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc35d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Revalda Putawara\\.conda\\envs\\bnntest\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c8d767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.autoguide import AutoDiagonalNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a7ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7fc6d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b592e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c330b6",
   "metadata": {},
   "source": [
    "## Defining Model and Loading Model Training Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a980a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02527b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianCNNSingleFC(PyroModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        prior_mu = 0.\n",
    "        #prior_sigma = 0.1 #accuracy 13.203704% 2 epochs\n",
    "        #prior_sigma = 1. #accuracy 31% 2 epochs\n",
    "        prior_sigma = torch.tensor(10., device=device) #accuracy 45% 10 epochs\n",
    "        #prior_sigma = 100 #accuracy 21% 10 epochs\n",
    "\n",
    "        self.conv1 = PyroModule[nn.Conv2d](3, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv1.weight = PyroSample(dist.Normal(prior_mu, prior_sigma).expand([32, 3, 5, 5]).to_event(4))\n",
    "        self.conv1.bias = PyroSample(dist.Normal(prior_mu, prior_sigma).expand([32]).to_event(1))\n",
    "\n",
    "        self.conv2 = PyroModule[nn.Conv2d](32, 64, kernel_size=5, stride=1, padding=2) #initially padding=1 kernel_size=3, without stride\n",
    "        self.conv2.weight = PyroSample(dist.Normal(prior_mu, prior_sigma).expand([64, 32, 5, 5]).to_event(4))\n",
    "        self.conv2.bias = PyroSample(dist.Normal(prior_mu, prior_sigma).expand([64]).to_event(1))\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = PyroModule[nn.Linear](64 * 16 * 16, num_classes)\n",
    "        self.fc1.weight = PyroSample(dist.Normal(prior_mu, prior_sigma).expand([num_classes, 64 * 16 * 16]).to_event(2))\n",
    "        self.fc1.bias = PyroSample(dist.Normal(prior_mu, prior_sigma).expand([num_classes]).to_event(1))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc1(x)\n",
    "        \n",
    "        # THIS IS THE MISSING PIECE: Define the likelihood\n",
    "        if y is not None:\n",
    "            with pyro.plate(\"data\", x.shape[0]):\n",
    "                pyro.sample(\"obs\", dist.Categorical(logits=logits), obs=y)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee23299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=54):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.3444, 0.3809, 0.4082], std=[0.1809, 0.1331, 0.1137])\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.EuroSAT(root='./data', transform=transform, download=False)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    with open('datasplit/split_indices.pkl', 'rb') as f:\n",
    "        split = pickle.load(f)\n",
    "        train_dataset = Subset(dataset, split['train'])\n",
    "        test_dataset = Subset(dataset, split['test'])\n",
    "\n",
    "    # Add num_workers and pin_memory for faster data loading\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                             num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                            num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4804393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "bayesian_model = BayesianCNNSingleFC(num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75ae6e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data_probs(model, test_loader, num_samples=10):\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_logits = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            logits_mc = torch.zeros(num_samples, images.size(0), model.fc1.out_features).to(device)\n",
    "\n",
    "            for i in range(num_samples):\n",
    "                guide_trace = pyro.poutine.trace(guide).get_trace(images)\n",
    "                replayed_model = pyro.poutine.replay(model, trace=guide_trace)\n",
    "                logits = replayed_model(images)\n",
    "                logits_mc[i] = logits\n",
    "\n",
    "            avg_logits = logits_mc.mean(dim=0)\n",
    "            predictions = torch.argmax(avg_logits, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_logits.extend(avg_logits.cpu().numpy())\n",
    "            all_probs.extend(F.softmax(avg_logits, dim=1).cpu().numpy())\n",
    "\n",
    "    return all_labels, all_predictions, all_logits, all_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3f2d9b",
   "metadata": {},
   "source": [
    "## Before Bitflip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b79df934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoDiagonalNormal.loc: torch.Size([217546])\n",
      "AutoDiagonalNormal.scale: torch.Size([217546])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revalda Putawara\\AppData\\Local\\Temp\\ipykernel_14524\\467444935.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  original_param_store[name] = torch.tensor(value.data, requires_grad=value.requires_grad)\n"
     ]
    }
   ],
   "source": [
    "model_path = 'results_eurosat/bayesian_cnn_model_std10_100_epoch.pth'\n",
    "guide_path = 'results_eurosat/bayesian_cnn_guide_std10_100_epoch_guide.pth'\n",
    "pyro_param_store_path = 'results_eurosat/pyro_param_store_std10_100_epoch.pkl'\n",
    "\n",
    "guide = AutoDiagonalNormal(bayesian_model).to(device)\n",
    "#guide.load_state_dict(torch.load(guide_path))\n",
    "\n",
    "pyro.get_param_store().set_state(torch.load(pyro_param_store_path,weights_only=False))\n",
    "\n",
    "original_param_store = {}\n",
    "\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print(f\"{name}: {value.shape}\")\n",
    "    original_param_store[name] = torch.tensor(value.data, requires_grad=value.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1631104a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = load_data(batch_size=54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5cca6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoDiagonalNormal.loc: torch.Size([217546])\n",
      "Parameter containing:\n",
      "tensor([ 3.1483, -2.4763, -1.0711,  ..., -2.4452,  4.6454,  1.5156],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "AutoDiagonalNormal.scale: torch.Size([217546])\n",
      "tensor([0.0454, 0.0385, 0.0440,  ..., 7.7091, 6.1614, 6.6950], device='cuda:0',\n",
      "       grad_fn=<SoftplusBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(f\"{name}: {value.shape}\")\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "834d68d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:47<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "all_labels, all_predictions, all_logits, all_probs = predict_data_probs(bayesian_model, test_loader, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a002d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6522a2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from confusion matrix: 74.907407%\n"
     ]
    }
   ],
   "source": [
    "#print accuracy from confusion matrix\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(f\"Accuracy from confusion matrix: {accuracy * 100:.6f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26ad8a",
   "metadata": {},
   "source": [
    "## Check parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35727787",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_param_store[\"AutoDiagonalNormal.loc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50594b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_param_store[\"AutoDiagonalNormal.scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca59d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(f\"{name}: {value.shape}\")\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9a32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "guide.loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a73710",
   "metadata": {},
   "outputs": [],
   "source": [
    "guide.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f054af",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = guide.get_posterior()\n",
    "print(\"Before:\", posterior.stddev[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d802f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af6740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior.stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4698ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def inv_softplus(x):\n",
    "    return torch.log(torch.exp(x) - 1.0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    scale_param = pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")\n",
    "    scale_param.data[-1] = inv_softplus(torch.tensor(88.7225, device=scale_param.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ea25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a055f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "guide.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the approach that works (from your earlier code)\n",
    "#with torch.no_grad():\n",
    "#    scale_param = pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")\n",
    "#    scale_param.data[0] = inv_softplus(torch.tensor(100.0, device=scale_param.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17bfe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59874e23",
   "metadata": {},
   "source": [
    "## Bitflip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee504020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitflip import bitflip_float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store()[\"AutoDiagonalNormal.loc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f42f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store()[\"AutoDiagonalNormal.scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a295c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_store = pyro.get_param_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3ee223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Parameter in module torch.nn.parameter object:\n",
      "\n",
      "class Parameter(torch.Tensor)\n",
      " |  Parameter(data=None, requires_grad=True)\n",
      " |\n",
      " |  A kind of Tensor that is to be considered a module parameter.\n",
      " |\n",
      " |  Parameters are :class:`~torch.Tensor` subclasses, that have a\n",
      " |  very special property when used with :class:`Module` s - when they're\n",
      " |  assigned as Module attributes they are automatically added to the list of\n",
      " |  its parameters, and will appear e.g. in :meth:`~Module.parameters` iterator.\n",
      " |  Assigning a Tensor doesn't have such effect. This is because one might\n",
      " |  want to cache some temporary state, like last hidden state of the RNN, in\n",
      " |  the model. If there was no such class as :class:`Parameter`, these\n",
      " |  temporaries would get registered too.\n",
      " |\n",
      " |  Args:\n",
      " |      data (Tensor): parameter tensor.\n",
      " |      requires_grad (bool, optional): if the parameter requires gradient. Note that\n",
      " |          the torch.no_grad() context does NOT affect the default behavior of\n",
      " |          Parameter creation--the Parameter will still have `requires_grad=True` in\n",
      " |          :class:`~no_grad` mode. See :ref:`locally-disable-grad-doc` for more\n",
      " |          details. Default: `True`\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      Parameter\n",
      " |      torch.Tensor\n",
      " |      torch._C.TensorBase\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __deepcopy__(self, memo)\n",
      " |      # Note: the 3 methods below only apply to standard Tensor. Parameters of custom tensor types\n",
      " |      # are still considered that custom tensor type and these methods will not be called for them.\n",
      " |\n",
      " |  __reduce_ex__(self, proto)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |\n",
      " |  __new__(cls, data=None, requires_grad=True)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |\n",
      " |  __torch_function__ = _disabled_torch_function_impl(...)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.Tensor:\n",
      " |\n",
      " |  __abs__ = abs(...)\n",
      " |\n",
      " |  __array__(self, dtype=None) from torch._tensor.Tensor\n",
      " |\n",
      " |  __array_wrap__(self, array) from torch._tensor.Tensor\n",
      " |      # Wrap Numpy array again in a suitable tensor when done, to support e.g.\n",
      " |      # `numpy.sin(tensor) -> tensor` or `numpy.greater(tensor, 0) -> ByteTensor`\n",
      " |\n",
      " |  __contains__(self, element: Any, /) -> bool from torch._tensor.Tensor\n",
      " |      Check if `element` is present in tensor\n",
      " |\n",
      " |      Args:\n",
      " |          element (Tensor or scalar): element to be checked\n",
      " |              for presence in current tensor\"\n",
      " |\n",
      " |  __dir__(self) from torch._tensor.Tensor\n",
      " |      Default dir() implementation.\n",
      " |\n",
      " |  __dlpack__(self, stream=None) from torch._tensor.Tensor\n",
      " |      Creates a DLpack `capsule https://data-apis.org/array-api/latest/design_topics/data_interchange.html#data-interchange`_\n",
      " |      of the current tensor to be exported to other libraries.\n",
      " |\n",
      " |      This function will be called from the `from_dlpack` method\n",
      " |      of the library that will consume the capsule. `from_dlpack` passes the current\n",
      " |      stream to this method as part of the specification.\n",
      " |\n",
      " |      Args:\n",
      " |          stream (integer or None): An optional Python integer representing a\n",
      " |          pointer to a CUDA stream. The current stream is synchronized with\n",
      " |          this stream before the capsule is created, and since the capsule\n",
      " |          shares its storage with the tensor this make it safe to access from\n",
      " |          both streams.  If None or -1 is passed then no synchronization is performed.\n",
      " |          If 1 (on CUDA) or 0 (on ROCM) then the default stream is used for\n",
      " |          synchronization.\n",
      " |\n",
      " |  __dlpack_device__(self) -> tuple[enum.IntEnum, int] from torch._tensor.Tensor\n",
      " |\n",
      " |  __floordiv__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __format__(self, format_spec) from torch._tensor.Tensor\n",
      " |      Default object formatter.\n",
      " |\n",
      " |      Return str(self) if format_spec is empty. Raise TypeError otherwise.\n",
      " |\n",
      " |  __hash__(self) from torch._tensor.Tensor\n",
      " |      Return hash(self).\n",
      " |\n",
      " |  __ipow__ = pow_(...) from torch._tensor.TensorBase\n",
      " |\n",
      " |  __iter__(self) from torch._tensor.Tensor\n",
      " |\n",
      " |  __itruediv__ = __idiv__(...)\n",
      " |\n",
      " |  __len__(self) from torch._tensor.Tensor\n",
      " |      Return len(self).\n",
      " |\n",
      " |  __neg__ = neg(...)\n",
      " |\n",
      " |  __pos__ = positive(...)\n",
      " |\n",
      " |  __pow__ = pow(...) from torch._tensor.TensorBase\n",
      " |\n",
      " |  __rdiv__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __reversed__(self) from torch._tensor.Tensor\n",
      " |      Reverses the tensor along dimension 0.\n",
      " |\n",
      " |  __rfloordiv__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __rlshift__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __rmatmul__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __rmod__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __rpow__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __rrshift__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __rsub__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __rtruediv__ = __rdiv__(self, other)\n",
      " |\n",
      " |  __setstate__(self, state) from torch._tensor.Tensor\n",
      " |\n",
      " |  align_to(self, *names) from torch._tensor.Tensor\n",
      " |      Permutes the dimensions of the :attr:`self` tensor to match the order\n",
      " |      specified in :attr:`names`, adding size-one dims for any new names.\n",
      " |\n",
      " |      All of the dims of :attr:`self` must be named in order to use this method.\n",
      " |      The resulting tensor is a view on the original tensor.\n",
      " |\n",
      " |      All dimension names of :attr:`self` must be present in :attr:`names`.\n",
      " |      :attr:`names` may contain additional names that are not in ``self.names``;\n",
      " |      the output tensor has a size-one dimension for each of those new names.\n",
      " |\n",
      " |      :attr:`names` may contain up to one Ellipsis (``...``).\n",
      " |      The Ellipsis is expanded to be equal to all dimension names of :attr:`self`\n",
      " |      that are not mentioned in :attr:`names`, in the order that they appear\n",
      " |      in :attr:`self`.\n",
      " |\n",
      " |      Python 2 does not support Ellipsis but one may use a string literal\n",
      " |      instead (``'...'``).\n",
      " |\n",
      " |      Args:\n",
      " |          names (iterable of str): The desired dimension ordering of the\n",
      " |              output tensor. May contain up to one Ellipsis that is expanded\n",
      " |              to all unmentioned dim names of :attr:`self`.\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |          >>> tensor = torch.randn(2, 2, 2, 2, 2, 2)\n",
      " |          >>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')\n",
      " |\n",
      " |          # Move the F and E dims to the front while keeping the rest in order\n",
      " |          >>> named_tensor.align_to('F', 'E', ...)\n",
      " |\n",
      " |      .. warning::\n",
      " |          The named tensor API is experimental and subject to change.\n",
      " |\n",
      " |  backward(self, gradient=None, retain_graph=None, create_graph=False, inputs=None) from torch._tensor.Tensor\n",
      " |      Computes the gradient of current tensor wrt graph leaves.\n",
      " |\n",
      " |      The graph is differentiated using the chain rule. If the tensor is\n",
      " |      non-scalar (i.e. its data has more than one element) and requires\n",
      " |      gradient, the function additionally requires specifying a ``gradient``.\n",
      " |      It should be a tensor of matching type and shape, that represents\n",
      " |      the gradient of the differentiated function w.r.t. ``self``.\n",
      " |\n",
      " |      This function accumulates gradients in the leaves - you might need to zero\n",
      " |      ``.grad`` attributes or set them to ``None`` before calling it.\n",
      " |      See :ref:`Default gradient layouts<default-grad-layouts>`\n",
      " |      for details on the memory layout of accumulated gradients.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          If you run any forward ops, create ``gradient``, and/or call ``backward``\n",
      " |          in a user-specified CUDA stream context, see\n",
      " |          :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          When ``inputs`` are provided and a given input is not a leaf,\n",
      " |          the current implementation will call its grad_fn (though it is not strictly needed to get this gradients).\n",
      " |          It is an implementation detail on which the user should not rely.\n",
      " |          See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.\n",
      " |\n",
      " |      Args:\n",
      " |          gradient (Tensor, optional): The gradient of the function\n",
      " |              being differentiated w.r.t. ``self``.\n",
      " |              This argument can be omitted if ``self`` is a scalar.\n",
      " |          retain_graph (bool, optional): If ``False``, the graph used to compute\n",
      " |              the grads will be freed. Note that in nearly all cases setting\n",
      " |              this option to True is not needed and often can be worked around\n",
      " |              in a much more efficient way. Defaults to the value of\n",
      " |              ``create_graph``.\n",
      " |          create_graph (bool, optional): If ``True``, graph of the derivative will\n",
      " |              be constructed, allowing to compute higher order derivative\n",
      " |              products. Defaults to ``False``.\n",
      " |          inputs (sequence of Tensor, optional): Inputs w.r.t. which the gradient will be\n",
      " |              accumulated into ``.grad``. All other tensors will be ignored. If not\n",
      " |              provided, the gradient is accumulated into all the leaf Tensors that were\n",
      " |              used to compute the :attr:`tensors`.\n",
      " |\n",
      " |  detach(...) from torch._C.TensorBase\n",
      " |      Returns a new Tensor, detached from the current graph.\n",
      " |\n",
      " |      The result will never require gradient.\n",
      " |\n",
      " |      This method also affects forward mode AD gradients and the result will never\n",
      " |      have forward mode AD gradients.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |        Returned Tensor shares the same storage with the original one.\n",
      " |        In-place modifications on either of them will be seen, and may trigger\n",
      " |        errors in correctness checks.\n",
      " |\n",
      " |  detach_(...) from torch._C.TensorBase\n",
      " |      Detaches the Tensor from the graph that created it, making it a leaf.\n",
      " |      Views cannot be detached in-place.\n",
      " |\n",
      " |      This method also affects forward mode AD gradients and the result will never\n",
      " |      have forward mode AD gradients.\n",
      " |\n",
      " |  dim_order(self, *, ambiguity_check: Union[bool, list[torch.memory_format]] = False) from torch._tensor.Tensor\n",
      " |      dim_order(ambiguity_check=False) -> tuple\n",
      " |\n",
      " |      Returns the uniquely determined tuple of int describing the dim order or\n",
      " |      physical layout of :attr:`self`.\n",
      " |\n",
      " |      The dim order represents how dimensions are laid out in memory of dense tensors,\n",
      " |      starting from the outermost to the innermost dimension.\n",
      " |\n",
      " |      Note that the dim order may not always be uniquely determined.\n",
      " |      If `ambiguity_check` is True, this function raises a RuntimeError when the dim order cannot be uniquely determined;\n",
      " |      If `ambiguity_check` is a list of memory formats, this function raises a RuntimeError when tensor can not be interpreted\n",
      " |      into exactly one of the given memory formats, or it cannot be uniquely determined.\n",
      " |      If `ambiguity_check` is False, it will return one of legal dim order(s) without checking its uniqueness.\n",
      " |      Otherwise, it will raise TypeError.\n",
      " |\n",
      " |      Args:\n",
      " |          ambiguity_check (bool or List[torch.memory_format]): The check method for ambiguity of dim order.\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |          >>> torch.empty((2, 3, 5, 7)).dim_order()\n",
      " |          (0, 1, 2, 3)\n",
      " |          >>> torch.empty((2, 3, 5, 7)).transpose(1, 2).dim_order()\n",
      " |          (0, 2, 1, 3)\n",
      " |          >>> torch.empty((2, 3, 5, 7), memory_format=torch.channels_last).dim_order()\n",
      " |          (0, 2, 3, 1)\n",
      " |          >>> torch.empty((1, 2, 3, 4)).dim_order()\n",
      " |          (0, 1, 2, 3)\n",
      " |          >>> try:\n",
      " |          ...     torch.empty((1, 2, 3, 4)).dim_order(ambiguity_check=True)\n",
      " |          ... except RuntimeError as e:\n",
      " |          ...     print(e)\n",
      " |          The tensor does not have unique dim order, or cannot map to exact one of the given memory formats.\n",
      " |          >>> torch.empty((1, 2, 3, 4)).dim_order(\n",
      " |          ...     ambiguity_check=[torch.contiguous_format, torch.channels_last]\n",
      " |          ... )  # It can be mapped to contiguous format\n",
      " |          (0, 1, 2, 3)\n",
      " |          >>> try:\n",
      " |          ...     torch.empty((1, 2, 3, 4)).dim_order(ambiguity_check=\"ILLEGAL\")\n",
      " |          ... except TypeError as e:\n",
      " |          ...     print(e)\n",
      " |          The ambiguity_check argument must be a bool or a list of memory formats.\n",
      " |\n",
      " |      .. warning::\n",
      " |          The dim_order tensor API is experimental and subject to change.\n",
      " |\n",
      " |  eig(self, eigenvectors=False) from torch._tensor.Tensor\n",
      " |\n",
      " |  is_shared(self) from torch._tensor.Tensor\n",
      " |      Checks if tensor is in shared memory.\n",
      " |\n",
      " |      This is always ``True`` for CUDA tensors.\n",
      " |\n",
      " |  istft(self, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: 'Optional[Tensor]' = None, center: bool = True, normalized: bool = False, onesided: Optional[bool] = None, length: Optional[int] = None, return_complex: bool = False) from torch._tensor.Tensor\n",
      " |      See :func:`torch.istft`\n",
      " |\n",
      " |  lstsq(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  lu(self, pivot=True, get_infos=False) from torch._tensor.Tensor\n",
      " |      See :func:`torch.lu`\n",
      " |\n",
      " |  module_load(self, other, assign=False) from torch._tensor.Tensor\n",
      " |      Defines how to transform ``other`` when loading it into ``self`` in :meth:`~nn.Module.load_state_dict`.\n",
      " |\n",
      " |      Used when :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\n",
      " |\n",
      " |      It is expected that ``self`` is a parameter or buffer in an ``nn.Module`` and ``other`` is the\n",
      " |      value in the state dictionary with the corresponding key, this method defines\n",
      " |      how ``other`` is remapped before being swapped with ``self`` via\n",
      " |      :func:`~torch.utils.swap_tensors` in :meth:`~nn.Module.load_state_dict`.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method should always return a new object that is not ``self`` or ``other``.\n",
      " |          For example, the default implementation returns ``self.copy_(other).detach()``\n",
      " |          if ``assign`` is ``False`` or ``other.detach()`` if ``assign`` is ``True``.\n",
      " |\n",
      " |      Args:\n",
      " |          other (Tensor): value in state dict with key corresponding to ``self``\n",
      " |          assign (bool): the assign argument passed to :meth:`nn.Module.load_state_dict`\n",
      " |\n",
      " |  norm(self, p: Union[float, str, NoneType] = 'fro', dim=None, keepdim=False, dtype=None) from torch._tensor.Tensor\n",
      " |      See :func:`torch.norm`\n",
      " |\n",
      " |  refine_names(self, *names) from torch._tensor.Tensor\n",
      " |      Refines the dimension names of :attr:`self` according to :attr:`names`.\n",
      " |\n",
      " |      Refining is a special case of renaming that \"lifts\" unnamed dimensions.\n",
      " |      A ``None`` dim can be refined to have any name; a named dim can only be\n",
      " |      refined to have the same name.\n",
      " |\n",
      " |      Because named tensors can coexist with unnamed tensors, refining names\n",
      " |      gives a nice way to write named-tensor-aware code that works with both\n",
      " |      named and unnamed tensors.\n",
      " |\n",
      " |      :attr:`names` may contain up to one Ellipsis (``...``).\n",
      " |      The Ellipsis is expanded greedily; it is expanded in-place to fill\n",
      " |      :attr:`names` to the same length as ``self.dim()`` using names from the\n",
      " |      corresponding indices of ``self.names``.\n",
      " |\n",
      " |      Python 2 does not support Ellipsis but one may use a string literal\n",
      " |      instead (``'...'``).\n",
      " |\n",
      " |      Args:\n",
      " |          names (iterable of str): The desired names of the output tensor. May\n",
      " |              contain up to one Ellipsis.\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |          >>> imgs = torch.randn(32, 3, 128, 128)\n",
      " |          >>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')\n",
      " |          >>> named_imgs.names\n",
      " |          ('N', 'C', 'H', 'W')\n",
      " |\n",
      " |          >>> tensor = torch.randn(2, 3, 5, 7, 11)\n",
      " |          >>> tensor = tensor.refine_names('A', ..., 'B', 'C')\n",
      " |          >>> tensor.names\n",
      " |          ('A', None, None, 'B', 'C')\n",
      " |\n",
      " |      .. warning::\n",
      " |          The named tensor API is experimental and subject to change.\n",
      " |\n",
      " |  register_hook(self, hook) from torch._tensor.Tensor\n",
      " |      Registers a backward hook.\n",
      " |\n",
      " |      The hook will be called every time a gradient with respect to the\n",
      " |      Tensor is computed. The hook should have the following signature::\n",
      " |\n",
      " |          hook(grad) -> Tensor or None\n",
      " |\n",
      " |\n",
      " |      The hook should not modify its argument, but it can optionally return\n",
      " |      a new gradient which will be used in place of :attr:`grad`.\n",
      " |\n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |\n",
      " |      .. note::\n",
      " |          See :ref:`backward-hooks-execution` for more information on how when this hook\n",
      " |          is executed, and how its execution is ordered relative to other hooks.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> v = torch.tensor([0., 0., 0.], requires_grad=True)\n",
      " |          >>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n",
      " |          >>> v.backward(torch.tensor([1., 2., 3.]))\n",
      " |          >>> v.grad\n",
      " |\n",
      " |           2\n",
      " |           4\n",
      " |           6\n",
      " |          [torch.FloatTensor of size (3,)]\n",
      " |\n",
      " |          >>> h.remove()  # removes the hook\n",
      " |\n",
      " |  register_post_accumulate_grad_hook(self, hook) from torch._tensor.Tensor\n",
      " |      Registers a backward hook that runs after grad accumulation.\n",
      " |\n",
      " |      The hook will be called after all gradients for a tensor have been accumulated,\n",
      " |      meaning that the .grad field has been updated on that tensor. The post\n",
      " |      accumulate grad hook is ONLY applicable for leaf tensors (tensors without a\n",
      " |      .grad_fn field). Registering this hook on a non-leaf tensor will error!\n",
      " |\n",
      " |      The hook should have the following signature::\n",
      " |\n",
      " |          hook(param: Tensor) -> None\n",
      " |\n",
      " |      Note that, unlike other autograd hooks, this hook operates on the tensor\n",
      " |      that requires grad and not the grad itself. The hook can in-place modify\n",
      " |      and access its Tensor argument, including its .grad field.\n",
      " |\n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |\n",
      " |      .. note::\n",
      " |          See :ref:`backward-hooks-execution` for more information on how when this hook\n",
      " |          is executed, and how its execution is ordered relative to other hooks. Since\n",
      " |          this hook runs during the backward pass, it will run in no_grad mode (unless\n",
      " |          create_graph is True). You can use torch.enable_grad() to re-enable autograd\n",
      " |          within the hook if you need it.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> v = torch.tensor([0., 0., 0.], requires_grad=True)\n",
      " |          >>> lr = 0.01\n",
      " |          >>> # simulate a simple SGD update\n",
      " |          >>> h = v.register_post_accumulate_grad_hook(lambda p: p.add_(p.grad, alpha=-lr))\n",
      " |          >>> v.backward(torch.tensor([1., 2., 3.]))\n",
      " |          >>> v\n",
      " |          tensor([-0.0100, -0.0200, -0.0300], requires_grad=True)\n",
      " |\n",
      " |          >>> h.remove()  # removes the hook\n",
      " |\n",
      " |  reinforce(self, reward) from torch._tensor.Tensor\n",
      " |\n",
      " |  rename(self, *names, **rename_map) from torch._tensor.Tensor\n",
      " |      Renames dimension names of :attr:`self`.\n",
      " |\n",
      " |      There are two main usages:\n",
      " |\n",
      " |      ``self.rename(**rename_map)`` returns a view on tensor that has dims\n",
      " |      renamed as specified in the mapping :attr:`rename_map`.\n",
      " |\n",
      " |      ``self.rename(*names)`` returns a view on tensor, renaming all\n",
      " |      dimensions positionally using :attr:`names`.\n",
      " |      Use ``self.rename(None)`` to drop names on a tensor.\n",
      " |\n",
      " |      One cannot specify both positional args :attr:`names` and keyword args\n",
      " |      :attr:`rename_map`.\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |          >>> imgs = torch.rand(2, 3, 5, 7, names=('N', 'C', 'H', 'W'))\n",
      " |          >>> renamed_imgs = imgs.rename(N='batch', C='channels')\n",
      " |          >>> renamed_imgs.names\n",
      " |          ('batch', 'channels', 'H', 'W')\n",
      " |\n",
      " |          >>> renamed_imgs = imgs.rename(None)\n",
      " |          >>> renamed_imgs.names\n",
      " |          (None, None, None, None)\n",
      " |\n",
      " |          >>> renamed_imgs = imgs.rename('batch', 'channel', 'height', 'width')\n",
      " |          >>> renamed_imgs.names\n",
      " |          ('batch', 'channel', 'height', 'width')\n",
      " |\n",
      " |      .. warning::\n",
      " |          The named tensor API is experimental and subject to change.\n",
      " |\n",
      " |  rename_(self, *names, **rename_map) from torch._tensor.Tensor\n",
      " |      In-place version of :meth:`~Tensor.rename`.\n",
      " |\n",
      " |  resize(self, *sizes) from torch._tensor.Tensor\n",
      " |\n",
      " |  resize_as(self, tensor) from torch._tensor.Tensor\n",
      " |\n",
      " |  share_memory_(self) from torch._tensor.Tensor\n",
      " |      Moves the underlying storage to shared memory.\n",
      " |\n",
      " |      This is a no-op if the underlying storage is already in shared memory\n",
      " |      and for CUDA tensors. Tensors in shared memory cannot be resized.\n",
      " |\n",
      " |      See :meth:`torch.UntypedStorage.share_memory_` for more details.\n",
      " |\n",
      " |  solve(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  split(self, split_size, dim=0) from torch._tensor.Tensor\n",
      " |      See :func:`torch.split`\n",
      " |\n",
      " |  stft(self, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: 'Optional[Tensor]' = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: Optional[bool] = None, return_complex: Optional[bool] = None, align_to_window: Optional[bool] = None) from torch._tensor.Tensor\n",
      " |      See :func:`torch.stft`\n",
      " |\n",
      " |      .. warning::\n",
      " |        This function changed signature at version 0.4.1. Calling with\n",
      " |        the previous signature may cause error or return incorrect result.\n",
      " |\n",
      " |  storage(self) from torch._tensor.Tensor\n",
      " |      storage() -> torch.TypedStorage\n",
      " |\n",
      " |      Returns the underlying :class:`TypedStorage`.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          :class:`TypedStorage` is deprecated. It will be removed in the future, and\n",
      " |          :class:`UntypedStorage` will be the only storage class. To access the\n",
      " |          :class:`UntypedStorage` directly, use :attr:`Tensor.untyped_storage()`.\n",
      " |\n",
      " |  storage_type(self) from torch._tensor.Tensor\n",
      " |      storage_type() -> type\n",
      " |\n",
      " |      Returns the type of the underlying storage.\n",
      " |\n",
      " |  symeig(self, eigenvectors=False) from torch._tensor.Tensor\n",
      " |\n",
      " |  to_sparse_coo(self) from torch._tensor.Tensor\n",
      " |      Convert a tensor to :ref:`coordinate format <sparse-coo-docs>`.\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |           >>> dense = torch.randn(5, 5)\n",
      " |           >>> sparse = dense.to_sparse_coo()\n",
      " |           >>> sparse._nnz()\n",
      " |           25\n",
      " |\n",
      " |  unflatten(self, dim, sizes) from torch._tensor.Tensor\n",
      " |      unflatten(dim, sizes) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.unflatten`.\n",
      " |\n",
      " |  unique(self, sorted=True, return_inverse=False, return_counts=False, dim=None) from torch._tensor.Tensor\n",
      " |      Returns the unique elements of the input tensor.\n",
      " |\n",
      " |      See :func:`torch.unique`\n",
      " |\n",
      " |  unique_consecutive(self, return_inverse=False, return_counts=False, dim=None) from torch._tensor.Tensor\n",
      " |      Eliminates all but the first element from every consecutive group of equivalent elements.\n",
      " |\n",
      " |      See :func:`torch.unique_consecutive`\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from torch.Tensor:\n",
      " |\n",
      " |  __torch_dispatch__ = _disabled_torch_dispatch_impl(...)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from torch.Tensor:\n",
      " |\n",
      " |  __cuda_array_interface__\n",
      " |      Array view description for cuda tensors.\n",
      " |\n",
      " |      See:\n",
      " |      https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.Tensor:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.Tensor:\n",
      " |\n",
      " |  __array_priority__ = 1000\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch._C.TensorBase:\n",
      " |\n",
      " |  __add__(...)\n",
      " |\n",
      " |  __and__(...)\n",
      " |\n",
      " |  __bool__(...)\n",
      " |\n",
      " |  __complex__(...)\n",
      " |\n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |\n",
      " |  __div__(...)\n",
      " |\n",
      " |  __eq__(...)\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __float__(...)\n",
      " |\n",
      " |  __ge__(...)\n",
      " |      Return self>=value.\n",
      " |\n",
      " |  __getitem__(self, key, /)\n",
      " |      Return self[key].\n",
      " |\n",
      " |  __gt__(...)\n",
      " |      Return self>value.\n",
      " |\n",
      " |  __iadd__(...)\n",
      " |\n",
      " |  __iand__(...)\n",
      " |\n",
      " |  __idiv__(...)\n",
      " |\n",
      " |  __ifloordiv__(...)\n",
      " |\n",
      " |  __ilshift__(...)\n",
      " |\n",
      " |  __imod__(...)\n",
      " |\n",
      " |  __imul__(...)\n",
      " |\n",
      " |  __index__(...)\n",
      " |\n",
      " |  __int__(...)\n",
      " |\n",
      " |  __invert__(...)\n",
      " |\n",
      " |  __ior__(...)\n",
      " |\n",
      " |  __irshift__(...)\n",
      " |\n",
      " |  __isub__(...)\n",
      " |\n",
      " |  __ixor__(...)\n",
      " |\n",
      " |  __le__(...)\n",
      " |      Return self<=value.\n",
      " |\n",
      " |  __long__(...)\n",
      " |\n",
      " |  __lshift__(...)\n",
      " |\n",
      " |  __lt__(...)\n",
      " |      Return self<value.\n",
      " |\n",
      " |  __matmul__(...)\n",
      " |\n",
      " |  __mod__(...)\n",
      " |\n",
      " |  __mul__(...)\n",
      " |\n",
      " |  __ne__(...)\n",
      " |      Return self!=value.\n",
      " |\n",
      " |  __nonzero__(...)\n",
      " |\n",
      " |  __or__(...)\n",
      " |      Return self|value.\n",
      " |\n",
      " |  __radd__(...)\n",
      " |\n",
      " |  __rand__(...)\n",
      " |\n",
      " |  __rmul__(...)\n",
      " |\n",
      " |  __ror__(...)\n",
      " |      Return value|self.\n",
      " |\n",
      " |  __rshift__(...)\n",
      " |\n",
      " |  __rxor__(...)\n",
      " |\n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |\n",
      " |  __sub__(...)\n",
      " |\n",
      " |  __truediv__(...)\n",
      " |\n",
      " |  __xor__(...)\n",
      " |\n",
      " |  abs(...)\n",
      " |      abs() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.abs`\n",
      " |\n",
      " |  abs_(...)\n",
      " |      abs_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.abs`\n",
      " |\n",
      " |  absolute(...)\n",
      " |      absolute() -> Tensor\n",
      " |\n",
      " |      Alias for :func:`abs`\n",
      " |\n",
      " |  absolute_(...)\n",
      " |      absolute_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.absolute`\n",
      " |      Alias for :func:`abs_`\n",
      " |\n",
      " |  acos(...)\n",
      " |      acos() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.acos`\n",
      " |\n",
      " |  acos_(...)\n",
      " |      acos_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.acos`\n",
      " |\n",
      " |  acosh(...)\n",
      " |      acosh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.acosh`\n",
      " |\n",
      " |  acosh_(...)\n",
      " |      acosh_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.acosh`\n",
      " |\n",
      " |  add(...)\n",
      " |      add(other, *, alpha=1) -> Tensor\n",
      " |\n",
      " |      Add a scalar or tensor to :attr:`self` tensor. If both :attr:`alpha`\n",
      " |      and :attr:`other` are specified, each element of :attr:`other` is scaled by\n",
      " |      :attr:`alpha` before being used.\n",
      " |\n",
      " |      When :attr:`other` is a tensor, the shape of :attr:`other` must be\n",
      " |      :ref:`broadcastable <broadcasting-semantics>` with the shape of the underlying\n",
      " |      tensor\n",
      " |\n",
      " |      See :func:`torch.add`\n",
      " |\n",
      " |  add_(...)\n",
      " |      add_(other, *, alpha=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.add`\n",
      " |\n",
      " |  addbmm(...)\n",
      " |      addbmm(batch1, batch2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.addbmm`\n",
      " |\n",
      " |  addbmm_(...)\n",
      " |      addbmm_(batch1, batch2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.addbmm`\n",
      " |\n",
      " |  addcdiv(...)\n",
      " |      addcdiv(tensor1, tensor2, *, value=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.addcdiv`\n",
      " |\n",
      " |  addcdiv_(...)\n",
      " |      addcdiv_(tensor1, tensor2, *, value=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.addcdiv`\n",
      " |\n",
      " |  addcmul(...)\n",
      " |      addcmul(tensor1, tensor2, *, value=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.addcmul`\n",
      " |\n",
      " |  addcmul_(...)\n",
      " |      addcmul_(tensor1, tensor2, *, value=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.addcmul`\n",
      " |\n",
      " |  addmm(...)\n",
      " |      addmm(mat1, mat2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.addmm`\n",
      " |\n",
      " |  addmm_(...)\n",
      " |      addmm_(mat1, mat2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.addmm`\n",
      " |\n",
      " |  addmv(...)\n",
      " |      addmv(mat, vec, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.addmv`\n",
      " |\n",
      " |  addmv_(...)\n",
      " |      addmv_(mat, vec, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.addmv`\n",
      " |\n",
      " |  addr(...)\n",
      " |      addr(vec1, vec2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.addr`\n",
      " |\n",
      " |  addr_(...)\n",
      " |      addr_(vec1, vec2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.addr`\n",
      " |\n",
      " |  adjoint(...)\n",
      " |      adjoint() -> Tensor\n",
      " |\n",
      " |      Alias for :func:`adjoint`\n",
      " |\n",
      " |  align_as(...)\n",
      " |      align_as(other) -> Tensor\n",
      " |\n",
      " |      Permutes the dimensions of the :attr:`self` tensor to match the dimension order\n",
      " |      in the :attr:`other` tensor, adding size-one dims for any new names.\n",
      " |\n",
      " |      This operation is useful for explicit broadcasting by names (see examples).\n",
      " |\n",
      " |      All of the dims of :attr:`self` must be named in order to use this method.\n",
      " |      The resulting tensor is a view on the original tensor.\n",
      " |\n",
      " |      All dimension names of :attr:`self` must be present in ``other.names``.\n",
      " |      :attr:`other` may contain named dimensions that are not in ``self.names``;\n",
      " |      the output tensor has a size-one dimension for each of those new names.\n",
      " |\n",
      " |      To align a tensor to a specific order, use :meth:`~Tensor.align_to`.\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |          # Example 1: Applying a mask\n",
      " |          >>> mask = torch.randint(2, [127, 128], dtype=torch.bool).refine_names('W', 'H')\n",
      " |          >>> imgs = torch.randn(32, 128, 127, 3, names=('N', 'H', 'W', 'C'))\n",
      " |          >>> imgs.masked_fill_(mask.align_as(imgs), 0)\n",
      " |\n",
      " |\n",
      " |          # Example 2: Applying a per-channel-scale\n",
      " |          >>> def scale_channels(input, scale):\n",
      " |          >>>    scale = scale.refine_names('C')\n",
      " |          >>>    return input * scale.align_as(input)\n",
      " |\n",
      " |          >>> num_channels = 3\n",
      " |          >>> scale = torch.randn(num_channels, names=('C',))\n",
      " |          >>> imgs = torch.rand(32, 128, 128, num_channels, names=('N', 'H', 'W', 'C'))\n",
      " |          >>> more_imgs = torch.rand(32, num_channels, 128, 128, names=('N', 'C', 'H', 'W'))\n",
      " |          >>> videos = torch.randn(3, num_channels, 128, 128, 128, names=('N', 'C', 'H', 'W', 'D'))\n",
      " |\n",
      " |          # scale_channels is agnostic to the dimension order of the input\n",
      " |          >>> scale_channels(imgs, scale)\n",
      " |          >>> scale_channels(more_imgs, scale)\n",
      " |          >>> scale_channels(videos, scale)\n",
      " |\n",
      " |      .. warning::\n",
      " |          The named tensor API is experimental and subject to change.\n",
      " |\n",
      " |  all(...)\n",
      " |      all(dim=None, keepdim=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.all`\n",
      " |\n",
      " |  allclose(...)\n",
      " |      allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.allclose`\n",
      " |\n",
      " |  amax(...)\n",
      " |      amax(dim=None, keepdim=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.amax`\n",
      " |\n",
      " |  amin(...)\n",
      " |      amin(dim=None, keepdim=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.amin`\n",
      " |\n",
      " |  aminmax(...)\n",
      " |      aminmax(*, dim=None, keepdim=False) -> (Tensor min, Tensor max)\n",
      " |\n",
      " |      See :func:`torch.aminmax`\n",
      " |\n",
      " |  angle(...)\n",
      " |      angle() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.angle`\n",
      " |\n",
      " |  any(...)\n",
      " |      any(dim=None, keepdim=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.any`\n",
      " |\n",
      " |  apply_(...)\n",
      " |      apply_(callable) -> Tensor\n",
      " |\n",
      " |      Applies the function :attr:`callable` to each element in the tensor, replacing\n",
      " |      each element with the value returned by :attr:`callable`.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          This function only works with CPU tensors and should not be used in code\n",
      " |          sections that require high performance.\n",
      " |\n",
      " |  arccos(...)\n",
      " |      arccos() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.arccos`\n",
      " |\n",
      " |  arccos_(...)\n",
      " |      arccos_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.arccos`\n",
      " |\n",
      " |  arccosh(...)\n",
      " |      acosh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.arccosh`\n",
      " |\n",
      " |  arccosh_(...)\n",
      " |      acosh_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.arccosh`\n",
      " |\n",
      " |  arcsin(...)\n",
      " |      arcsin() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.arcsin`\n",
      " |\n",
      " |  arcsin_(...)\n",
      " |      arcsin_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.arcsin`\n",
      " |\n",
      " |  arcsinh(...)\n",
      " |      arcsinh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.arcsinh`\n",
      " |\n",
      " |  arcsinh_(...)\n",
      " |      arcsinh_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.arcsinh`\n",
      " |\n",
      " |  arctan(...)\n",
      " |      arctan() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.arctan`\n",
      " |\n",
      " |  arctan2(...)\n",
      " |      arctan2(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.arctan2`\n",
      " |\n",
      " |  arctan2_(...)\n",
      " |      atan2_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.arctan2`\n",
      " |\n",
      " |  arctan_(...)\n",
      " |      arctan_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.arctan`\n",
      " |\n",
      " |  arctanh(...)\n",
      " |      arctanh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.arctanh`\n",
      " |\n",
      " |  arctanh_(...)\n",
      " |      arctanh_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.arctanh`\n",
      " |\n",
      " |  argmax(...)\n",
      " |      argmax(dim=None, keepdim=False) -> LongTensor\n",
      " |\n",
      " |      See :func:`torch.argmax`\n",
      " |\n",
      " |  argmin(...)\n",
      " |      argmin(dim=None, keepdim=False) -> LongTensor\n",
      " |\n",
      " |      See :func:`torch.argmin`\n",
      " |\n",
      " |  argsort(...)\n",
      " |      argsort(dim=-1, descending=False) -> LongTensor\n",
      " |\n",
      " |      See :func:`torch.argsort`\n",
      " |\n",
      " |  argwhere(...)\n",
      " |      argwhere() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.argwhere`\n",
      " |\n",
      " |  as_strided(...)\n",
      " |      as_strided(size, stride, storage_offset=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.as_strided`\n",
      " |\n",
      " |  as_strided_(...)\n",
      " |      as_strided_(size, stride, storage_offset=None) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.as_strided`\n",
      " |\n",
      " |  as_strided_scatter(...)\n",
      " |      as_strided_scatter(src, size, stride, storage_offset=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.as_strided_scatter`\n",
      " |\n",
      " |  as_subclass(...)\n",
      " |      as_subclass(cls) -> Tensor\n",
      " |\n",
      " |      Makes a ``cls`` instance with the same data pointer as ``self``. Changes\n",
      " |      in the output mirror changes in ``self``, and the output stays attached\n",
      " |      to the autograd graph. ``cls`` must be a subclass of ``Tensor``.\n",
      " |\n",
      " |  asin(...)\n",
      " |      asin() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.asin`\n",
      " |\n",
      " |  asin_(...)\n",
      " |      asin_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.asin`\n",
      " |\n",
      " |  asinh(...)\n",
      " |      asinh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.asinh`\n",
      " |\n",
      " |  asinh_(...)\n",
      " |      asinh_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.asinh`\n",
      " |\n",
      " |  atan(...)\n",
      " |      atan() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.atan`\n",
      " |\n",
      " |  atan2(...)\n",
      " |      atan2(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.atan2`\n",
      " |\n",
      " |  atan2_(...)\n",
      " |      atan2_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.atan2`\n",
      " |\n",
      " |  atan_(...)\n",
      " |      atan_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.atan`\n",
      " |\n",
      " |  atanh(...)\n",
      " |      atanh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.atanh`\n",
      " |\n",
      " |  atanh_(...)\n",
      " |      atanh_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.atanh`\n",
      " |\n",
      " |  baddbmm(...)\n",
      " |      baddbmm(batch1, batch2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.baddbmm`\n",
      " |\n",
      " |  baddbmm_(...)\n",
      " |      baddbmm_(batch1, batch2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.baddbmm`\n",
      " |\n",
      " |  bernoulli(...)\n",
      " |      bernoulli(*, generator=None) -> Tensor\n",
      " |\n",
      " |      Returns a result tensor where each :math:`\\texttt{result[i]}` is independently\n",
      " |      sampled from :math:`\\text{Bernoulli}(\\texttt{self[i]})`. :attr:`self` must have\n",
      " |      floating point ``dtype``, and the result will have the same ``dtype``.\n",
      " |\n",
      " |      See :func:`torch.bernoulli`\n",
      " |\n",
      " |  bernoulli_(...)\n",
      " |      bernoulli_(p=0.5, *, generator=None) -> Tensor\n",
      " |\n",
      " |      Fills each location of :attr:`self` with an independent sample from\n",
      " |      :math:`\\text{Bernoulli}(\\texttt{p})`. :attr:`self` can have integral\n",
      " |      ``dtype``.\n",
      " |\n",
      " |      :attr:`p` should either be a scalar or tensor containing probabilities to be\n",
      " |      used for drawing the binary random number.\n",
      " |\n",
      " |      If it is a tensor, the :math:`\\text{i}^{th}` element of :attr:`self` tensor\n",
      " |      will be set to a value sampled from\n",
      " |      :math:`\\text{Bernoulli}(\\texttt{p\\_tensor[i]})`. In this case `p` must have\n",
      " |      floating point ``dtype``.\n",
      " |\n",
      " |      See also :meth:`~Tensor.bernoulli` and :func:`torch.bernoulli`\n",
      " |\n",
      " |  bfloat16(...)\n",
      " |      bfloat16(memory_format=torch.preserve_format) -> Tensor\n",
      " |      ``self.bfloat16()`` is equivalent to ``self.to(torch.bfloat16)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  bincount(...)\n",
      " |      bincount(weights=None, minlength=0) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.bincount`\n",
      " |\n",
      " |  bitwise_and(...)\n",
      " |      bitwise_and() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.bitwise_and`\n",
      " |\n",
      " |  bitwise_and_(...)\n",
      " |      bitwise_and_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.bitwise_and`\n",
      " |\n",
      " |  bitwise_left_shift(...)\n",
      " |      bitwise_left_shift(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.bitwise_left_shift`\n",
      " |\n",
      " |  bitwise_left_shift_(...)\n",
      " |      bitwise_left_shift_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.bitwise_left_shift`\n",
      " |\n",
      " |  bitwise_not(...)\n",
      " |      bitwise_not() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.bitwise_not`\n",
      " |\n",
      " |  bitwise_not_(...)\n",
      " |      bitwise_not_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.bitwise_not`\n",
      " |\n",
      " |  bitwise_or(...)\n",
      " |      bitwise_or() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.bitwise_or`\n",
      " |\n",
      " |  bitwise_or_(...)\n",
      " |      bitwise_or_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.bitwise_or`\n",
      " |\n",
      " |  bitwise_right_shift(...)\n",
      " |      bitwise_right_shift(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.bitwise_right_shift`\n",
      " |\n",
      " |  bitwise_right_shift_(...)\n",
      " |      bitwise_right_shift_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.bitwise_right_shift`\n",
      " |\n",
      " |  bitwise_xor(...)\n",
      " |      bitwise_xor() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.bitwise_xor`\n",
      " |\n",
      " |  bitwise_xor_(...)\n",
      " |      bitwise_xor_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.bitwise_xor`\n",
      " |\n",
      " |  bmm(...)\n",
      " |      bmm(batch2) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.bmm`\n",
      " |\n",
      " |  bool(...)\n",
      " |      bool(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.bool()`` is equivalent to ``self.to(torch.bool)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  broadcast_to(...)\n",
      " |      broadcast_to(shape) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.broadcast_to`.\n",
      " |\n",
      " |  byte(...)\n",
      " |      byte(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.byte()`` is equivalent to ``self.to(torch.uint8)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  cauchy_(...)\n",
      " |      cauchy_(median=0, sigma=1, *, generator=None) -> Tensor\n",
      " |\n",
      " |      Fills the tensor with numbers drawn from the Cauchy distribution:\n",
      " |\n",
      " |      .. math::\n",
      " |\n",
      " |          f(x) = \\dfrac{1}{\\pi} \\dfrac{\\sigma}{(x - \\text{median})^2 + \\sigma^2}\n",
      " |\n",
      " |      .. note::\n",
      " |        Sigma (:math:`\\sigma`) is used to denote the scale parameter in Cauchy distribution.\n",
      " |\n",
      " |  ccol_indices(...)\n",
      " |\n",
      " |  cdouble(...)\n",
      " |      cdouble(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.cdouble()`` is equivalent to ``self.to(torch.complex128)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  ceil(...)\n",
      " |      ceil() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.ceil`\n",
      " |\n",
      " |  ceil_(...)\n",
      " |      ceil_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.ceil`\n",
      " |\n",
      " |  cfloat(...)\n",
      " |      cfloat(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.cfloat()`` is equivalent to ``self.to(torch.complex64)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  chalf(...)\n",
      " |      chalf(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.chalf()`` is equivalent to ``self.to(torch.complex32)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |           memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  char(...)\n",
      " |      char(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.char()`` is equivalent to ``self.to(torch.int8)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  cholesky(...)\n",
      " |      cholesky(upper=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cholesky`\n",
      " |\n",
      " |  cholesky_inverse(...)\n",
      " |      cholesky_inverse(upper=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cholesky_inverse`\n",
      " |\n",
      " |  cholesky_solve(...)\n",
      " |      cholesky_solve(input2, upper=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cholesky_solve`\n",
      " |\n",
      " |  chunk(...)\n",
      " |      chunk(chunks, dim=0) -> List of Tensors\n",
      " |\n",
      " |      See :func:`torch.chunk`\n",
      " |\n",
      " |  clamp(...)\n",
      " |      clamp(min=None, max=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.clamp`\n",
      " |\n",
      " |  clamp_(...)\n",
      " |      clamp_(min=None, max=None) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.clamp`\n",
      " |\n",
      " |  clamp_max(...)\n",
      " |\n",
      " |  clamp_max_(...)\n",
      " |\n",
      " |  clamp_min(...)\n",
      " |\n",
      " |  clamp_min_(...)\n",
      " |\n",
      " |  clip(...)\n",
      " |      clip(min=None, max=None) -> Tensor\n",
      " |\n",
      " |      Alias for :meth:`~Tensor.clamp`.\n",
      " |\n",
      " |  clip_(...)\n",
      " |      clip_(min=None, max=None) -> Tensor\n",
      " |\n",
      " |      Alias for :meth:`~Tensor.clamp_`.\n",
      " |\n",
      " |  clone(...)\n",
      " |      clone(*, memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.clone`\n",
      " |\n",
      " |  coalesce(...)\n",
      " |      coalesce() -> Tensor\n",
      " |\n",
      " |      Returns a coalesced copy of :attr:`self` if :attr:`self` is an\n",
      " |      :ref:`uncoalesced tensor <sparse-uncoalesced-coo-docs>`.\n",
      " |\n",
      " |      Returns :attr:`self` if :attr:`self` is a coalesced tensor.\n",
      " |\n",
      " |      .. warning::\n",
      " |        Throws an error if :attr:`self` is not a sparse COO tensor.\n",
      " |\n",
      " |  col_indices(...)\n",
      " |      col_indices() -> IntTensor\n",
      " |\n",
      " |      Returns the tensor containing the column indices of the :attr:`self`\n",
      " |      tensor when :attr:`self` is a sparse CSR tensor of layout ``sparse_csr``.\n",
      " |      The ``col_indices`` tensor is strictly of shape (:attr:`self`.nnz())\n",
      " |      and of type ``int32`` or ``int64``.  When using MKL routines such as sparse\n",
      " |      matrix multiplication, it is necessary to use ``int32`` indexing in order\n",
      " |      to avoid downcasting and potentially losing information.\n",
      " |\n",
      " |      Example::\n",
      " |          >>> csr = torch.eye(5,5).to_sparse_csr()\n",
      " |          >>> csr.col_indices()\n",
      " |          tensor([0, 1, 2, 3, 4], dtype=torch.int32)\n",
      " |\n",
      " |  conj(...)\n",
      " |      conj() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.conj`\n",
      " |\n",
      " |  conj_physical(...)\n",
      " |      conj_physical() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.conj_physical`\n",
      " |\n",
      " |  conj_physical_(...)\n",
      " |      conj_physical_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.conj_physical`\n",
      " |\n",
      " |  contiguous(...)\n",
      " |      contiguous(memory_format=torch.contiguous_format) -> Tensor\n",
      " |\n",
      " |      Returns a contiguous in memory tensor containing the same data as :attr:`self` tensor. If\n",
      " |      :attr:`self` tensor is already in the specified memory format, this function returns the\n",
      " |      :attr:`self` tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.contiguous_format``.\n",
      " |\n",
      " |  copy_(...)\n",
      " |      copy_(src, non_blocking=False) -> Tensor\n",
      " |\n",
      " |      Copies the elements from :attr:`src` into :attr:`self` tensor and returns\n",
      " |      :attr:`self`.\n",
      " |\n",
      " |      The :attr:`src` tensor must be :ref:`broadcastable <broadcasting-semantics>`\n",
      " |      with the :attr:`self` tensor. It may be of a different data type or reside on a\n",
      " |      different device.\n",
      " |\n",
      " |      Args:\n",
      " |          src (Tensor): the source tensor to copy from\n",
      " |          non_blocking (bool): if ``True`` and this copy is between CPU and GPU,\n",
      " |              the copy may occur asynchronously with respect to the host. For other\n",
      " |              cases, this argument has no effect.\n",
      " |\n",
      " |  copysign(...)\n",
      " |      copysign(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.copysign`\n",
      " |\n",
      " |  copysign_(...)\n",
      " |      copysign_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.copysign`\n",
      " |\n",
      " |  corrcoef(...)\n",
      " |      corrcoef() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.corrcoef`\n",
      " |\n",
      " |  cos(...)\n",
      " |      cos() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cos`\n",
      " |\n",
      " |  cos_(...)\n",
      " |      cos_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.cos`\n",
      " |\n",
      " |  cosh(...)\n",
      " |      cosh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cosh`\n",
      " |\n",
      " |  cosh_(...)\n",
      " |      cosh_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.cosh`\n",
      " |\n",
      " |  count_nonzero(...)\n",
      " |      count_nonzero(dim=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.count_nonzero`\n",
      " |\n",
      " |  cov(...)\n",
      " |      cov(*, correction=1, fweights=None, aweights=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cov`\n",
      " |\n",
      " |  cpu(...)\n",
      " |      cpu(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      Returns a copy of this object in CPU memory.\n",
      " |\n",
      " |      If this object is already in CPU memory,\n",
      " |      then no copy is performed and the original object is returned.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  cross(...)\n",
      " |      cross(other, dim=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cross`\n",
      " |\n",
      " |  crow_indices(...)\n",
      " |      crow_indices() -> IntTensor\n",
      " |\n",
      " |      Returns the tensor containing the compressed row indices of the :attr:`self`\n",
      " |      tensor when :attr:`self` is a sparse CSR tensor of layout ``sparse_csr``.\n",
      " |      The ``crow_indices`` tensor is strictly of shape (:attr:`self`.size(0) + 1)\n",
      " |      and of type ``int32`` or ``int64``. When using MKL routines such as sparse\n",
      " |      matrix multiplication, it is necessary to use ``int32`` indexing in order\n",
      " |      to avoid downcasting and potentially losing information.\n",
      " |\n",
      " |      Example::\n",
      " |          >>> csr = torch.eye(5,5).to_sparse_csr()\n",
      " |          >>> csr.crow_indices()\n",
      " |          tensor([0, 1, 2, 3, 4, 5], dtype=torch.int32)\n",
      " |\n",
      " |  cuda(...)\n",
      " |      cuda(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      Returns a copy of this object in CUDA memory.\n",
      " |\n",
      " |      If this object is already in CUDA memory and on the correct device,\n",
      " |      then no copy is performed and the original object is returned.\n",
      " |\n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The destination GPU device.\n",
      " |              Defaults to the current CUDA device.\n",
      " |          non_blocking (bool): If ``True`` and the source is in pinned memory,\n",
      " |              the copy will be asynchronous with respect to the host.\n",
      " |              Otherwise, the argument has no effect. Default: ``False``.\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  cummax(...)\n",
      " |      cummax(dim) -> (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.cummax`\n",
      " |\n",
      " |  cummin(...)\n",
      " |      cummin(dim) -> (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.cummin`\n",
      " |\n",
      " |  cumprod(...)\n",
      " |      cumprod(dim, dtype=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cumprod`\n",
      " |\n",
      " |  cumprod_(...)\n",
      " |      cumprod_(dim, dtype=None) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.cumprod`\n",
      " |\n",
      " |  cumsum(...)\n",
      " |      cumsum(dim, dtype=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cumsum`\n",
      " |\n",
      " |  cumsum_(...)\n",
      " |      cumsum_(dim, dtype=None) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.cumsum`\n",
      " |\n",
      " |  data_ptr(...)\n",
      " |      data_ptr() -> int\n",
      " |\n",
      " |      Returns the address of the first element of :attr:`self` tensor.\n",
      " |\n",
      " |  deg2rad(...)\n",
      " |      deg2rad() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.deg2rad`\n",
      " |\n",
      " |  deg2rad_(...)\n",
      " |      deg2rad_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.deg2rad`\n",
      " |\n",
      " |  dense_dim(...)\n",
      " |      dense_dim() -> int\n",
      " |\n",
      " |      Return the number of dense dimensions in a :ref:`sparse tensor <sparse-docs>` :attr:`self`.\n",
      " |\n",
      " |      .. note::\n",
      " |        Returns ``len(self.shape)`` if :attr:`self` is not a sparse tensor.\n",
      " |\n",
      " |      See also :meth:`Tensor.sparse_dim` and :ref:`hybrid tensors <sparse-hybrid-coo-docs>`.\n",
      " |\n",
      " |  dequantize(...)\n",
      " |      dequantize() -> Tensor\n",
      " |\n",
      " |      Given a quantized Tensor, dequantize it and return the dequantized float Tensor.\n",
      " |\n",
      " |  det(...)\n",
      " |      det() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.det`\n",
      " |\n",
      " |  diag(...)\n",
      " |      diag(diagonal=0) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.diag`\n",
      " |\n",
      " |  diag_embed(...)\n",
      " |      diag_embed(offset=0, dim1=-2, dim2=-1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.diag_embed`\n",
      " |\n",
      " |  diagflat(...)\n",
      " |      diagflat(offset=0) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.diagflat`\n",
      " |\n",
      " |  diagonal(...)\n",
      " |      diagonal(offset=0, dim1=0, dim2=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.diagonal`\n",
      " |\n",
      " |  diagonal_scatter(...)\n",
      " |      diagonal_scatter(src, offset=0, dim1=0, dim2=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.diagonal_scatter`\n",
      " |\n",
      " |  diff(...)\n",
      " |      diff(n=1, dim=-1, prepend=None, append=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.diff`\n",
      " |\n",
      " |  digamma(...)\n",
      " |      digamma() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.digamma`\n",
      " |\n",
      " |  digamma_(...)\n",
      " |      digamma_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.digamma`\n",
      " |\n",
      " |  dim(...)\n",
      " |      dim() -> int\n",
      " |\n",
      " |      Returns the number of dimensions of :attr:`self` tensor.\n",
      " |\n",
      " |  dist(...)\n",
      " |      dist(other, p=2) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.dist`\n",
      " |\n",
      " |  div(...)\n",
      " |      div(value, *, rounding_mode=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.div`\n",
      " |\n",
      " |  div_(...)\n",
      " |      div_(value, *, rounding_mode=None) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.div`\n",
      " |\n",
      " |  divide(...)\n",
      " |      divide(value, *, rounding_mode=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.divide`\n",
      " |\n",
      " |  divide_(...)\n",
      " |      divide_(value, *, rounding_mode=None) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.divide`\n",
      " |\n",
      " |  dot(...)\n",
      " |      dot(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.dot`\n",
      " |\n",
      " |  double(...)\n",
      " |      double(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.double()`` is equivalent to ``self.to(torch.float64)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  dsplit(...)\n",
      " |      dsplit(split_size_or_sections) -> List of Tensors\n",
      " |\n",
      " |      See :func:`torch.dsplit`\n",
      " |\n",
      " |  element_size(...)\n",
      " |      element_size() -> int\n",
      " |\n",
      " |      Returns the size in bytes of an individual element.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> torch.tensor([]).element_size()\n",
      " |          4\n",
      " |          >>> torch.tensor([], dtype=torch.uint8).element_size()\n",
      " |          1\n",
      " |\n",
      " |  eq(...)\n",
      " |      eq(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.eq`\n",
      " |\n",
      " |  eq_(...)\n",
      " |      eq_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.eq`\n",
      " |\n",
      " |  equal(...)\n",
      " |      equal(other) -> bool\n",
      " |\n",
      " |      See :func:`torch.equal`\n",
      " |\n",
      " |  erf(...)\n",
      " |      erf() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.erf`\n",
      " |\n",
      " |  erf_(...)\n",
      " |      erf_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.erf`\n",
      " |\n",
      " |  erfc(...)\n",
      " |      erfc() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.erfc`\n",
      " |\n",
      " |  erfc_(...)\n",
      " |      erfc_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.erfc`\n",
      " |\n",
      " |  erfinv(...)\n",
      " |      erfinv() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.erfinv`\n",
      " |\n",
      " |  erfinv_(...)\n",
      " |      erfinv_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.erfinv`\n",
      " |\n",
      " |  exp(...)\n",
      " |      exp() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.exp`\n",
      " |\n",
      " |  exp2(...)\n",
      " |      exp2() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.exp2`\n",
      " |\n",
      " |  exp2_(...)\n",
      " |      exp2_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.exp2`\n",
      " |\n",
      " |  exp_(...)\n",
      " |      exp_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.exp`\n",
      " |\n",
      " |  expand(...)\n",
      " |      expand(*sizes) -> Tensor\n",
      " |\n",
      " |      Returns a new view of the :attr:`self` tensor with singleton dimensions expanded\n",
      " |      to a larger size.\n",
      " |\n",
      " |      Passing -1 as the size for a dimension means not changing the size of\n",
      " |      that dimension.\n",
      " |\n",
      " |      Tensor can be also expanded to a larger number of dimensions, and the\n",
      " |      new ones will be appended at the front. For the new dimensions, the\n",
      " |      size cannot be set to -1.\n",
      " |\n",
      " |      Expanding a tensor does not allocate new memory, but only creates a\n",
      " |      new view on the existing tensor where a dimension of size one is\n",
      " |      expanded to a larger size by setting the ``stride`` to 0. Any dimension\n",
      " |      of size 1 can be expanded to an arbitrary value without allocating new\n",
      " |      memory.\n",
      " |\n",
      " |      Args:\n",
      " |          *sizes (torch.Size or int...): the desired expanded size\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          More than one element of an expanded tensor may refer to a single\n",
      " |          memory location. As a result, in-place operations (especially ones that\n",
      " |          are vectorized) may result in incorrect behavior. If you need to write\n",
      " |          to the tensors, please clone them first.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.tensor([[1], [2], [3]])\n",
      " |          >>> x.size()\n",
      " |          torch.Size([3, 1])\n",
      " |          >>> x.expand(3, 4)\n",
      " |          tensor([[ 1,  1,  1,  1],\n",
      " |                  [ 2,  2,  2,  2],\n",
      " |                  [ 3,  3,  3,  3]])\n",
      " |          >>> x.expand(-1, 4)   # -1 means not changing the size of that dimension\n",
      " |          tensor([[ 1,  1,  1,  1],\n",
      " |                  [ 2,  2,  2,  2],\n",
      " |                  [ 3,  3,  3,  3]])\n",
      " |\n",
      " |  expand_as(...)\n",
      " |      expand_as(other) -> Tensor\n",
      " |\n",
      " |      Expand this tensor to the same size as :attr:`other`.\n",
      " |      ``self.expand_as(other)`` is equivalent to ``self.expand(other.size())``.\n",
      " |\n",
      " |      Please see :meth:`~Tensor.expand` for more information about ``expand``.\n",
      " |\n",
      " |      Args:\n",
      " |          other (:class:`torch.Tensor`): The result tensor has the same size\n",
      " |              as :attr:`other`.\n",
      " |\n",
      " |  expm1(...)\n",
      " |      expm1() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.expm1`\n",
      " |\n",
      " |  expm1_(...)\n",
      " |      expm1_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.expm1`\n",
      " |\n",
      " |  exponential_(...)\n",
      " |      exponential_(lambd=1, *, generator=None) -> Tensor\n",
      " |\n",
      " |      Fills :attr:`self` tensor with elements drawn from the PDF (probability density function):\n",
      " |\n",
      " |      .. math::\n",
      " |\n",
      " |          f(x) = \\lambda e^{-\\lambda x}, x > 0\n",
      " |\n",
      " |      .. note::\n",
      " |        In probability theory, exponential distribution is supported on interval [0, :math:`\\inf`) (i.e., :math:`x >= 0`)\n",
      " |        implying that zero can be sampled from the exponential distribution.\n",
      " |        However, :func:`torch.Tensor.exponential_` does not sample zero,\n",
      " |        which means that its actual support is the interval (0, :math:`\\inf`).\n",
      " |\n",
      " |        Note that :func:`torch.distributions.exponential.Exponential` is supported on the interval [0, :math:`\\inf`) and can sample zero.\n",
      " |\n",
      " |  fill_(...)\n",
      " |      fill_(value) -> Tensor\n",
      " |\n",
      " |      Fills :attr:`self` tensor with the specified value.\n",
      " |\n",
      " |  fill_diagonal_(...)\n",
      " |      fill_diagonal_(fill_value, wrap=False) -> Tensor\n",
      " |\n",
      " |      Fill the main diagonal of a tensor that has at least 2-dimensions.\n",
      " |      When dims>2, all dimensions of input must be of equal length.\n",
      " |      This function modifies the input tensor in-place, and returns the input tensor.\n",
      " |\n",
      " |      Arguments:\n",
      " |          fill_value (Scalar): the fill value\n",
      " |          wrap (bool): the diagonal 'wrapped' after N columns for tall matrices.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> a = torch.zeros(3, 3)\n",
      " |          >>> a.fill_diagonal_(5)\n",
      " |          tensor([[5., 0., 0.],\n",
      " |                  [0., 5., 0.],\n",
      " |                  [0., 0., 5.]])\n",
      " |          >>> b = torch.zeros(7, 3)\n",
      " |          >>> b.fill_diagonal_(5)\n",
      " |          tensor([[5., 0., 0.],\n",
      " |                  [0., 5., 0.],\n",
      " |                  [0., 0., 5.],\n",
      " |                  [0., 0., 0.],\n",
      " |                  [0., 0., 0.],\n",
      " |                  [0., 0., 0.],\n",
      " |                  [0., 0., 0.]])\n",
      " |          >>> c = torch.zeros(7, 3)\n",
      " |          >>> c.fill_diagonal_(5, wrap=True)\n",
      " |          tensor([[5., 0., 0.],\n",
      " |                  [0., 5., 0.],\n",
      " |                  [0., 0., 5.],\n",
      " |                  [0., 0., 0.],\n",
      " |                  [5., 0., 0.],\n",
      " |                  [0., 5., 0.],\n",
      " |                  [0., 0., 5.]])\n",
      " |\n",
      " |  fix(...)\n",
      " |      fix() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.fix`.\n",
      " |\n",
      " |  fix_(...)\n",
      " |      fix_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.fix`\n",
      " |\n",
      " |  flatten(...)\n",
      " |      flatten(start_dim=0, end_dim=-1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.flatten`\n",
      " |\n",
      " |  flip(...)\n",
      " |      flip(dims) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.flip`\n",
      " |\n",
      " |  fliplr(...)\n",
      " |      fliplr() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.fliplr`\n",
      " |\n",
      " |  flipud(...)\n",
      " |      flipud() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.flipud`\n",
      " |\n",
      " |  float(...)\n",
      " |      float(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.float()`` is equivalent to ``self.to(torch.float32)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  float_power(...)\n",
      " |      float_power(exponent) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.float_power`\n",
      " |\n",
      " |  float_power_(...)\n",
      " |      float_power_(exponent) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.float_power`\n",
      " |\n",
      " |  floor(...)\n",
      " |      floor() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.floor`\n",
      " |\n",
      " |  floor_(...)\n",
      " |      floor_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.floor`\n",
      " |\n",
      " |  floor_divide(...)\n",
      " |      floor_divide(value) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.floor_divide`\n",
      " |\n",
      " |  floor_divide_(...)\n",
      " |      floor_divide_(value) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.floor_divide`\n",
      " |\n",
      " |  fmax(...)\n",
      " |      fmax(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.fmax`\n",
      " |\n",
      " |  fmin(...)\n",
      " |      fmin(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.fmin`\n",
      " |\n",
      " |  fmod(...)\n",
      " |      fmod(divisor) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.fmod`\n",
      " |\n",
      " |  fmod_(...)\n",
      " |      fmod_(divisor) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.fmod`\n",
      " |\n",
      " |  frac(...)\n",
      " |      frac() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.frac`\n",
      " |\n",
      " |  frac_(...)\n",
      " |      frac_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.frac`\n",
      " |\n",
      " |  frexp(...)\n",
      " |      frexp(input) -> (Tensor mantissa, Tensor exponent)\n",
      " |\n",
      " |      See :func:`torch.frexp`\n",
      " |\n",
      " |  gather(...)\n",
      " |      gather(dim, index) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.gather`\n",
      " |\n",
      " |  gcd(...)\n",
      " |      gcd(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.gcd`\n",
      " |\n",
      " |  gcd_(...)\n",
      " |      gcd_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.gcd`\n",
      " |\n",
      " |  ge(...)\n",
      " |      ge(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.ge`.\n",
      " |\n",
      " |  ge_(...)\n",
      " |      ge_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.ge`.\n",
      " |\n",
      " |  geometric_(...)\n",
      " |      geometric_(p, *, generator=None) -> Tensor\n",
      " |\n",
      " |      Fills :attr:`self` tensor with elements drawn from the geometric distribution:\n",
      " |\n",
      " |      .. math::\n",
      " |\n",
      " |          P(X=k) = (1 - p)^{k - 1} p, k = 1, 2, ...\n",
      " |\n",
      " |      .. note::\n",
      " |        :func:`torch.Tensor.geometric_` `k`-th trial is the first success hence draws samples in :math:`\\{1, 2, \\ldots\\}`, whereas\n",
      " |        :func:`torch.distributions.geometric.Geometric` :math:`(k+1)`-th trial is the first success\n",
      " |        hence draws samples in :math:`\\{0, 1, \\ldots\\}`.\n",
      " |\n",
      " |  geqrf(...)\n",
      " |      geqrf() -> (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.geqrf`\n",
      " |\n",
      " |  ger(...)\n",
      " |      ger(vec2) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.ger`\n",
      " |\n",
      " |  get_device(...)\n",
      " |      get_device() -> Device ordinal (Integer)\n",
      " |\n",
      " |      For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.\n",
      " |      For CPU tensors, this function returns `-1`.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.randn(3, 4, 5, device='cuda:0')\n",
      " |          >>> x.get_device()\n",
      " |          0\n",
      " |          >>> x.cpu().get_device()\n",
      " |          -1\n",
      " |\n",
      " |  greater(...)\n",
      " |      greater(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.greater`.\n",
      " |\n",
      " |  greater_(...)\n",
      " |      greater_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.greater`.\n",
      " |\n",
      " |  greater_equal(...)\n",
      " |      greater_equal(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.greater_equal`.\n",
      " |\n",
      " |  greater_equal_(...)\n",
      " |      greater_equal_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.greater_equal`.\n",
      " |\n",
      " |  gt(...)\n",
      " |      gt(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.gt`.\n",
      " |\n",
      " |  gt_(...)\n",
      " |      gt_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.gt`.\n",
      " |\n",
      " |  half(...)\n",
      " |      half(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.half()`` is equivalent to ``self.to(torch.float16)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  hardshrink(...)\n",
      " |      hardshrink(lambd=0.5) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.nn.functional.hardshrink`\n",
      " |\n",
      " |  has_names(...)\n",
      " |      Is ``True`` if any of this tensor's dimensions are named. Otherwise, is ``False``.\n",
      " |\n",
      " |  heaviside(...)\n",
      " |      heaviside(values) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.heaviside`\n",
      " |\n",
      " |  heaviside_(...)\n",
      " |      heaviside_(values) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.heaviside`\n",
      " |\n",
      " |  histc(...)\n",
      " |      histc(bins=100, min=0, max=0) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.histc`\n",
      " |\n",
      " |  histogram(...)\n",
      " |      histogram(input, bins, *, range=None, weight=None, density=False) -> (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.histogram`\n",
      " |\n",
      " |  hsplit(...)\n",
      " |      hsplit(split_size_or_sections) -> List of Tensors\n",
      " |\n",
      " |      See :func:`torch.hsplit`\n",
      " |\n",
      " |  hypot(...)\n",
      " |      hypot(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.hypot`\n",
      " |\n",
      " |  hypot_(...)\n",
      " |      hypot_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.hypot`\n",
      " |\n",
      " |  i0(...)\n",
      " |      i0() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.i0`\n",
      " |\n",
      " |  i0_(...)\n",
      " |      i0_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.i0`\n",
      " |\n",
      " |  igamma(...)\n",
      " |      igamma(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.igamma`\n",
      " |\n",
      " |  igamma_(...)\n",
      " |      igamma_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.igamma`\n",
      " |\n",
      " |  igammac(...)\n",
      " |      igammac(other) -> Tensor\n",
      " |      See :func:`torch.igammac`\n",
      " |\n",
      " |  igammac_(...)\n",
      " |      igammac_(other) -> Tensor\n",
      " |      In-place version of :meth:`~Tensor.igammac`\n",
      " |\n",
      " |  index_add(...)\n",
      " |      index_add(dim, index, source, *, alpha=1) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.index_add_`.\n",
      " |\n",
      " |  index_add_(...)\n",
      " |      index_add_(dim, index, source, *, alpha=1) -> Tensor\n",
      " |\n",
      " |      Accumulate the elements of :attr:`alpha` times ``source`` into the :attr:`self`\n",
      " |      tensor by adding to the indices in the order given in :attr:`index`. For example,\n",
      " |      if ``dim == 0``, ``index[i] == j``, and ``alpha=-1``, then the ``i``\\ th row of\n",
      " |      ``source`` is subtracted from the ``j``\\ th row of :attr:`self`.\n",
      " |\n",
      " |      The :attr:`dim`\\ th dimension of ``source`` must have the same size as the\n",
      " |      length of :attr:`index` (which must be a vector), and all other dimensions must\n",
      " |      match :attr:`self`, or an error will be raised.\n",
      " |\n",
      " |      For a 3-D tensor the output is given as::\n",
      " |\n",
      " |          self[index[i], :, :] += alpha * src[i, :, :]  # if dim == 0\n",
      " |          self[:, index[i], :] += alpha * src[:, i, :]  # if dim == 1\n",
      " |          self[:, :, index[i]] += alpha * src[:, :, i]  # if dim == 2\n",
      " |\n",
      " |      Note:\n",
      " |          This operation may behave nondeterministically when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int): dimension along which to index\n",
      " |          index (Tensor): indices of ``source`` to select from,\n",
      " |                  should have dtype either `torch.int64` or `torch.int32`\n",
      " |          source (Tensor): the tensor containing values to add\n",
      " |\n",
      " |      Keyword args:\n",
      " |          alpha (Number): the scalar multiplier for ``source``\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.ones(5, 3)\n",
      " |          >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
      " |          >>> index = torch.tensor([0, 4, 2])\n",
      " |          >>> x.index_add_(0, index, t)\n",
      " |          tensor([[  2.,   3.,   4.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  8.,   9.,  10.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  5.,   6.,   7.]])\n",
      " |          >>> x.index_add_(0, index, t, alpha=-1)\n",
      " |          tensor([[  1.,   1.,   1.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  1.,   1.,   1.]])\n",
      " |\n",
      " |  index_copy(...)\n",
      " |      index_copy(dim, index, tensor2) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.index_copy_`.\n",
      " |\n",
      " |  index_copy_(...)\n",
      " |      index_copy_(dim, index, tensor) -> Tensor\n",
      " |\n",
      " |      Copies the elements of :attr:`tensor` into the :attr:`self` tensor by selecting\n",
      " |      the indices in the order given in :attr:`index`. For example, if ``dim == 0``\n",
      " |      and ``index[i] == j``, then the ``i``\\ th row of :attr:`tensor` is copied to the\n",
      " |      ``j``\\ th row of :attr:`self`.\n",
      " |\n",
      " |      The :attr:`dim`\\ th dimension of :attr:`tensor` must have the same size as the\n",
      " |      length of :attr:`index` (which must be a vector), and all other dimensions must\n",
      " |      match :attr:`self`, or an error will be raised.\n",
      " |\n",
      " |      .. note::\n",
      " |          If :attr:`index` contains duplicate entries, multiple elements from\n",
      " |          :attr:`tensor` will be copied to the same index of :attr:`self`. The result\n",
      " |          is nondeterministic since it depends on which copy occurs last.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int): dimension along which to index\n",
      " |          index (LongTensor): indices of :attr:`tensor` to select from\n",
      " |          tensor (Tensor): the tensor containing values to copy\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.zeros(5, 3)\n",
      " |          >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
      " |          >>> index = torch.tensor([0, 4, 2])\n",
      " |          >>> x.index_copy_(0, index, t)\n",
      " |          tensor([[ 1.,  2.,  3.],\n",
      " |                  [ 0.,  0.,  0.],\n",
      " |                  [ 7.,  8.,  9.],\n",
      " |                  [ 0.,  0.,  0.],\n",
      " |                  [ 4.,  5.,  6.]])\n",
      " |\n",
      " |  index_fill(...)\n",
      " |      index_fill(dim, index, value) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.index_fill_`.\n",
      " |\n",
      " |  index_fill_(...)\n",
      " |      index_fill_(dim, index, value) -> Tensor\n",
      " |\n",
      " |      Fills the elements of the :attr:`self` tensor with value :attr:`value` by\n",
      " |      selecting the indices in the order given in :attr:`index`.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int): dimension along which to index\n",
      " |          index (LongTensor): indices of :attr:`self` tensor to fill in\n",
      " |          value (float): the value to fill with\n",
      " |\n",
      " |      Example::\n",
      " |          >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
      " |          >>> index = torch.tensor([0, 2])\n",
      " |          >>> x.index_fill_(1, index, -1)\n",
      " |          tensor([[-1.,  2., -1.],\n",
      " |                  [-1.,  5., -1.],\n",
      " |                  [-1.,  8., -1.]])\n",
      " |\n",
      " |  index_put(...)\n",
      " |      index_put(indices, values, accumulate=False) -> Tensor\n",
      " |\n",
      " |      Out-place version of :meth:`~Tensor.index_put_`.\n",
      " |\n",
      " |  index_put_(...)\n",
      " |      index_put_(indices, values, accumulate=False) -> Tensor\n",
      " |\n",
      " |      Puts values from the tensor :attr:`values` into the tensor :attr:`self` using\n",
      " |      the indices specified in :attr:`indices` (which is a tuple of Tensors). The\n",
      " |      expression ``tensor.index_put_(indices, values)`` is equivalent to\n",
      " |      ``tensor[indices] = values``. Returns :attr:`self`.\n",
      " |\n",
      " |      If :attr:`accumulate` is ``True``, the elements in :attr:`values` are added to\n",
      " |      :attr:`self`. If accumulate is ``False``, the behavior is undefined if indices\n",
      " |      contain duplicate elements.\n",
      " |\n",
      " |      Args:\n",
      " |          indices (tuple of LongTensor): tensors used to index into `self`.\n",
      " |          values (Tensor): tensor of same dtype as `self`.\n",
      " |          accumulate (bool): whether to accumulate into self\n",
      " |\n",
      " |  index_reduce(...)\n",
      " |\n",
      " |  index_reduce_(...)\n",
      " |      index_reduce_(dim, index, source, reduce, *, include_self=True) -> Tensor\n",
      " |\n",
      " |      Accumulate the elements of ``source`` into the :attr:`self`\n",
      " |      tensor by accumulating to the indices in the order given in :attr:`index`\n",
      " |      using the reduction given by the ``reduce`` argument. For example, if ``dim == 0``,\n",
      " |      ``index[i] == j``, ``reduce == prod`` and ``include_self == True`` then the ``i``\\ th\n",
      " |      row of ``source`` is multiplied by the ``j``\\ th row of :attr:`self`. If\n",
      " |      :obj:`include_self=\"True\"`, the values in the :attr:`self` tensor are included\n",
      " |      in the reduction, otherwise, rows in the :attr:`self` tensor that are accumulated\n",
      " |      to are treated as if they were filled with the reduction identites.\n",
      " |\n",
      " |      The :attr:`dim`\\ th dimension of ``source`` must have the same size as the\n",
      " |      length of :attr:`index` (which must be a vector), and all other dimensions must\n",
      " |      match :attr:`self`, or an error will be raised.\n",
      " |\n",
      " |      For a 3-D tensor with :obj:`reduce=\"prod\"` and :obj:`include_self=True` the\n",
      " |      output is given as::\n",
      " |\n",
      " |          self[index[i], :, :] *= src[i, :, :]  # if dim == 0\n",
      " |          self[:, index[i], :] *= src[:, i, :]  # if dim == 1\n",
      " |          self[:, :, index[i]] *= src[:, :, i]  # if dim == 2\n",
      " |\n",
      " |      Note:\n",
      " |          This operation may behave nondeterministically when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          This function only supports floating point tensors.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          This function is in beta and may change in the near future.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int): dimension along which to index\n",
      " |          index (Tensor): indices of ``source`` to select from,\n",
      " |              should have dtype either `torch.int64` or `torch.int32`\n",
      " |          source (FloatTensor): the tensor containing values to accumulate\n",
      " |          reduce (str): the reduction operation to apply\n",
      " |              (:obj:`\"prod\"`, :obj:`\"mean\"`, :obj:`\"amax\"`, :obj:`\"amin\"`)\n",
      " |\n",
      " |      Keyword args:\n",
      " |          include_self (bool): whether the elements from the ``self`` tensor are\n",
      " |              included in the reduction\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.empty(5, 3).fill_(2)\n",
      " |          >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], dtype=torch.float)\n",
      " |          >>> index = torch.tensor([0, 4, 2, 0])\n",
      " |          >>> x.index_reduce_(0, index, t, 'prod')\n",
      " |          tensor([[20., 44., 72.],\n",
      " |                  [ 2.,  2.,  2.],\n",
      " |                  [14., 16., 18.],\n",
      " |                  [ 2.,  2.,  2.],\n",
      " |                  [ 8., 10., 12.]])\n",
      " |          >>> x = torch.empty(5, 3).fill_(2)\n",
      " |          >>> x.index_reduce_(0, index, t, 'prod', include_self=False)\n",
      " |          tensor([[10., 22., 36.],\n",
      " |                  [ 2.,  2.,  2.],\n",
      " |                  [ 7.,  8.,  9.],\n",
      " |                  [ 2.,  2.,  2.],\n",
      " |                  [ 4.,  5.,  6.]])\n",
      " |\n",
      " |  index_select(...)\n",
      " |      index_select(dim, index) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.index_select`\n",
      " |\n",
      " |  indices(...)\n",
      " |      indices() -> Tensor\n",
      " |\n",
      " |      Return the indices tensor of a :ref:`sparse COO tensor <sparse-coo-docs>`.\n",
      " |\n",
      " |      .. warning::\n",
      " |        Throws an error if :attr:`self` is not a sparse COO tensor.\n",
      " |\n",
      " |      See also :meth:`Tensor.values`.\n",
      " |\n",
      " |      .. note::\n",
      " |        This method can only be called on a coalesced sparse tensor. See\n",
      " |        :meth:`Tensor.coalesce` for details.\n",
      " |\n",
      " |  inner(...)\n",
      " |      inner(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.inner`.\n",
      " |\n",
      " |  int(...)\n",
      " |      int(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.int()`` is equivalent to ``self.to(torch.int32)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  int_repr(...)\n",
      " |      int_repr() -> Tensor\n",
      " |\n",
      " |      Given a quantized Tensor,\n",
      " |      ``self.int_repr()`` returns a CPU Tensor with uint8_t as data type that stores the\n",
      " |      underlying uint8_t values of the given Tensor.\n",
      " |\n",
      " |  inverse(...)\n",
      " |      inverse() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.inverse`\n",
      " |\n",
      " |  ipu(...)\n",
      " |      ipu(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      Returns a copy of this object in IPU memory.\n",
      " |\n",
      " |      If this object is already in IPU memory and on the correct device,\n",
      " |      then no copy is performed and the original object is returned.\n",
      " |\n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The destination IPU device.\n",
      " |              Defaults to the current IPU device.\n",
      " |          non_blocking (bool): If ``True`` and the source is in pinned memory,\n",
      " |              the copy will be asynchronous with respect to the host.\n",
      " |              Otherwise, the argument has no effect. Default: ``False``.\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  is_coalesced(...)\n",
      " |      is_coalesced() -> bool\n",
      " |\n",
      " |      Returns ``True`` if :attr:`self` is a :ref:`sparse COO tensor\n",
      " |      <sparse-coo-docs>` that is coalesced, ``False`` otherwise.\n",
      " |\n",
      " |      .. warning::\n",
      " |        Throws an error if :attr:`self` is not a sparse COO tensor.\n",
      " |\n",
      " |      See :meth:`coalesce` and :ref:`uncoalesced tensors <sparse-uncoalesced-coo-docs>`.\n",
      " |\n",
      " |  is_complex(...)\n",
      " |      is_complex() -> bool\n",
      " |\n",
      " |      Returns True if the data type of :attr:`self` is a complex data type.\n",
      " |\n",
      " |  is_conj(...)\n",
      " |      is_conj() -> bool\n",
      " |\n",
      " |      Returns True if the conjugate bit of :attr:`self` is set to true.\n",
      " |\n",
      " |  is_contiguous(...)\n",
      " |      is_contiguous(memory_format=torch.contiguous_format) -> bool\n",
      " |\n",
      " |      Returns True if :attr:`self` tensor is contiguous in memory in the order specified\n",
      " |      by memory format.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): Specifies memory allocation\n",
      " |              order. Default: ``torch.contiguous_format``.\n",
      " |\n",
      " |  is_distributed(...)\n",
      " |\n",
      " |  is_floating_point(...)\n",
      " |      is_floating_point() -> bool\n",
      " |\n",
      " |      Returns True if the data type of :attr:`self` is a floating point data type.\n",
      " |\n",
      " |  is_inference(...)\n",
      " |      is_inference() -> bool\n",
      " |\n",
      " |      See :func:`torch.is_inference`\n",
      " |\n",
      " |  is_neg(...)\n",
      " |      is_neg() -> bool\n",
      " |\n",
      " |      Returns True if the negative bit of :attr:`self` is set to true.\n",
      " |\n",
      " |  is_nonzero(...)\n",
      " |\n",
      " |  is_pinned(...)\n",
      " |      Returns true if this tensor resides in pinned memory.\n",
      " |      By default, the device pinned memory on will be the current :ref:`accelerator<accelerators>`.\n",
      " |\n",
      " |  is_same_size(...)\n",
      " |\n",
      " |  is_set_to(...)\n",
      " |      is_set_to(tensor) -> bool\n",
      " |\n",
      " |      Returns True if both tensors are pointing to the exact same memory (same\n",
      " |      storage, offset, size and stride).\n",
      " |\n",
      " |  is_signed(...)\n",
      " |      is_signed() -> bool\n",
      " |\n",
      " |      Returns True if the data type of :attr:`self` is a signed data type.\n",
      " |\n",
      " |  isclose(...)\n",
      " |      isclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.isclose`\n",
      " |\n",
      " |  isfinite(...)\n",
      " |      isfinite() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.isfinite`\n",
      " |\n",
      " |  isinf(...)\n",
      " |      isinf() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.isinf`\n",
      " |\n",
      " |  isnan(...)\n",
      " |      isnan() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.isnan`\n",
      " |\n",
      " |  isneginf(...)\n",
      " |      isneginf() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.isneginf`\n",
      " |\n",
      " |  isposinf(...)\n",
      " |      isposinf() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.isposinf`\n",
      " |\n",
      " |  isreal(...)\n",
      " |      isreal() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.isreal`\n",
      " |\n",
      " |  item(...)\n",
      " |      item() -> number\n",
      " |\n",
      " |      Returns the value of this tensor as a standard Python number. This only works\n",
      " |      for tensors with one element. For other cases, see :meth:`~Tensor.tolist`.\n",
      " |\n",
      " |      This operation is not differentiable.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.tensor([1.0])\n",
      " |          >>> x.item()\n",
      " |          1.0\n",
      " |\n",
      " |  kron(...)\n",
      " |      kron(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.kron`\n",
      " |\n",
      " |  kthvalue(...)\n",
      " |      kthvalue(k, dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
      " |\n",
      " |      See :func:`torch.kthvalue`\n",
      " |\n",
      " |  lcm(...)\n",
      " |      lcm(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.lcm`\n",
      " |\n",
      " |  lcm_(...)\n",
      " |      lcm_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.lcm`\n",
      " |\n",
      " |  ldexp(...)\n",
      " |      ldexp(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.ldexp`\n",
      " |\n",
      " |  ldexp_(...)\n",
      " |      ldexp_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.ldexp`\n",
      " |\n",
      " |  le(...)\n",
      " |      le(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.le`.\n",
      " |\n",
      " |  le_(...)\n",
      " |      le_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.le`.\n",
      " |\n",
      " |  lerp(...)\n",
      " |      lerp(end, weight) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.lerp`\n",
      " |\n",
      " |  lerp_(...)\n",
      " |      lerp_(end, weight) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.lerp`\n",
      " |\n",
      " |  less(...)\n",
      " |      lt(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.less`.\n",
      " |\n",
      " |  less_(...)\n",
      " |      less_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.less`.\n",
      " |\n",
      " |  less_equal(...)\n",
      " |      less_equal(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.less_equal`.\n",
      " |\n",
      " |  less_equal_(...)\n",
      " |      less_equal_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.less_equal`.\n",
      " |\n",
      " |  lgamma(...)\n",
      " |      lgamma() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.lgamma`\n",
      " |\n",
      " |  lgamma_(...)\n",
      " |      lgamma_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.lgamma`\n",
      " |\n",
      " |  log(...)\n",
      " |      log() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.log`\n",
      " |\n",
      " |  log10(...)\n",
      " |      log10() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.log10`\n",
      " |\n",
      " |  log10_(...)\n",
      " |      log10_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.log10`\n",
      " |\n",
      " |  log1p(...)\n",
      " |      log1p() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.log1p`\n",
      " |\n",
      " |  log1p_(...)\n",
      " |      log1p_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.log1p`\n",
      " |\n",
      " |  log2(...)\n",
      " |      log2() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.log2`\n",
      " |\n",
      " |  log2_(...)\n",
      " |      log2_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.log2`\n",
      " |\n",
      " |  log_(...)\n",
      " |      log_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.log`\n",
      " |\n",
      " |  log_normal_(...)\n",
      " |      log_normal_(mean=1, std=2, *, generator=None)\n",
      " |\n",
      " |      Fills :attr:`self` tensor with numbers samples from the log-normal distribution\n",
      " |      parameterized by the given mean :math:`\\mu` and standard deviation\n",
      " |      :math:`\\sigma`. Note that :attr:`mean` and :attr:`std` are the mean and\n",
      " |      standard deviation of the underlying normal distribution, and not of the\n",
      " |      returned distribution:\n",
      " |\n",
      " |      .. math::\n",
      " |\n",
      " |          f(x) = \\dfrac{1}{x \\sigma \\sqrt{2\\pi}}\\ e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}}\n",
      " |\n",
      " |  log_softmax(...)\n",
      " |\n",
      " |  logaddexp(...)\n",
      " |      logaddexp(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logaddexp`\n",
      " |\n",
      " |  logaddexp2(...)\n",
      " |      logaddexp2(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logaddexp2`\n",
      " |\n",
      " |  logcumsumexp(...)\n",
      " |      logcumsumexp(dim) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logcumsumexp`\n",
      " |\n",
      " |  logdet(...)\n",
      " |      logdet() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logdet`\n",
      " |\n",
      " |  logical_and(...)\n",
      " |      logical_and() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logical_and`\n",
      " |\n",
      " |  logical_and_(...)\n",
      " |      logical_and_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.logical_and`\n",
      " |\n",
      " |  logical_not(...)\n",
      " |      logical_not() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logical_not`\n",
      " |\n",
      " |  logical_not_(...)\n",
      " |      logical_not_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.logical_not`\n",
      " |\n",
      " |  logical_or(...)\n",
      " |      logical_or() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logical_or`\n",
      " |\n",
      " |  logical_or_(...)\n",
      " |      logical_or_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.logical_or`\n",
      " |\n",
      " |  logical_xor(...)\n",
      " |      logical_xor() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logical_xor`\n",
      " |\n",
      " |  logical_xor_(...)\n",
      " |      logical_xor_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.logical_xor`\n",
      " |\n",
      " |  logit(...)\n",
      " |      logit() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logit`\n",
      " |\n",
      " |  logit_(...)\n",
      " |      logit_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.logit`\n",
      " |\n",
      " |  logsumexp(...)\n",
      " |      logsumexp(dim, keepdim=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logsumexp`\n",
      " |\n",
      " |  long(...)\n",
      " |      long(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.long()`` is equivalent to ``self.to(torch.int64)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  lt(...)\n",
      " |      lt(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.lt`.\n",
      " |\n",
      " |  lt_(...)\n",
      " |      lt_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.lt`.\n",
      " |\n",
      " |  lu_solve(...)\n",
      " |      lu_solve(LU_data, LU_pivots) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.lu_solve`\n",
      " |\n",
      " |  map2_(...)\n",
      " |\n",
      " |  map_(...)\n",
      " |      map_(tensor, callable)\n",
      " |\n",
      " |      Applies :attr:`callable` for each element in :attr:`self` tensor and the given\n",
      " |      :attr:`tensor` and stores the results in :attr:`self` tensor. :attr:`self` tensor and\n",
      " |      the given :attr:`tensor` must be :ref:`broadcastable <broadcasting-semantics>`.\n",
      " |\n",
      " |      The :attr:`callable` should have the signature::\n",
      " |\n",
      " |          def callable(a, b) -> number\n",
      " |\n",
      " |  masked_fill(...)\n",
      " |      masked_fill(mask, value) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.masked_fill_`\n",
      " |\n",
      " |  masked_fill_(...)\n",
      " |      masked_fill_(mask, value)\n",
      " |\n",
      " |      Fills elements of :attr:`self` tensor with :attr:`value` where :attr:`mask` is\n",
      " |      True. The shape of :attr:`mask` must be\n",
      " |      :ref:`broadcastable <broadcasting-semantics>` with the shape of the underlying\n",
      " |      tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          mask (BoolTensor): the boolean mask\n",
      " |          value (float): the value to fill in with\n",
      " |\n",
      " |  masked_scatter(...)\n",
      " |      masked_scatter(mask, tensor) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.masked_scatter_`\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          The inputs :attr:`self` and :attr:`mask`\n",
      " |          :ref:`broadcast <broadcasting-semantics>`.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          >>> self = torch.tensor([0, 0, 0, 0, 0])\n",
      " |          >>> mask = torch.tensor([[0, 0, 0, 1, 1], [1, 1, 0, 1, 1]], dtype=torch.bool)\n",
      " |          >>> source = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n",
      " |          >>> self.masked_scatter(mask, source)\n",
      " |          tensor([[0, 0, 0, 0, 1],\n",
      " |                  [2, 3, 0, 4, 5]])\n",
      " |\n",
      " |  masked_scatter_(...)\n",
      " |      masked_scatter_(mask, source)\n",
      " |\n",
      " |      Copies elements from :attr:`source` into :attr:`self` tensor at positions where\n",
      " |      the :attr:`mask` is True. Elements from :attr:`source` are copied into :attr:`self`\n",
      " |      starting at position 0 of :attr:`source` and continuing in order one-by-one for each\n",
      " |      occurrence of :attr:`mask` being True.\n",
      " |      The shape of :attr:`mask` must be :ref:`broadcastable <broadcasting-semantics>`\n",
      " |      with the shape of the underlying tensor. The :attr:`source` should have at least\n",
      " |      as many elements as the number of ones in :attr:`mask`.\n",
      " |\n",
      " |      Args:\n",
      " |          mask (BoolTensor): the boolean mask\n",
      " |          source (Tensor): the tensor to copy from\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          The :attr:`mask` operates on the :attr:`self` tensor, not on the given\n",
      " |          :attr:`source` tensor.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          >>> self = torch.tensor([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]])\n",
      " |          >>> mask = torch.tensor([[0, 0, 0, 1, 1], [1, 1, 0, 1, 1]], dtype=torch.bool)\n",
      " |          >>> source = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n",
      " |          >>> self.masked_scatter_(mask, source)\n",
      " |          tensor([[0, 0, 0, 0, 1],\n",
      " |                  [2, 3, 0, 4, 5]])\n",
      " |\n",
      " |  masked_select(...)\n",
      " |      masked_select(mask) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.masked_select`\n",
      " |\n",
      " |  matmul(...)\n",
      " |      matmul(tensor2) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.matmul`\n",
      " |\n",
      " |  matrix_exp(...)\n",
      " |      matrix_exp() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.matrix_exp`\n",
      " |\n",
      " |  matrix_power(...)\n",
      " |      matrix_power(n) -> Tensor\n",
      " |\n",
      " |      .. note:: :meth:`~Tensor.matrix_power` is deprecated, use :func:`torch.linalg.matrix_power` instead.\n",
      " |\n",
      " |      Alias for :func:`torch.linalg.matrix_power`\n",
      " |\n",
      " |  max(...)\n",
      " |      max(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.max`\n",
      " |\n",
      " |  maximum(...)\n",
      " |      maximum(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.maximum`\n",
      " |\n",
      " |  mean(...)\n",
      " |      mean(dim=None, keepdim=False, *, dtype=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.mean`\n",
      " |\n",
      " |  median(...)\n",
      " |      median(dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
      " |\n",
      " |      See :func:`torch.median`\n",
      " |\n",
      " |  min(...)\n",
      " |      min(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.min`\n",
      " |\n",
      " |  minimum(...)\n",
      " |      minimum(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.minimum`\n",
      " |\n",
      " |  mm(...)\n",
      " |      mm(mat2) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.mm`\n",
      " |\n",
      " |  mode(...)\n",
      " |      mode(dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
      " |\n",
      " |      See :func:`torch.mode`\n",
      " |\n",
      " |  moveaxis(...)\n",
      " |      moveaxis(source, destination) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.moveaxis`\n",
      " |\n",
      " |  movedim(...)\n",
      " |      movedim(source, destination) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.movedim`\n",
      " |\n",
      " |  msort(...)\n",
      " |      msort() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.msort`\n",
      " |\n",
      " |  mtia(...)\n",
      " |      mtia(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      Returns a copy of this object in MTIA memory.\n",
      " |\n",
      " |      If this object is already in MTIA memory and on the correct device,\n",
      " |      then no copy is performed and the original object is returned.\n",
      " |\n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The destination MTIA device.\n",
      " |              Defaults to the current MTIA device.\n",
      " |          non_blocking (bool): If ``True`` and the source is in pinned memory,\n",
      " |              the copy will be asynchronous with respect to the host.\n",
      " |              Otherwise, the argument has no effect. Default: ``False``.\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  mul(...)\n",
      " |      mul(value) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.mul`.\n",
      " |\n",
      " |  mul_(...)\n",
      " |      mul_(value) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.mul`.\n",
      " |\n",
      " |  multinomial(...)\n",
      " |      multinomial(num_samples, replacement=False, *, generator=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.multinomial`\n",
      " |\n",
      " |  multiply(...)\n",
      " |      multiply(value) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.multiply`.\n",
      " |\n",
      " |  multiply_(...)\n",
      " |      multiply_(value) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.multiply`.\n",
      " |\n",
      " |  mv(...)\n",
      " |      mv(vec) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.mv`\n",
      " |\n",
      " |  mvlgamma(...)\n",
      " |      mvlgamma(p) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.mvlgamma`\n",
      " |\n",
      " |  mvlgamma_(...)\n",
      " |      mvlgamma_(p) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.mvlgamma`\n",
      " |\n",
      " |  nan_to_num(...)\n",
      " |      nan_to_num(nan=0.0, posinf=None, neginf=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.nan_to_num`.\n",
      " |\n",
      " |  nan_to_num_(...)\n",
      " |      nan_to_num_(nan=0.0, posinf=None, neginf=None) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.nan_to_num`.\n",
      " |\n",
      " |  nanmean(...)\n",
      " |      nanmean(dim=None, keepdim=False, *, dtype=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.nanmean`\n",
      " |\n",
      " |  nanmedian(...)\n",
      " |      nanmedian(dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
      " |\n",
      " |      See :func:`torch.nanmedian`\n",
      " |\n",
      " |  nanquantile(...)\n",
      " |      nanquantile(q, dim=None, keepdim=False, *, interpolation='linear') -> Tensor\n",
      " |\n",
      " |      See :func:`torch.nanquantile`\n",
      " |\n",
      " |  nansum(...)\n",
      " |      nansum(dim=None, keepdim=False, dtype=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.nansum`\n",
      " |\n",
      " |  narrow(...)\n",
      " |      narrow(dimension, start, length) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.narrow`.\n",
      " |\n",
      " |  narrow_copy(...)\n",
      " |      narrow_copy(dimension, start, length) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.narrow_copy`.\n",
      " |\n",
      " |  ndimension(...)\n",
      " |      ndimension() -> int\n",
      " |\n",
      " |      Alias for :meth:`~Tensor.dim()`\n",
      " |\n",
      " |  ne(...)\n",
      " |      ne(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.ne`.\n",
      " |\n",
      " |  ne_(...)\n",
      " |      ne_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.ne`.\n",
      " |\n",
      " |  neg(...)\n",
      " |      neg() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.neg`\n",
      " |\n",
      " |  neg_(...)\n",
      " |      neg_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.neg`\n",
      " |\n",
      " |  negative(...)\n",
      " |      negative() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.negative`\n",
      " |\n",
      " |  negative_(...)\n",
      " |      negative_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.negative`\n",
      " |\n",
      " |  nelement(...)\n",
      " |      nelement() -> int\n",
      " |\n",
      " |      Alias for :meth:`~Tensor.numel`\n",
      " |\n",
      " |  new(...)\n",
      " |\n",
      " |  new_empty(...)\n",
      " |      new_empty(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |\n",
      " |\n",
      " |      Returns a Tensor of size :attr:`size` filled with uninitialized data.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
      " |              shape of the output tensor.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> tensor = torch.ones(())\n",
      " |          >>> tensor.new_empty((2, 3))\n",
      " |          tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],\n",
      " |                  [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])\n",
      " |\n",
      " |  new_empty_strided(...)\n",
      " |      new_empty_strided(size, stride, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |\n",
      " |\n",
      " |      Returns a Tensor of size :attr:`size` and strides :attr:`stride` filled with\n",
      " |      uninitialized data. By default, the returned Tensor has the same\n",
      " |      :class:`torch.dtype` and :class:`torch.device` as this tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
      " |              shape of the output tensor.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> tensor = torch.ones(())\n",
      " |          >>> tensor.new_empty_strided((2, 3), (3, 1))\n",
      " |          tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],\n",
      " |                  [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])\n",
      " |\n",
      " |  new_full(...)\n",
      " |      new_full(size, fill_value, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |\n",
      " |\n",
      " |      Returns a Tensor of size :attr:`size` filled with :attr:`fill_value`.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          fill_value (scalar): the number to fill the output tensor with.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> tensor = torch.ones((2,), dtype=torch.float64)\n",
      " |          >>> tensor.new_full((3, 4), 3.141592)\n",
      " |          tensor([[ 3.1416,  3.1416,  3.1416,  3.1416],\n",
      " |                  [ 3.1416,  3.1416,  3.1416,  3.1416],\n",
      " |                  [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)\n",
      " |\n",
      " |  new_ones(...)\n",
      " |      new_ones(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |\n",
      " |\n",
      " |      Returns a Tensor of size :attr:`size` filled with ``1``.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
      " |              shape of the output tensor.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> tensor = torch.tensor((), dtype=torch.int32)\n",
      " |          >>> tensor.new_ones((2, 3))\n",
      " |          tensor([[ 1,  1,  1],\n",
      " |                  [ 1,  1,  1]], dtype=torch.int32)\n",
      " |\n",
      " |  new_tensor(...)\n",
      " |      new_tensor(data, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |\n",
      " |\n",
      " |      Returns a new Tensor with :attr:`data` as the tensor data.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          :func:`new_tensor` always copies :attr:`data`. If you have a Tensor\n",
      " |          ``data`` and want to avoid a copy, use :func:`torch.Tensor.requires_grad_`\n",
      " |          or :func:`torch.Tensor.detach`.\n",
      " |          If you have a numpy array and want to avoid a copy, use\n",
      " |          :func:`torch.from_numpy`.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          When data is a tensor `x`, :func:`new_tensor()` reads out 'the data' from whatever it is passed,\n",
      " |          and constructs a leaf variable. Therefore ``tensor.new_tensor(x)`` is equivalent to ``x.detach().clone()``\n",
      " |          and ``tensor.new_tensor(x, requires_grad=True)`` is equivalent to ``x.detach().clone().requires_grad_(True)``.\n",
      " |          The equivalents using ``detach()`` and ``clone()`` are recommended.\n",
      " |\n",
      " |      Args:\n",
      " |          data (array_like): The returned Tensor copies :attr:`data`.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> tensor = torch.ones((2,), dtype=torch.int8)\n",
      " |          >>> data = [[0, 1], [2, 3]]\n",
      " |          >>> tensor.new_tensor(data)\n",
      " |          tensor([[ 0,  1],\n",
      " |                  [ 2,  3]], dtype=torch.int8)\n",
      " |\n",
      " |  new_zeros(...)\n",
      " |      new_zeros(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |\n",
      " |\n",
      " |      Returns a Tensor of size :attr:`size` filled with ``0``.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
      " |              shape of the output tensor.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> tensor = torch.tensor((), dtype=torch.float64)\n",
      " |          >>> tensor.new_zeros((2, 3))\n",
      " |          tensor([[ 0.,  0.,  0.],\n",
      " |                  [ 0.,  0.,  0.]], dtype=torch.float64)\n",
      " |\n",
      " |  nextafter(...)\n",
      " |      nextafter(other) -> Tensor\n",
      " |      See :func:`torch.nextafter`\n",
      " |\n",
      " |  nextafter_(...)\n",
      " |      nextafter_(other) -> Tensor\n",
      " |      In-place version of :meth:`~Tensor.nextafter`\n",
      " |\n",
      " |  nonzero(...)\n",
      " |      nonzero() -> LongTensor\n",
      " |\n",
      " |      See :func:`torch.nonzero`\n",
      " |\n",
      " |  nonzero_static(...)\n",
      " |      nonzero_static(input, *, size, fill_value=-1) -> Tensor\n",
      " |\n",
      " |      Returns a 2-D tensor where each row is the index for a non-zero value.\n",
      " |      The returned Tensor has the same `torch.dtype` as `torch.nonzero()`.\n",
      " |\n",
      " |      Args:\n",
      " |          input (Tensor): the input tensor to count non-zero elements.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          size (int): the size of non-zero elements expected to be included in the out\n",
      " |              tensor. Pad the out tensor with `fill_value` if the `size` is larger\n",
      " |              than total number of non-zero elements, truncate out tensor if `size`\n",
      " |              is smaller. The size must be a non-negative integer.\n",
      " |          fill_value (int): the value to fill the output tensor with when `size` is larger\n",
      " |              than the total number of non-zero elements. Default is `-1` to represent\n",
      " |              invalid index.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          # Example 1: Padding\n",
      " |          >>> input_tensor = torch.tensor([[1, 0], [3, 2]])\n",
      " |          >>> static_size = 4\n",
      " |          >>> t = torch.nonzero_static(input_tensor, size = static_size)\n",
      " |          tensor([[  0,   0],\n",
      " |                  [  1,   0],\n",
      " |                  [  1,   1],\n",
      " |                  [  -1, -1]], dtype=torch.int64)\n",
      " |\n",
      " |          # Example 2: Truncating\n",
      " |          >>> input_tensor = torch.tensor([[1, 0], [3, 2]])\n",
      " |          >>> static_size = 2\n",
      " |          >>> t = torch.nonzero_static(input_tensor, size = static_size)\n",
      " |          tensor([[  0,   0],\n",
      " |                  [  1,   0]], dtype=torch.int64)\n",
      " |\n",
      " |          # Example 3: 0 size\n",
      " |          >>> input_tensor = torch.tensor([10])\n",
      " |          >>> static_size = 0\n",
      " |          >>> t = torch.nonzero_static(input_tensor, size = static_size)\n",
      " |          tensor([], size=(0, 1), dtype=torch.int64)\n",
      " |\n",
      " |          # Example 4: 0 rank input\n",
      " |          >>> input_tensor = torch.tensor(10)\n",
      " |          >>> static_size = 2\n",
      " |          >>> t = torch.nonzero_static(input_tensor, size = static_size)\n",
      " |          tensor([], size=(2, 0), dtype=torch.int64)\n",
      " |\n",
      " |  normal_(...)\n",
      " |      normal_(mean=0, std=1, *, generator=None) -> Tensor\n",
      " |\n",
      " |      Fills :attr:`self` tensor with elements samples from the normal distribution\n",
      " |      parameterized by :attr:`mean` and :attr:`std`.\n",
      " |\n",
      " |  not_equal(...)\n",
      " |      not_equal(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.not_equal`.\n",
      " |\n",
      " |  not_equal_(...)\n",
      " |      not_equal_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.not_equal`.\n",
      " |\n",
      " |  numel(...)\n",
      " |      numel() -> int\n",
      " |\n",
      " |      See :func:`torch.numel`\n",
      " |\n",
      " |  numpy(...)\n",
      " |      numpy(*, force=False) -> numpy.ndarray\n",
      " |\n",
      " |      Returns the tensor as a NumPy :class:`ndarray`.\n",
      " |\n",
      " |      If :attr:`force` is ``False`` (the default), the conversion\n",
      " |      is performed only if the tensor is on the CPU, does not require grad,\n",
      " |      does not have its conjugate bit set, and is a dtype and layout that\n",
      " |      NumPy supports. The returned ndarray and the tensor will share their\n",
      " |      storage, so changes to the tensor will be reflected in the ndarray\n",
      " |      and vice versa.\n",
      " |\n",
      " |      If :attr:`force` is ``True`` this is equivalent to\n",
      " |      calling ``t.detach().cpu().resolve_conj().resolve_neg().numpy()``.\n",
      " |      If the tensor isn't on the CPU or the conjugate or negative bit is set,\n",
      " |      the tensor won't share its storage with the returned ndarray.\n",
      " |      Setting :attr:`force` to ``True`` can be a useful shorthand.\n",
      " |\n",
      " |      Args:\n",
      " |          force (bool): if ``True``, the ndarray may be a copy of the tensor\n",
      " |                     instead of always sharing memory, defaults to ``False``.\n",
      " |\n",
      " |  orgqr(...)\n",
      " |      orgqr(input2) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.orgqr`\n",
      " |\n",
      " |  ormqr(...)\n",
      " |      ormqr(input2, input3, left=True, transpose=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.ormqr`\n",
      " |\n",
      " |  outer(...)\n",
      " |      outer(vec2) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.outer`.\n",
      " |\n",
      " |  permute(...)\n",
      " |      permute(*dims) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.permute`\n",
      " |\n",
      " |  pin_memory(...)\n",
      " |      pin_memory() -> Tensor\n",
      " |\n",
      " |      Copies the tensor to pinned memory, if it's not already pinned.\n",
      " |      By default, the device pinned memory on will be the current :ref:`accelerator<accelerators>`.\n",
      " |\n",
      " |  pinverse(...)\n",
      " |      pinverse() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.pinverse`\n",
      " |\n",
      " |  polygamma(...)\n",
      " |      polygamma(n) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.polygamma`\n",
      " |\n",
      " |  polygamma_(...)\n",
      " |      polygamma_(n) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.polygamma`\n",
      " |\n",
      " |  positive(...)\n",
      " |      positive() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.positive`\n",
      " |\n",
      " |  pow(...)\n",
      " |      pow(exponent) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.pow`\n",
      " |\n",
      " |  pow_(...)\n",
      " |      pow_(exponent) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.pow`\n",
      " |\n",
      " |  prelu(...)\n",
      " |\n",
      " |  prod(...)\n",
      " |      prod(dim=None, keepdim=False, dtype=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.prod`\n",
      " |\n",
      " |  put(...)\n",
      " |      put(input, index, source, accumulate=False) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.put_`.\n",
      " |      `input` corresponds to `self` in :meth:`torch.Tensor.put_`.\n",
      " |\n",
      " |  put_(...)\n",
      " |      put_(index, source, accumulate=False) -> Tensor\n",
      " |\n",
      " |      Copies the elements from :attr:`source` into the positions specified by\n",
      " |      :attr:`index`. For the purpose of indexing, the :attr:`self` tensor is treated as if\n",
      " |      it were a 1-D tensor.\n",
      " |\n",
      " |      :attr:`index` and :attr:`source` need to have the same number of elements, but not necessarily\n",
      " |      the same shape.\n",
      " |\n",
      " |      If :attr:`accumulate` is ``True``, the elements in :attr:`source` are added to\n",
      " |      :attr:`self`. If accumulate is ``False``, the behavior is undefined if :attr:`index`\n",
      " |      contain duplicate elements.\n",
      " |\n",
      " |      Args:\n",
      " |          index (LongTensor): the indices into self\n",
      " |          source (Tensor): the tensor containing values to copy from\n",
      " |          accumulate (bool): whether to accumulate into self\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> src = torch.tensor([[4, 3, 5],\n",
      " |          ...                     [6, 7, 8]])\n",
      " |          >>> src.put_(torch.tensor([1, 3]), torch.tensor([9, 10]))\n",
      " |          tensor([[  4,   9,   5],\n",
      " |                  [ 10,   7,   8]])\n",
      " |\n",
      " |  q_per_channel_axis(...)\n",
      " |      q_per_channel_axis() -> int\n",
      " |\n",
      " |      Given a Tensor quantized by linear (affine) per-channel quantization,\n",
      " |      returns the index of dimension on which per-channel quantization is applied.\n",
      " |\n",
      " |  q_per_channel_scales(...)\n",
      " |      q_per_channel_scales() -> Tensor\n",
      " |\n",
      " |      Given a Tensor quantized by linear (affine) per-channel quantization,\n",
      " |      returns a Tensor of scales of the underlying quantizer. It has the number of\n",
      " |      elements that matches the corresponding dimensions (from q_per_channel_axis) of\n",
      " |      the tensor.\n",
      " |\n",
      " |  q_per_channel_zero_points(...)\n",
      " |      q_per_channel_zero_points() -> Tensor\n",
      " |\n",
      " |      Given a Tensor quantized by linear (affine) per-channel quantization,\n",
      " |      returns a tensor of zero_points of the underlying quantizer. It has the number of\n",
      " |      elements that matches the corresponding dimensions (from q_per_channel_axis) of\n",
      " |      the tensor.\n",
      " |\n",
      " |  q_scale(...)\n",
      " |      q_scale() -> float\n",
      " |\n",
      " |      Given a Tensor quantized by linear(affine) quantization,\n",
      " |      returns the scale of the underlying quantizer().\n",
      " |\n",
      " |  q_zero_point(...)\n",
      " |      q_zero_point() -> int\n",
      " |\n",
      " |      Given a Tensor quantized by linear(affine) quantization,\n",
      " |      returns the zero_point of the underlying quantizer().\n",
      " |\n",
      " |  qr(...)\n",
      " |      qr(some=True) -> (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.qr`\n",
      " |\n",
      " |  qscheme(...)\n",
      " |      qscheme() -> torch.qscheme\n",
      " |\n",
      " |      Returns the quantization scheme of a given QTensor.\n",
      " |\n",
      " |  quantile(...)\n",
      " |      quantile(q, dim=None, keepdim=False, *, interpolation='linear') -> Tensor\n",
      " |\n",
      " |      See :func:`torch.quantile`\n",
      " |\n",
      " |  rad2deg(...)\n",
      " |      rad2deg() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.rad2deg`\n",
      " |\n",
      " |  rad2deg_(...)\n",
      " |      rad2deg_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.rad2deg`\n",
      " |\n",
      " |  random_(...)\n",
      " |      random_(from=0, to=None, *, generator=None) -> Tensor\n",
      " |\n",
      " |      Fills :attr:`self` tensor with numbers sampled from the discrete uniform\n",
      " |      distribution over ``[from, to - 1]``. If not specified, the values are usually\n",
      " |      only bounded by :attr:`self` tensor's data type. However, for floating point\n",
      " |      types, if unspecified, range will be ``[0, 2^mantissa]`` to ensure that every\n",
      " |      value is representable. For example, `torch.tensor(1, dtype=torch.double).random_()`\n",
      " |      will be uniform in ``[0, 2^53]``.\n",
      " |\n",
      " |  ravel(...)\n",
      " |      ravel() -> Tensor\n",
      " |\n",
      " |      see :func:`torch.ravel`\n",
      " |\n",
      " |  reciprocal(...)\n",
      " |      reciprocal() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.reciprocal`\n",
      " |\n",
      " |  reciprocal_(...)\n",
      " |      reciprocal_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.reciprocal`\n",
      " |\n",
      " |  record_stream(...)\n",
      " |      record_stream(stream)\n",
      " |\n",
      " |      Marks the tensor as having been used by this stream.  When the tensor\n",
      " |      is deallocated, ensure the tensor memory is not reused for another tensor\n",
      " |      until all work queued on :attr:`stream` at the time of deallocation is\n",
      " |      complete.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          The caching allocator is aware of only the stream where a tensor was\n",
      " |          allocated. Due to the awareness, it already correctly manages the life\n",
      " |          cycle of tensors on only one stream. But if a tensor is used on a stream\n",
      " |          different from the stream of origin, the allocator might reuse the memory\n",
      " |          unexpectedly. Calling this method lets the allocator know which streams\n",
      " |          have used the tensor.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          This method is most suitable for use cases where you are providing a\n",
      " |          function that created a tensor on a side stream, and want users to be able\n",
      " |          to make use of the tensor without having to think carefully about stream\n",
      " |          safety when making use of them.  These safety guarantees come at some\n",
      " |          performance and predictability cost (analogous to the tradeoff between GC\n",
      " |          and manual memory management), so if you are in a situation where\n",
      " |          you manage the full lifetime of your tensors, you may consider instead\n",
      " |          manually managing CUDA events so that calling this method is not necessary.\n",
      " |          In particular, when you call this method, on later allocations the\n",
      " |          allocator will poll the recorded stream to see if all operations have\n",
      " |          completed yet; you can potentially race with side stream computation and\n",
      " |          non-deterministically reuse or fail to reuse memory for an allocation.\n",
      " |\n",
      " |          You can safely use tensors allocated on side streams without\n",
      " |          :meth:`~Tensor.record_stream`; you must manually ensure that\n",
      " |          any non-creation stream uses of a tensor are synced back to the creation\n",
      " |          stream before you deallocate the tensor.  As the CUDA caching allocator\n",
      " |          guarantees that the memory will only be reused with the same creation stream,\n",
      " |          this is sufficient to ensure that writes to future reallocations of the\n",
      " |          memory will be delayed until non-creation stream uses are done.\n",
      " |          (Counterintuitively, you may observe that on the CPU side we have already\n",
      " |          reallocated the tensor, even though CUDA kernels on the old tensor are\n",
      " |          still in progress.  This is fine, because CUDA operations on the new\n",
      " |          tensor will appropriately wait for the old operations to complete, as they\n",
      " |          are all on the same stream.)\n",
      " |\n",
      " |          Concretely, this looks like this::\n",
      " |\n",
      " |              with torch.cuda.stream(s0):\n",
      " |                  x = torch.zeros(N)\n",
      " |\n",
      " |              s1.wait_stream(s0)\n",
      " |              with torch.cuda.stream(s1):\n",
      " |                  y = some_comm_op(x)\n",
      " |\n",
      " |              ... some compute on s0 ...\n",
      " |\n",
      " |              # synchronize creation stream s0 to side stream s1\n",
      " |              # before deallocating x\n",
      " |              s0.wait_stream(s1)\n",
      " |              del x\n",
      " |\n",
      " |          Note that some discretion is required when deciding when to perform\n",
      " |          ``s0.wait_stream(s1)``.  In particular, if we were to wait immediately\n",
      " |          after ``some_comm_op``, there wouldn't be any point in having the side\n",
      " |          stream; it would be equivalent to have run ``some_comm_op`` on ``s0``.\n",
      " |          Instead, the synchronization must be placed at some appropriate, later\n",
      " |          point in time where you expect the side stream ``s1`` to have finished\n",
      " |          work.  This location is typically identified via profiling, e.g., using\n",
      " |          Chrome traces produced\n",
      " |          :meth:`torch.autograd.profiler.profile.export_chrome_trace`.  If you\n",
      " |          place the wait too early, work on s0 will block until ``s1`` has finished,\n",
      " |          preventing further overlapping of communication and computation.  If you\n",
      " |          place the wait too late, you will use more memory than is strictly\n",
      " |          necessary (as you are keeping ``x`` live for longer.)  For a concrete\n",
      " |          example of how this guidance can be applied in practice, see this post:\n",
      " |          `FSDP and CUDACachingAllocator\n",
      " |          <https://dev-discuss.pytorch.org/t/fsdp-cudacachingallocator-an-outsider-newb-perspective/1486>`_.\n",
      " |\n",
      " |  relu(...)\n",
      " |\n",
      " |  relu_(...)\n",
      " |\n",
      " |  remainder(...)\n",
      " |      remainder(divisor) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.remainder`\n",
      " |\n",
      " |  remainder_(...)\n",
      " |      remainder_(divisor) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.remainder`\n",
      " |\n",
      " |  renorm(...)\n",
      " |      renorm(p, dim, maxnorm) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.renorm`\n",
      " |\n",
      " |  renorm_(...)\n",
      " |      renorm_(p, dim, maxnorm) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.renorm`\n",
      " |\n",
      " |  repeat(...)\n",
      " |      repeat(*repeats) -> Tensor\n",
      " |\n",
      " |      Repeats this tensor along the specified dimensions.\n",
      " |\n",
      " |      Unlike :meth:`~Tensor.expand`, this function copies the tensor's data.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          :meth:`~Tensor.repeat` behaves differently from\n",
      " |          `numpy.repeat <https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html>`_,\n",
      " |          but is more similar to\n",
      " |          `numpy.tile <https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html>`_.\n",
      " |          For the operator similar to `numpy.repeat`, see :func:`torch.repeat_interleave`.\n",
      " |\n",
      " |      Args:\n",
      " |          repeat (torch.Size, int..., tuple of int or list of int): The number of times to repeat this tensor along each dimension\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.tensor([1, 2, 3])\n",
      " |          >>> x.repeat(4, 2)\n",
      " |          tensor([[ 1,  2,  3,  1,  2,  3],\n",
      " |                  [ 1,  2,  3,  1,  2,  3],\n",
      " |                  [ 1,  2,  3,  1,  2,  3],\n",
      " |                  [ 1,  2,  3,  1,  2,  3]])\n",
      " |          >>> x.repeat(4, 2, 1).size()\n",
      " |          torch.Size([4, 2, 3])\n",
      " |\n",
      " |  repeat_interleave(...)\n",
      " |      repeat_interleave(repeats, dim=None, *, output_size=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.repeat_interleave`.\n",
      " |\n",
      " |  requires_grad_(...)\n",
      " |      requires_grad_(requires_grad=True) -> Tensor\n",
      " |\n",
      " |      Change if autograd should record operations on this tensor: sets this tensor's\n",
      " |      :attr:`requires_grad` attribute in-place. Returns this tensor.\n",
      " |\n",
      " |      :func:`requires_grad_`'s main use case is to tell autograd to begin recording\n",
      " |      operations on a Tensor ``tensor``. If ``tensor`` has ``requires_grad=False``\n",
      " |      (because it was obtained through a DataLoader, or required preprocessing or\n",
      " |      initialization), ``tensor.requires_grad_()`` makes it so that autograd will\n",
      " |      begin to record operations on ``tensor``.\n",
      " |\n",
      " |      Args:\n",
      " |          requires_grad (bool): If autograd should record operations on this tensor.\n",
      " |              Default: ``True``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # Let's say we want to preprocess some saved weights and use\n",
      " |          >>> # the result as new weights.\n",
      " |          >>> saved_weights = [0.1, 0.2, 0.3, 0.25]\n",
      " |          >>> loaded_weights = torch.tensor(saved_weights)\n",
      " |          >>> weights = preprocess(loaded_weights)  # some function\n",
      " |          >>> weights\n",
      " |          tensor([-0.5503,  0.4926, -2.1158, -0.8303])\n",
      " |\n",
      " |          >>> # Now, start to record operations done to weights\n",
      " |          >>> weights.requires_grad_()\n",
      " |          >>> out = weights.pow(2).sum()\n",
      " |          >>> out.backward()\n",
      " |          >>> weights.grad\n",
      " |          tensor([-1.1007,  0.9853, -4.2316, -1.6606])\n",
      " |\n",
      " |  reshape(...)\n",
      " |      reshape(*shape) -> Tensor\n",
      " |\n",
      " |      Returns a tensor with the same data and number of elements as :attr:`self`\n",
      " |      but with the specified shape. This method returns a view if :attr:`shape` is\n",
      " |      compatible with the current shape. See :meth:`torch.Tensor.view` on when it is\n",
      " |      possible to return a view.\n",
      " |\n",
      " |      See :func:`torch.reshape`\n",
      " |\n",
      " |      Args:\n",
      " |          shape (tuple of ints or int...): the desired shape\n",
      " |\n",
      " |  reshape_as(...)\n",
      " |      reshape_as(other) -> Tensor\n",
      " |\n",
      " |      Returns this tensor as the same shape as :attr:`other`.\n",
      " |      ``self.reshape_as(other)`` is equivalent to ``self.reshape(other.sizes())``.\n",
      " |      This method returns a view if ``other.sizes()`` is compatible with the current\n",
      " |      shape. See :meth:`torch.Tensor.view` on when it is possible to return a view.\n",
      " |\n",
      " |      Please see :meth:`reshape` for more information about ``reshape``.\n",
      " |\n",
      " |      Args:\n",
      " |          other (:class:`torch.Tensor`): The result tensor has the same shape\n",
      " |              as :attr:`other`.\n",
      " |\n",
      " |  resize_(...)\n",
      " |      resize_(*sizes, memory_format=torch.contiguous_format) -> Tensor\n",
      " |\n",
      " |      Resizes :attr:`self` tensor to the specified size. If the number of elements is\n",
      " |      larger than the current storage size, then the underlying storage is resized\n",
      " |      to fit the new number of elements. If the number of elements is smaller, the\n",
      " |      underlying storage is not changed. Existing elements are preserved but any new\n",
      " |      memory is uninitialized.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          This is a low-level method. The storage is reinterpreted as C-contiguous,\n",
      " |          ignoring the current strides (unless the target size equals the current\n",
      " |          size, in which case the tensor is left unchanged). For most purposes, you\n",
      " |          will instead want to use :meth:`~Tensor.view()`, which checks for\n",
      " |          contiguity, or :meth:`~Tensor.reshape()`, which copies data if needed. To\n",
      " |          change the size in-place with custom strides, see :meth:`~Tensor.set_()`.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          If :func:`torch.use_deterministic_algorithms()` and\n",
      " |          :attr:`torch.utils.deterministic.fill_uninitialized_memory` are both set to\n",
      " |          ``True``, new elements are initialized to prevent nondeterministic behavior\n",
      " |          from using the result as an input to an operation. Floating point and\n",
      " |          complex values are set to NaN, and integer values are set to the maximum\n",
      " |          value.\n",
      " |\n",
      " |      Args:\n",
      " |          sizes (torch.Size or int...): the desired size\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              Tensor. Default: ``torch.contiguous_format``. Note that memory format of\n",
      " |              :attr:`self` is going to be unaffected if ``self.size()`` matches ``sizes``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
      " |          >>> x.resize_(2, 2)\n",
      " |          tensor([[ 1,  2],\n",
      " |                  [ 3,  4]])\n",
      " |\n",
      " |  resize_as_(...)\n",
      " |      resize_as_(tensor, memory_format=torch.contiguous_format) -> Tensor\n",
      " |\n",
      " |      Resizes the :attr:`self` tensor to be the same size as the specified\n",
      " |      :attr:`tensor`. This is equivalent to ``self.resize_(tensor.size())``.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              Tensor. Default: ``torch.contiguous_format``. Note that memory format of\n",
      " |              :attr:`self` is going to be unaffected if ``self.size()`` matches ``tensor.size()``.\n",
      " |\n",
      " |  resize_as_sparse_(...)\n",
      " |\n",
      " |  resolve_conj(...)\n",
      " |      resolve_conj() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.resolve_conj`\n",
      " |\n",
      " |  resolve_neg(...)\n",
      " |      resolve_neg() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.resolve_neg`\n",
      " |\n",
      " |  retain_grad(...)\n",
      " |      retain_grad() -> None\n",
      " |\n",
      " |      Enables this Tensor to have their :attr:`grad` populated during\n",
      " |      :func:`backward`. This is a no-op for leaf tensors.\n",
      " |\n",
      " |  roll(...)\n",
      " |      roll(shifts, dims) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.roll`\n",
      " |\n",
      " |  rot90(...)\n",
      " |      rot90(k, dims) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.rot90`\n",
      " |\n",
      " |  round(...)\n",
      " |      round(decimals=0) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.round`\n",
      " |\n",
      " |  round_(...)\n",
      " |      round_(decimals=0) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.round`\n",
      " |\n",
      " |  row_indices(...)\n",
      " |\n",
      " |  rsqrt(...)\n",
      " |      rsqrt() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.rsqrt`\n",
      " |\n",
      " |  rsqrt_(...)\n",
      " |      rsqrt_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.rsqrt`\n",
      " |\n",
      " |  scatter(...)\n",
      " |      scatter(dim, index, src) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.scatter_`\n",
      " |\n",
      " |  scatter_(...)\n",
      " |      scatter_(dim, index, src, *, reduce=None) -> Tensor\n",
      " |\n",
      " |      Writes all values from the tensor :attr:`src` into :attr:`self` at the indices\n",
      " |      specified in the :attr:`index` tensor. For each value in :attr:`src`, its output\n",
      " |      index is specified by its index in :attr:`src` for ``dimension != dim`` and by\n",
      " |      the corresponding value in :attr:`index` for ``dimension = dim``.\n",
      " |\n",
      " |      For a 3-D tensor, :attr:`self` is updated as::\n",
      " |\n",
      " |          self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\n",
      " |          self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\n",
      " |          self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n",
      " |\n",
      " |      This is the reverse operation of the manner described in :meth:`~Tensor.gather`.\n",
      " |\n",
      " |      :attr:`self`, :attr:`index` and :attr:`src` (if it is a Tensor) should all have\n",
      " |      the same number of dimensions. It is also required that\n",
      " |      ``index.size(d) <= src.size(d)`` for all dimensions ``d``, and that\n",
      " |      ``index.size(d) <= self.size(d)`` for all dimensions ``d != dim``.\n",
      " |      Note that ``index`` and ``src`` do not broadcast.\n",
      " |\n",
      " |      Moreover, as for :meth:`~Tensor.gather`, the values of :attr:`index` must be\n",
      " |      between ``0`` and ``self.size(dim) - 1`` inclusive.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          When indices are not unique, the behavior is non-deterministic (one of the\n",
      " |          values from ``src`` will be picked arbitrarily) and the gradient will be\n",
      " |          incorrect (it will be propagated to all locations in the source that\n",
      " |          correspond to the same index)!\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          The backward pass is implemented only for ``src.shape == index.shape``.\n",
      " |\n",
      " |      Additionally accepts an optional :attr:`reduce` argument that allows\n",
      " |      specification of an optional reduction operation, which is applied to all\n",
      " |      values in the tensor :attr:`src` into :attr:`self` at the indices\n",
      " |      specified in the :attr:`index`. For each value in :attr:`src`, the reduction\n",
      " |      operation is applied to an index in :attr:`self` which is specified by\n",
      " |      its index in :attr:`src` for ``dimension != dim`` and by the corresponding\n",
      " |      value in :attr:`index` for ``dimension = dim``.\n",
      " |\n",
      " |      Given a 3-D tensor and reduction using the multiplication operation, :attr:`self`\n",
      " |      is updated as::\n",
      " |\n",
      " |          self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0\n",
      " |          self[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1\n",
      " |          self[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2\n",
      " |\n",
      " |      Reducing with the addition operation is the same as using\n",
      " |      :meth:`~torch.Tensor.scatter_add_`.\n",
      " |\n",
      " |      .. warning::\n",
      " |          The reduce argument with Tensor ``src`` is deprecated and will be removed in\n",
      " |          a future PyTorch release. Please use :meth:`~torch.Tensor.scatter_reduce_`\n",
      " |          instead for more reduction options.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int): the axis along which to index\n",
      " |          index (LongTensor): the indices of elements to scatter, can be either empty\n",
      " |              or of the same dimensionality as ``src``. When empty, the operation\n",
      " |              returns ``self`` unchanged.\n",
      " |          src (Tensor): the source element(s) to scatter.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          reduce (str, optional): reduction operation to apply, can be either\n",
      " |              ``'add'`` or ``'multiply'``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> src = torch.arange(1, 11).reshape((2, 5))\n",
      " |          >>> src\n",
      " |          tensor([[ 1,  2,  3,  4,  5],\n",
      " |                  [ 6,  7,  8,  9, 10]])\n",
      " |          >>> index = torch.tensor([[0, 1, 2, 0]])\n",
      " |          >>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)\n",
      " |          tensor([[1, 0, 0, 4, 0],\n",
      " |                  [0, 2, 0, 0, 0],\n",
      " |                  [0, 0, 3, 0, 0]])\n",
      " |          >>> index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n",
      " |          >>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)\n",
      " |          tensor([[1, 2, 3, 0, 0],\n",
      " |                  [6, 7, 0, 0, 8],\n",
      " |                  [0, 0, 0, 0, 0]])\n",
      " |\n",
      " |          >>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
      " |          ...            1.23, reduce='multiply')\n",
      " |          tensor([[2.0000, 2.0000, 2.4600, 2.0000],\n",
      " |                  [2.0000, 2.0000, 2.0000, 2.4600]])\n",
      " |          >>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
      " |          ...            1.23, reduce='add')\n",
      " |          tensor([[2.0000, 2.0000, 3.2300, 2.0000],\n",
      " |                  [2.0000, 2.0000, 2.0000, 3.2300]])\n",
      " |\n",
      " |      .. function:: scatter_(dim, index, value, *, reduce=None) -> Tensor:\n",
      " |         :noindex:\n",
      " |\n",
      " |      Writes the value from :attr:`value` into :attr:`self` at the indices\n",
      " |      specified in the :attr:`index` tensor.  This operation is equivalent to the previous version,\n",
      " |      with the :attr:`src` tensor filled entirely with :attr:`value`.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int): the axis along which to index\n",
      " |          index (LongTensor): the indices of elements to scatter, can be either empty\n",
      " |              or of the same dimensionality as ``src``. When empty, the operation\n",
      " |              returns ``self`` unchanged.\n",
      " |          value (Scalar): the value to scatter.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          reduce (str, optional): reduction operation to apply, can be either\n",
      " |              ``'add'`` or ``'multiply'``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> index = torch.tensor([[0, 1]])\n",
      " |          >>> value = 2\n",
      " |          >>> torch.zeros(3, 5).scatter_(0, index, value)\n",
      " |          tensor([[2., 0., 0., 0., 0.],\n",
      " |                  [0., 2., 0., 0., 0.],\n",
      " |                  [0., 0., 0., 0., 0.]])\n",
      " |\n",
      " |  scatter_add(...)\n",
      " |      scatter_add(dim, index, src) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.scatter_add_`\n",
      " |\n",
      " |  scatter_add_(...)\n",
      " |      scatter_add_(dim, index, src) -> Tensor\n",
      " |\n",
      " |      Adds all values from the tensor :attr:`src` into :attr:`self` at the indices\n",
      " |      specified in the :attr:`index` tensor in a similar fashion as\n",
      " |      :meth:`~torch.Tensor.scatter_`. For each value in :attr:`src`, it is added to\n",
      " |      an index in :attr:`self` which is specified by its index in :attr:`src`\n",
      " |      for ``dimension != dim`` and by the corresponding value in :attr:`index` for\n",
      " |      ``dimension = dim``.\n",
      " |\n",
      " |      For a 3-D tensor, :attr:`self` is updated as::\n",
      " |\n",
      " |          self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\n",
      " |          self[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\n",
      " |          self[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2\n",
      " |\n",
      " |      :attr:`self`, :attr:`index` and :attr:`src` should have same number of\n",
      " |      dimensions. It is also required that ``index.size(d) <= src.size(d)`` for all\n",
      " |      dimensions ``d``, and that ``index.size(d) <= self.size(d)`` for all dimensions\n",
      " |      ``d != dim``. Note that ``index`` and ``src`` do not broadcast.\n",
      " |\n",
      " |      Note:\n",
      " |          This operation may behave nondeterministically when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          The backward pass is implemented only for ``src.shape == index.shape``.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int): the axis along which to index\n",
      " |          index (LongTensor): the indices of elements to scatter and add, can be\n",
      " |              either empty or of the same dimensionality as ``src``. When empty, the\n",
      " |              operation returns ``self`` unchanged.\n",
      " |          src (Tensor): the source elements to scatter and add\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> src = torch.ones((2, 5))\n",
      " |          >>> index = torch.tensor([[0, 1, 2, 0, 0]])\n",
      " |          >>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\n",
      " |          tensor([[1., 0., 0., 1., 1.],\n",
      " |                  [0., 1., 0., 0., 0.],\n",
      " |                  [0., 0., 1., 0., 0.]])\n",
      " |          >>> index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]])\n",
      " |          >>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\n",
      " |          tensor([[2., 0., 0., 1., 1.],\n",
      " |                  [0., 2., 0., 0., 0.],\n",
      " |                  [0., 0., 2., 1., 1.]])\n",
      " |\n",
      " |  scatter_reduce(...)\n",
      " |      scatter_reduce(dim, index, src, reduce, *, include_self=True) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.scatter_reduce_`\n",
      " |\n",
      " |  scatter_reduce_(...)\n",
      " |      scatter_reduce_(dim, index, src, reduce, *, include_self=True) -> Tensor\n",
      " |\n",
      " |      Reduces all values from the :attr:`src` tensor to the indices specified in\n",
      " |      the :attr:`index` tensor in the :attr:`self` tensor using the applied reduction\n",
      " |      defined via the :attr:`reduce` argument (:obj:`\"sum\"`, :obj:`\"prod\"`, :obj:`\"mean\"`,\n",
      " |      :obj:`\"amax\"`, :obj:`\"amin\"`). For each value in :attr:`src`, it is reduced to an\n",
      " |      index in :attr:`self` which is specified by its index in :attr:`src` for\n",
      " |      ``dimension != dim`` and by the corresponding value in :attr:`index` for\n",
      " |      ``dimension = dim``. If :obj:`include_self=\"True\"`, the values in the :attr:`self`\n",
      " |      tensor are included in the reduction.\n",
      " |\n",
      " |      :attr:`self`, :attr:`index` and :attr:`src` should all have\n",
      " |      the same number of dimensions. It is also required that\n",
      " |      ``index.size(d) <= src.size(d)`` for all dimensions ``d``, and that\n",
      " |      ``index.size(d) <= self.size(d)`` for all dimensions ``d != dim``.\n",
      " |      Note that ``index`` and ``src`` do not broadcast.\n",
      " |\n",
      " |      For a 3-D tensor with :obj:`reduce=\"sum\"` and :obj:`include_self=True` the\n",
      " |      output is given as::\n",
      " |\n",
      " |          self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\n",
      " |          self[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\n",
      " |          self[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2\n",
      " |\n",
      " |      Note:\n",
      " |          This operation may behave nondeterministically when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          The backward pass is implemented only for ``src.shape == index.shape``.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          This function is in beta and may change in the near future.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int): the axis along which to index\n",
      " |          index (LongTensor): the indices of elements to scatter and reduce.\n",
      " |          src (Tensor): the source elements to scatter and reduce\n",
      " |          reduce (str): the reduction operation to apply for non-unique indices\n",
      " |              (:obj:`\"sum\"`, :obj:`\"prod\"`, :obj:`\"mean\"`, :obj:`\"amax\"`, :obj:`\"amin\"`)\n",
      " |          include_self (bool): whether elements from the :attr:`self` tensor are\n",
      " |              included in the reduction\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> src = torch.tensor([1., 2., 3., 4., 5., 6.])\n",
      " |          >>> index = torch.tensor([0, 1, 0, 1, 2, 1])\n",
      " |          >>> input = torch.tensor([1., 2., 3., 4.])\n",
      " |          >>> input.scatter_reduce(0, index, src, reduce=\"sum\")\n",
      " |          tensor([5., 14., 8., 4.])\n",
      " |          >>> input.scatter_reduce(0, index, src, reduce=\"sum\", include_self=False)\n",
      " |          tensor([4., 12., 5., 4.])\n",
      " |          >>> input2 = torch.tensor([5., 4., 3., 2.])\n",
      " |          >>> input2.scatter_reduce(0, index, src, reduce=\"amax\")\n",
      " |          tensor([5., 6., 5., 2.])\n",
      " |          >>> input2.scatter_reduce(0, index, src, reduce=\"amax\", include_self=False)\n",
      " |          tensor([3., 6., 5., 2.])\n",
      " |\n",
      " |  select(...)\n",
      " |      select(dim, index) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.select`\n",
      " |\n",
      " |  select_scatter(...)\n",
      " |      select_scatter(src, dim, index) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.select_scatter`\n",
      " |\n",
      " |  set_(...)\n",
      " |      set_(source=None, storage_offset=0, size=None, stride=None) -> Tensor\n",
      " |\n",
      " |      Sets the underlying storage, size, and strides. If :attr:`source` is a tensor,\n",
      " |      :attr:`self` tensor will share the same storage and have the same size and\n",
      " |      strides as :attr:`source`. Changes to elements in one tensor will be reflected\n",
      " |      in the other.\n",
      " |\n",
      " |      If :attr:`source` is a :class:`~torch.Storage`, the method sets the underlying\n",
      " |      storage, offset, size, and stride.\n",
      " |\n",
      " |      Args:\n",
      " |          source (Tensor or Storage): the tensor or storage to use\n",
      " |          storage_offset (int, optional): the offset in the storage\n",
      " |          size (torch.Size, optional): the desired size. Defaults to the size of the source.\n",
      " |          stride (tuple, optional): the desired stride. Defaults to C-contiguous strides.\n",
      " |\n",
      " |  sgn(...)\n",
      " |      sgn() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sgn`\n",
      " |\n",
      " |  sgn_(...)\n",
      " |      sgn_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.sgn`\n",
      " |\n",
      " |  short(...)\n",
      " |      short(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.short()`` is equivalent to ``self.to(torch.int16)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  sigmoid(...)\n",
      " |      sigmoid() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sigmoid`\n",
      " |\n",
      " |  sigmoid_(...)\n",
      " |      sigmoid_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.sigmoid`\n",
      " |\n",
      " |  sign(...)\n",
      " |      sign() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sign`\n",
      " |\n",
      " |  sign_(...)\n",
      " |      sign_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.sign`\n",
      " |\n",
      " |  signbit(...)\n",
      " |      signbit() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.signbit`\n",
      " |\n",
      " |  sin(...)\n",
      " |      sin() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sin`\n",
      " |\n",
      " |  sin_(...)\n",
      " |      sin_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.sin`\n",
      " |\n",
      " |  sinc(...)\n",
      " |      sinc() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sinc`\n",
      " |\n",
      " |  sinc_(...)\n",
      " |      sinc_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.sinc`\n",
      " |\n",
      " |  sinh(...)\n",
      " |      sinh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sinh`\n",
      " |\n",
      " |  sinh_(...)\n",
      " |      sinh_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.sinh`\n",
      " |\n",
      " |  size(...)\n",
      " |      size(dim=None) -> torch.Size or int\n",
      " |\n",
      " |      Returns the size of the :attr:`self` tensor. If ``dim`` is not specified,\n",
      " |      the returned value is a :class:`torch.Size`, a subclass of :class:`tuple`.\n",
      " |      If ``dim`` is specified, returns an int holding the size of that dimension.\n",
      " |\n",
      " |      Args:\n",
      " |        dim (int, optional): The dimension for which to retrieve the size.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> t = torch.empty(3, 4, 5)\n",
      " |          >>> t.size()\n",
      " |          torch.Size([3, 4, 5])\n",
      " |          >>> t.size(dim=1)\n",
      " |          4\n",
      " |\n",
      " |  slice_inverse(...)\n",
      " |\n",
      " |  slice_scatter(...)\n",
      " |      slice_scatter(src, dim=0, start=None, end=None, step=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.slice_scatter`\n",
      " |\n",
      " |  slogdet(...)\n",
      " |      slogdet() -> (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.slogdet`\n",
      " |\n",
      " |  smm(...)\n",
      " |      smm(mat) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.smm`\n",
      " |\n",
      " |  softmax(...)\n",
      " |      softmax(dim) -> Tensor\n",
      " |\n",
      " |      Alias for :func:`torch.nn.functional.softmax`.\n",
      " |\n",
      " |  sort(...)\n",
      " |      sort(dim=-1, descending=False) -> (Tensor, LongTensor)\n",
      " |\n",
      " |      See :func:`torch.sort`\n",
      " |\n",
      " |  sparse_dim(...)\n",
      " |      sparse_dim() -> int\n",
      " |\n",
      " |      Return the number of sparse dimensions in a :ref:`sparse tensor <sparse-docs>` :attr:`self`.\n",
      " |\n",
      " |      .. note::\n",
      " |        Returns ``0`` if :attr:`self` is not a sparse tensor.\n",
      " |\n",
      " |      See also :meth:`Tensor.dense_dim` and :ref:`hybrid tensors <sparse-hybrid-coo-docs>`.\n",
      " |\n",
      " |  sparse_mask(...)\n",
      " |      sparse_mask(mask) -> Tensor\n",
      " |\n",
      " |      Returns a new :ref:`sparse tensor <sparse-docs>` with values from a\n",
      " |      strided tensor :attr:`self` filtered by the indices of the sparse\n",
      " |      tensor :attr:`mask`. The values of :attr:`mask` sparse tensor are\n",
      " |      ignored. :attr:`self` and :attr:`mask` tensors must have the same\n",
      " |      shape.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |        The returned sparse tensor might contain duplicate values if :attr:`mask`\n",
      " |        is not coalesced. It is therefore advisable to pass ``mask.coalesce()``\n",
      " |        if such behavior is not desired.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |        The returned sparse tensor has the same indices as the sparse tensor\n",
      " |        :attr:`mask`, even when the corresponding values in :attr:`self` are\n",
      " |        zeros.\n",
      " |\n",
      " |      Args:\n",
      " |          mask (Tensor): a sparse tensor whose indices are used as a filter\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> nse = 5\n",
      " |          >>> dims = (5, 5, 2, 2)\n",
      " |          >>> I = torch.cat([torch.randint(0, dims[0], size=(nse,)),\n",
      " |          ...                torch.randint(0, dims[1], size=(nse,))], 0).reshape(2, nse)\n",
      " |          >>> V = torch.randn(nse, dims[2], dims[3])\n",
      " |          >>> S = torch.sparse_coo_tensor(I, V, dims).coalesce()\n",
      " |          >>> D = torch.randn(dims)\n",
      " |          >>> D.sparse_mask(S)\n",
      " |          tensor(indices=tensor([[0, 0, 0, 2],\n",
      " |                                 [0, 1, 4, 3]]),\n",
      " |                 values=tensor([[[ 1.6550,  0.2397],\n",
      " |                                 [-0.1611, -0.0779]],\n",
      " |\n",
      " |                                [[ 0.2326, -1.0558],\n",
      " |                                 [ 1.4711,  1.9678]],\n",
      " |\n",
      " |                                [[-0.5138, -0.0411],\n",
      " |                                 [ 1.9417,  0.5158]],\n",
      " |\n",
      " |                                [[ 0.0793,  0.0036],\n",
      " |                                 [-0.2569, -0.1055]]]),\n",
      " |                 size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)\n",
      " |\n",
      " |  sparse_resize_(...)\n",
      " |      sparse_resize_(size, sparse_dim, dense_dim) -> Tensor\n",
      " |\n",
      " |      Resizes :attr:`self` :ref:`sparse tensor <sparse-docs>` to the desired\n",
      " |      size and the number of sparse and dense dimensions.\n",
      " |\n",
      " |      .. note::\n",
      " |        If the number of specified elements in :attr:`self` is zero, then\n",
      " |        :attr:`size`, :attr:`sparse_dim`, and :attr:`dense_dim` can be any\n",
      " |        size and positive integers such that ``len(size) == sparse_dim +\n",
      " |        dense_dim``.\n",
      " |\n",
      " |        If :attr:`self` specifies one or more elements, however, then each\n",
      " |        dimension in :attr:`size` must not be smaller than the corresponding\n",
      " |        dimension of :attr:`self`, :attr:`sparse_dim` must equal the number\n",
      " |        of sparse dimensions in :attr:`self`, and :attr:`dense_dim` must\n",
      " |        equal the number of dense dimensions in :attr:`self`.\n",
      " |\n",
      " |      .. warning::\n",
      " |        Throws an error if :attr:`self` is not a sparse tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          size (torch.Size): the desired size. If :attr:`self` is non-empty\n",
      " |            sparse tensor, the desired size cannot be smaller than the\n",
      " |            original size.\n",
      " |          sparse_dim (int): the number of sparse dimensions\n",
      " |          dense_dim (int): the number of dense dimensions\n",
      " |\n",
      " |  sparse_resize_and_clear_(...)\n",
      " |      sparse_resize_and_clear_(size, sparse_dim, dense_dim) -> Tensor\n",
      " |\n",
      " |      Removes all specified elements from a :ref:`sparse tensor\n",
      " |      <sparse-docs>` :attr:`self` and resizes :attr:`self` to the desired\n",
      " |      size and the number of sparse and dense dimensions.\n",
      " |\n",
      " |      .. warning:\n",
      " |        Throws an error if :attr:`self` is not a sparse tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          size (torch.Size): the desired size.\n",
      " |          sparse_dim (int): the number of sparse dimensions\n",
      " |          dense_dim (int): the number of dense dimensions\n",
      " |\n",
      " |  split_with_sizes(...)\n",
      " |\n",
      " |  sqrt(...)\n",
      " |      sqrt() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sqrt`\n",
      " |\n",
      " |  sqrt_(...)\n",
      " |      sqrt_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.sqrt`\n",
      " |\n",
      " |  square(...)\n",
      " |      square() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.square`\n",
      " |\n",
      " |  square_(...)\n",
      " |      square_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.square`\n",
      " |\n",
      " |  squeeze(...)\n",
      " |      squeeze(dim=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.squeeze`\n",
      " |\n",
      " |  squeeze_(...)\n",
      " |      squeeze_(dim=None) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.squeeze`\n",
      " |\n",
      " |  sspaddmm(...)\n",
      " |      sspaddmm(mat1, mat2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sspaddmm`\n",
      " |\n",
      " |  std(...)\n",
      " |      std(dim=None, *, correction=1, keepdim=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.std`\n",
      " |\n",
      " |  storage_offset(...)\n",
      " |      storage_offset() -> int\n",
      " |\n",
      " |      Returns :attr:`self` tensor's offset in the underlying storage in terms of\n",
      " |      number of storage elements (not bytes).\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.tensor([1, 2, 3, 4, 5])\n",
      " |          >>> x.storage_offset()\n",
      " |          0\n",
      " |          >>> x[3:].storage_offset()\n",
      " |          3\n",
      " |\n",
      " |  stride(...)\n",
      " |      stride(dim) -> tuple or int\n",
      " |\n",
      " |      Returns the stride of :attr:`self` tensor.\n",
      " |\n",
      " |      Stride is the jump necessary to go from one element to the next one in the\n",
      " |      specified dimension :attr:`dim`. A tuple of all strides is returned when no\n",
      " |      argument is passed in. Otherwise, an integer value is returned as the stride in\n",
      " |      the particular dimension :attr:`dim`.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int, optional): the desired dimension in which stride is required\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n",
      " |          >>> x.stride()\n",
      " |          (5, 1)\n",
      " |          >>> x.stride(0)\n",
      " |          5\n",
      " |          >>> x.stride(-1)\n",
      " |          1\n",
      " |\n",
      " |  sub(...)\n",
      " |      sub(other, *, alpha=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sub`.\n",
      " |\n",
      " |  sub_(...)\n",
      " |      sub_(other, *, alpha=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.sub`\n",
      " |\n",
      " |  subtract(...)\n",
      " |      subtract(other, *, alpha=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.subtract`.\n",
      " |\n",
      " |  subtract_(...)\n",
      " |      subtract_(other, *, alpha=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.subtract`.\n",
      " |\n",
      " |  sum(...)\n",
      " |      sum(dim=None, keepdim=False, dtype=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sum`\n",
      " |\n",
      " |  sum_to_size(...)\n",
      " |      sum_to_size(*size) -> Tensor\n",
      " |\n",
      " |      Sum ``this`` tensor to :attr:`size`.\n",
      " |      :attr:`size` must be broadcastable to ``this`` tensor size.\n",
      " |\n",
      " |      Args:\n",
      " |          size (int...): a sequence of integers defining the shape of the output tensor.\n",
      " |\n",
      " |  svd(...)\n",
      " |      svd(some=True, compute_uv=True) -> (Tensor, Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.svd`\n",
      " |\n",
      " |  swapaxes(...)\n",
      " |      swapaxes(axis0, axis1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.swapaxes`\n",
      " |\n",
      " |  swapaxes_(...)\n",
      " |      swapaxes_(axis0, axis1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.swapaxes`\n",
      " |\n",
      " |  swapdims(...)\n",
      " |      swapdims(dim0, dim1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.swapdims`\n",
      " |\n",
      " |  swapdims_(...)\n",
      " |      swapdims_(dim0, dim1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.swapdims`\n",
      " |\n",
      " |  t(...)\n",
      " |      t() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.t`\n",
      " |\n",
      " |  t_(...)\n",
      " |      t_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.t`\n",
      " |\n",
      " |  take(...)\n",
      " |      take(indices) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.take`\n",
      " |\n",
      " |  take_along_dim(...)\n",
      " |      take_along_dim(indices, dim) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.take_along_dim`\n",
      " |\n",
      " |  tan(...)\n",
      " |      tan() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.tan`\n",
      " |\n",
      " |  tan_(...)\n",
      " |      tan_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.tan`\n",
      " |\n",
      " |  tanh(...)\n",
      " |      tanh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.tanh`\n",
      " |\n",
      " |  tanh_(...)\n",
      " |      tanh_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.tanh`\n",
      " |\n",
      " |  tensor_split(...)\n",
      " |      tensor_split(indices_or_sections, dim=0) -> List of Tensors\n",
      " |\n",
      " |      See :func:`torch.tensor_split`\n",
      " |\n",
      " |  tile(...)\n",
      " |      tile(dims) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.tile`\n",
      " |\n",
      " |  to(...)\n",
      " |      to(*args, **kwargs) -> Tensor\n",
      " |\n",
      " |      Performs Tensor dtype and/or device conversion. A :class:`torch.dtype` and :class:`torch.device` are\n",
      " |      inferred from the arguments of ``self.to(*args, **kwargs)``.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          If the ``self`` Tensor already\n",
      " |          has the correct :class:`torch.dtype` and :class:`torch.device`, then ``self`` is returned.\n",
      " |          Otherwise, the returned tensor is a copy of ``self`` with the desired\n",
      " |          :class:`torch.dtype` and :class:`torch.device`.\n",
      " |\n",
      " |      Here are the ways to call ``to``:\n",
      " |\n",
      " |      .. method:: to(dtype, non_blocking=False, copy=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |         :noindex:\n",
      " |\n",
      " |          Returns a Tensor with the specified :attr:`dtype`\n",
      " |\n",
      " |          Args:\n",
      " |              memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |      .. method:: to(device=None, dtype=None, non_blocking=False, copy=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |         :noindex:\n",
      " |\n",
      " |          Returns a Tensor with the specified :attr:`device` and (optional)\n",
      " |          :attr:`dtype`. If :attr:`dtype` is ``None`` it is inferred to be ``self.dtype``.\n",
      " |          When :attr:`non_blocking` is set to ``True``, the function attempts to perform\n",
      " |          the conversion asynchronously with respect to the host, if possible. This\n",
      " |          asynchronous behavior applies to both pinned and pageable memory. However,\n",
      " |          caution is advised when using this feature. For more information, refer to the\n",
      " |          `tutorial on good usage of non_blocking and pin_memory <https://pytorch.org/tutorials/intermediate/pinmem_nonblock.html>`__.\n",
      " |          When :attr:`copy` is set, a new Tensor is created even when the Tensor\n",
      " |          already matches the desired conversion.\n",
      " |\n",
      " |          Args:\n",
      " |              memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |      .. method:: to(other, non_blocking=False, copy=False) -> Tensor\n",
      " |         :noindex:\n",
      " |\n",
      " |          Returns a Tensor with same :class:`torch.dtype` and :class:`torch.device` as\n",
      " |          the Tensor :attr:`other`.\n",
      " |          When :attr:`non_blocking` is set to ``True``, the function attempts to perform\n",
      " |          the conversion asynchronously with respect to the host, if possible. This\n",
      " |          asynchronous behavior applies to both pinned and pageable memory. However,\n",
      " |          caution is advised when using this feature. For more information, refer to the\n",
      " |          `tutorial on good usage of non_blocking and pin_memory <https://pytorch.org/tutorials/intermediate/pinmem_nonblock.html>`__.\n",
      " |          When :attr:`copy` is set, a new Tensor is created even when the Tensor\n",
      " |          already matches the desired conversion.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu\n",
      " |          >>> tensor.to(torch.float64)\n",
      " |          tensor([[-0.5044,  0.0005],\n",
      " |                  [ 0.3310, -0.0584]], dtype=torch.float64)\n",
      " |\n",
      " |          >>> cuda0 = torch.device('cuda:0')\n",
      " |          >>> tensor.to(cuda0)\n",
      " |          tensor([[-0.5044,  0.0005],\n",
      " |                  [ 0.3310, -0.0584]], device='cuda:0')\n",
      " |\n",
      " |          >>> tensor.to(cuda0, dtype=torch.float64)\n",
      " |          tensor([[-0.5044,  0.0005],\n",
      " |                  [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n",
      " |\n",
      " |          >>> other = torch.randn((), dtype=torch.float64, device=cuda0)\n",
      " |          >>> tensor.to(other, non_blocking=True)\n",
      " |          tensor([[-0.5044,  0.0005],\n",
      " |                  [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n",
      " |\n",
      " |  to_dense(...)\n",
      " |      to_dense(dtype=None, *, masked_grad=True) -> Tensor\n",
      " |\n",
      " |      Creates a strided copy of :attr:`self` if :attr:`self` is not a strided tensor, otherwise returns :attr:`self`.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          {dtype}\n",
      " |          masked_grad (bool, optional): If set to ``True`` (default) and\n",
      " |            :attr:`self` has a sparse layout then the backward of\n",
      " |            :meth:`to_dense` returns ``grad.sparse_mask(self)``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> s = torch.sparse_coo_tensor(\n",
      " |          ...        torch.tensor([[1, 1],\n",
      " |          ...                      [0, 2]]),\n",
      " |          ...        torch.tensor([9, 10]),\n",
      " |          ...        size=(3, 3))\n",
      " |          >>> s.to_dense()\n",
      " |          tensor([[ 0,  0,  0],\n",
      " |                  [ 9,  0, 10],\n",
      " |                  [ 0,  0,  0]])\n",
      " |\n",
      " |  to_mkldnn(...)\n",
      " |      to_mkldnn() -> Tensor\n",
      " |      Returns a copy of the tensor in ``torch.mkldnn`` layout.\n",
      " |\n",
      " |  to_padded_tensor(...)\n",
      " |      to_padded_tensor(padding, output_size=None) -> Tensor\n",
      " |      See :func:`to_padded_tensor`\n",
      " |\n",
      " |  to_sparse(...)\n",
      " |      to_sparse(sparseDims) -> Tensor\n",
      " |\n",
      " |      Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in\n",
      " |      :ref:`coordinate format <sparse-coo-docs>`.\n",
      " |\n",
      " |      Args:\n",
      " |          sparseDims (int, optional): the number of sparse dimensions to include in the new sparse tensor\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]])\n",
      " |          >>> d\n",
      " |          tensor([[ 0,  0,  0],\n",
      " |                  [ 9,  0, 10],\n",
      " |                  [ 0,  0,  0]])\n",
      " |          >>> d.to_sparse()\n",
      " |          tensor(indices=tensor([[1, 1],\n",
      " |                                 [0, 2]]),\n",
      " |                 values=tensor([ 9, 10]),\n",
      " |                 size=(3, 3), nnz=2, layout=torch.sparse_coo)\n",
      " |          >>> d.to_sparse(1)\n",
      " |          tensor(indices=tensor([[1]]),\n",
      " |                 values=tensor([[ 9,  0, 10]]),\n",
      " |                 size=(3, 3), nnz=1, layout=torch.sparse_coo)\n",
      " |\n",
      " |      .. method:: to_sparse(*, layout=None, blocksize=None, dense_dim=None) -> Tensor\n",
      " |         :noindex:\n",
      " |\n",
      " |      Returns a sparse tensor with the specified layout and blocksize.  If\n",
      " |      the :attr:`self` is strided, the number of dense dimensions could be\n",
      " |      specified, and a hybrid sparse tensor will be created, with\n",
      " |      `dense_dim` dense dimensions and `self.dim() - 2 - dense_dim` batch\n",
      " |      dimension.\n",
      " |\n",
      " |      .. note:: If the :attr:`self` layout and blocksize parameters match\n",
      " |                with the specified layout and blocksize, return\n",
      " |                :attr:`self`. Otherwise, return a sparse tensor copy of\n",
      " |                :attr:`self`.\n",
      " |\n",
      " |      Args:\n",
      " |\n",
      " |          layout (:class:`torch.layout`, optional): The desired sparse\n",
      " |            layout. One of ``torch.sparse_coo``, ``torch.sparse_csr``,\n",
      " |            ``torch.sparse_csc``, ``torch.sparse_bsr``, or\n",
      " |            ``torch.sparse_bsc``. Default: if ``None``,\n",
      " |            ``torch.sparse_coo``.\n",
      " |\n",
      " |          blocksize (list, tuple, :class:`torch.Size`, optional): Block size\n",
      " |            of the resulting BSR or BSC tensor. For other layouts,\n",
      " |            specifying the block size that is not ``None`` will result in a\n",
      " |            RuntimeError exception.  A block size must be a tuple of length\n",
      " |            two such that its items evenly divide the two sparse dimensions.\n",
      " |\n",
      " |          dense_dim (int, optional): Number of dense dimensions of the\n",
      " |            resulting CSR, CSC, BSR or BSC tensor.  This argument should be\n",
      " |            used only if :attr:`self` is a strided tensor, and must be a\n",
      " |            value between 0 and dimension of :attr:`self` tensor minus two.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.tensor([[1, 0], [0, 0], [2, 3]])\n",
      " |          >>> x.to_sparse(layout=torch.sparse_coo)\n",
      " |          tensor(indices=tensor([[0, 2, 2],\n",
      " |                                 [0, 0, 1]]),\n",
      " |                 values=tensor([1, 2, 3]),\n",
      " |                 size=(3, 2), nnz=3, layout=torch.sparse_coo)\n",
      " |          >>> x.to_sparse(layout=torch.sparse_bsr, blocksize=(1, 2))\n",
      " |          tensor(crow_indices=tensor([0, 1, 1, 2]),\n",
      " |                 col_indices=tensor([0, 0]),\n",
      " |                 values=tensor([[[1, 0]],\n",
      " |                                [[2, 3]]]), size=(3, 2), nnz=2, layout=torch.sparse_bsr)\n",
      " |          >>> x.to_sparse(layout=torch.sparse_bsr, blocksize=(2, 1))\n",
      " |          RuntimeError: Tensor size(-2) 3 needs to be divisible by blocksize[0] 2\n",
      " |          >>> x.to_sparse(layout=torch.sparse_csr, blocksize=(3, 1))\n",
      " |          RuntimeError: to_sparse for Strided to SparseCsr conversion does not use specified blocksize\n",
      " |\n",
      " |          >>> x = torch.tensor([[[1], [0]], [[0], [0]], [[2], [3]]])\n",
      " |          >>> x.to_sparse(layout=torch.sparse_csr, dense_dim=1)\n",
      " |          tensor(crow_indices=tensor([0, 1, 1, 3]),\n",
      " |                 col_indices=tensor([0, 0, 1]),\n",
      " |                 values=tensor([[1],\n",
      " |                                [2],\n",
      " |                                [3]]), size=(3, 2, 1), nnz=3, layout=torch.sparse_csr)\n",
      " |\n",
      " |  to_sparse_bsc(...)\n",
      " |      to_sparse_bsc(blocksize, dense_dim) -> Tensor\n",
      " |\n",
      " |      Convert a tensor to a block sparse column (BSC) storage format of\n",
      " |      given blocksize.  If the :attr:`self` is strided, then the number of\n",
      " |      dense dimensions could be specified, and a hybrid BSC tensor will be\n",
      " |      created, with `dense_dim` dense dimensions and `self.dim() - 2 -\n",
      " |      dense_dim` batch dimension.\n",
      " |\n",
      " |      Args:\n",
      " |\n",
      " |          blocksize (list, tuple, :class:`torch.Size`, optional): Block size\n",
      " |            of the resulting BSC tensor. A block size must be a tuple of\n",
      " |            length two such that its items evenly divide the two sparse\n",
      " |            dimensions.\n",
      " |\n",
      " |          dense_dim (int, optional): Number of dense dimensions of the\n",
      " |            resulting BSC tensor.  This argument should be used only if\n",
      " |            :attr:`self` is a strided tensor, and must be a value between 0\n",
      " |            and dimension of :attr:`self` tensor minus two.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> dense = torch.randn(10, 10)\n",
      " |          >>> sparse = dense.to_sparse_csr()\n",
      " |          >>> sparse_bsc = sparse.to_sparse_bsc((5, 5))\n",
      " |          >>> sparse_bsc.row_indices()\n",
      " |          tensor([0, 1, 0, 1])\n",
      " |\n",
      " |          >>> dense = torch.zeros(4, 3, 1)\n",
      " |          >>> dense[0:2, 0] = dense[0:2, 2] = dense[2:4, 1] = 1\n",
      " |          >>> dense.to_sparse_bsc((2, 1), 1)\n",
      " |          tensor(ccol_indices=tensor([0, 1, 2, 3]),\n",
      " |                 row_indices=tensor([0, 1, 0]),\n",
      " |                 values=tensor([[[[1.]],\n",
      " |\n",
      " |                                 [[1.]]],\n",
      " |\n",
      " |\n",
      " |                                [[[1.]],\n",
      " |\n",
      " |                                 [[1.]]],\n",
      " |\n",
      " |\n",
      " |                                [[[1.]],\n",
      " |\n",
      " |                                 [[1.]]]]), size=(4, 3, 1), nnz=3,\n",
      " |                 layout=torch.sparse_bsc)\n",
      " |\n",
      " |  to_sparse_bsr(...)\n",
      " |      to_sparse_bsr(blocksize, dense_dim) -> Tensor\n",
      " |\n",
      " |      Convert a tensor to a block sparse row (BSR) storage format of given\n",
      " |      blocksize.  If the :attr:`self` is strided, then the number of dense\n",
      " |      dimensions could be specified, and a hybrid BSR tensor will be\n",
      " |      created, with `dense_dim` dense dimensions and `self.dim() - 2 -\n",
      " |      dense_dim` batch dimension.\n",
      " |\n",
      " |      Args:\n",
      " |\n",
      " |          blocksize (list, tuple, :class:`torch.Size`, optional): Block size\n",
      " |            of the resulting BSR tensor. A block size must be a tuple of\n",
      " |            length two such that its items evenly divide the two sparse\n",
      " |            dimensions.\n",
      " |\n",
      " |          dense_dim (int, optional): Number of dense dimensions of the\n",
      " |            resulting BSR tensor.  This argument should be used only if\n",
      " |            :attr:`self` is a strided tensor, and must be a value between 0\n",
      " |            and dimension of :attr:`self` tensor minus two.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> dense = torch.randn(10, 10)\n",
      " |          >>> sparse = dense.to_sparse_csr()\n",
      " |          >>> sparse_bsr = sparse.to_sparse_bsr((5, 5))\n",
      " |          >>> sparse_bsr.col_indices()\n",
      " |          tensor([0, 1, 0, 1])\n",
      " |\n",
      " |          >>> dense = torch.zeros(4, 3, 1)\n",
      " |          >>> dense[0:2, 0] = dense[0:2, 2] = dense[2:4, 1] = 1\n",
      " |          >>> dense.to_sparse_bsr((2, 1), 1)\n",
      " |          tensor(crow_indices=tensor([0, 2, 3]),\n",
      " |                 col_indices=tensor([0, 2, 1]),\n",
      " |                 values=tensor([[[[1.]],\n",
      " |\n",
      " |                                 [[1.]]],\n",
      " |\n",
      " |\n",
      " |                                [[[1.]],\n",
      " |\n",
      " |                                 [[1.]]],\n",
      " |\n",
      " |\n",
      " |                                [[[1.]],\n",
      " |\n",
      " |                                 [[1.]]]]), size=(4, 3, 1), nnz=3,\n",
      " |                 layout=torch.sparse_bsr)\n",
      " |\n",
      " |  to_sparse_csc(...)\n",
      " |      to_sparse_csc() -> Tensor\n",
      " |\n",
      " |      Convert a tensor to compressed column storage (CSC) format.  Except\n",
      " |      for strided tensors, only works with 2D tensors.  If the :attr:`self`\n",
      " |      is strided, then the number of dense dimensions could be specified,\n",
      " |      and a hybrid CSC tensor will be created, with `dense_dim` dense\n",
      " |      dimensions and `self.dim() - 2 - dense_dim` batch dimension.\n",
      " |\n",
      " |      Args:\n",
      " |\n",
      " |          dense_dim (int, optional): Number of dense dimensions of the\n",
      " |            resulting CSC tensor.  This argument should be used only if\n",
      " |            :attr:`self` is a strided tensor, and must be a value between 0\n",
      " |            and dimension of :attr:`self` tensor minus two.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> dense = torch.randn(5, 5)\n",
      " |          >>> sparse = dense.to_sparse_csc()\n",
      " |          >>> sparse._nnz()\n",
      " |          25\n",
      " |\n",
      " |          >>> dense = torch.zeros(3, 3, 1, 1)\n",
      " |          >>> dense[0, 0] = dense[1, 2] = dense[2, 1] = 1\n",
      " |          >>> dense.to_sparse_csc(dense_dim=2)\n",
      " |          tensor(ccol_indices=tensor([0, 1, 2, 3]),\n",
      " |                 row_indices=tensor([0, 2, 1]),\n",
      " |                 values=tensor([[[1.]],\n",
      " |\n",
      " |                                [[1.]],\n",
      " |\n",
      " |                                [[1.]]]), size=(3, 3, 1, 1), nnz=3,\n",
      " |                 layout=torch.sparse_csc)\n",
      " |\n",
      " |  to_sparse_csr(...)\n",
      " |      to_sparse_csr(dense_dim=None) -> Tensor\n",
      " |\n",
      " |      Convert a tensor to compressed row storage format (CSR).  Except for\n",
      " |      strided tensors, only works with 2D tensors.  If the :attr:`self` is\n",
      " |      strided, then the number of dense dimensions could be specified, and a\n",
      " |      hybrid CSR tensor will be created, with `dense_dim` dense dimensions\n",
      " |      and `self.dim() - 2 - dense_dim` batch dimension.\n",
      " |\n",
      " |      Args:\n",
      " |\n",
      " |          dense_dim (int, optional): Number of dense dimensions of the\n",
      " |            resulting CSR tensor.  This argument should be used only if\n",
      " |            :attr:`self` is a strided tensor, and must be a value between 0\n",
      " |            and dimension of :attr:`self` tensor minus two.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> dense = torch.randn(5, 5)\n",
      " |          >>> sparse = dense.to_sparse_csr()\n",
      " |          >>> sparse._nnz()\n",
      " |          25\n",
      " |\n",
      " |          >>> dense = torch.zeros(3, 3, 1, 1)\n",
      " |          >>> dense[0, 0] = dense[1, 2] = dense[2, 1] = 1\n",
      " |          >>> dense.to_sparse_csr(dense_dim=2)\n",
      " |          tensor(crow_indices=tensor([0, 1, 2, 3]),\n",
      " |                 col_indices=tensor([0, 2, 1]),\n",
      " |                 values=tensor([[[1.]],\n",
      " |\n",
      " |                                [[1.]],\n",
      " |\n",
      " |                                [[1.]]]), size=(3, 3, 1, 1), nnz=3,\n",
      " |                 layout=torch.sparse_csr)\n",
      " |\n",
      " |  tolist(...)\n",
      " |      tolist() -> list or number\n",
      " |\n",
      " |      Returns the tensor as a (nested) list. For scalars, a standard\n",
      " |      Python number is returned, just like with :meth:`~Tensor.item`.\n",
      " |      Tensors are automatically moved to the CPU first if necessary.\n",
      " |\n",
      " |      This operation is not differentiable.\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |          >>> a = torch.randn(2, 2)\n",
      " |          >>> a.tolist()\n",
      " |          [[0.012766935862600803, 0.5415473580360413],\n",
      " |           [-0.08909505605697632, 0.7729271650314331]]\n",
      " |          >>> a[0,0].tolist()\n",
      " |          0.012766935862600803\n",
      " |\n",
      " |  topk(...)\n",
      " |      topk(k, dim=None, largest=True, sorted=True) -> (Tensor, LongTensor)\n",
      " |\n",
      " |      See :func:`torch.topk`\n",
      " |\n",
      " |  trace(...)\n",
      " |      trace() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.trace`\n",
      " |\n",
      " |  transpose(...)\n",
      " |      transpose(dim0, dim1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.transpose`\n",
      " |\n",
      " |  transpose_(...)\n",
      " |      transpose_(dim0, dim1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.transpose`\n",
      " |\n",
      " |  triangular_solve(...)\n",
      " |      triangular_solve(A, upper=True, transpose=False, unitriangular=False) -> (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.triangular_solve`\n",
      " |\n",
      " |  tril(...)\n",
      " |      tril(diagonal=0) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.tril`\n",
      " |\n",
      " |  tril_(...)\n",
      " |      tril_(diagonal=0) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.tril`\n",
      " |\n",
      " |  triu(...)\n",
      " |      triu(diagonal=0) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.triu`\n",
      " |\n",
      " |  triu_(...)\n",
      " |      triu_(diagonal=0) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.triu`\n",
      " |\n",
      " |  true_divide(...)\n",
      " |      true_divide(value) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.true_divide`\n",
      " |\n",
      " |  true_divide_(...)\n",
      " |      true_divide_(value) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.true_divide_`\n",
      " |\n",
      " |  trunc(...)\n",
      " |      trunc() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.trunc`\n",
      " |\n",
      " |  trunc_(...)\n",
      " |      trunc_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.trunc`\n",
      " |\n",
      " |  type(...)\n",
      " |      type(dtype=None, non_blocking=False, **kwargs) -> str or Tensor\n",
      " |      Returns the type if `dtype` is not provided, else casts this object to\n",
      " |      the specified type.\n",
      " |\n",
      " |      If this is already of the correct type, no copy is performed and the\n",
      " |      original object is returned.\n",
      " |\n",
      " |      Args:\n",
      " |          dtype (dtype or string): The desired type\n",
      " |          non_blocking (bool): If ``True``, and the source is in pinned memory\n",
      " |              and destination is on the GPU or vice versa, the copy is performed\n",
      " |              asynchronously with respect to the host. Otherwise, the argument\n",
      " |              has no effect.\n",
      " |          **kwargs: For compatibility, may contain the key ``async`` in place of\n",
      " |              the ``non_blocking`` argument. The ``async`` arg is deprecated.\n",
      " |\n",
      " |  type_as(...)\n",
      " |      type_as(tensor) -> Tensor\n",
      " |\n",
      " |      Returns this tensor cast to the type of the given tensor.\n",
      " |\n",
      " |      This is a no-op if the tensor is already of the correct type. This is\n",
      " |      equivalent to ``self.type(tensor.type())``\n",
      " |\n",
      " |      Args:\n",
      " |          tensor (Tensor): the tensor which has the desired type\n",
      " |\n",
      " |  unbind(...)\n",
      " |      unbind(dim=0) -> seq\n",
      " |\n",
      " |      See :func:`torch.unbind`\n",
      " |\n",
      " |  unfold(...)\n",
      " |      unfold(dimension, size, step) -> Tensor\n",
      " |\n",
      " |      Returns a view of the original tensor which contains all slices of size :attr:`size` from\n",
      " |      :attr:`self` tensor in the dimension :attr:`dimension`.\n",
      " |\n",
      " |      Step between two slices is given by :attr:`step`.\n",
      " |\n",
      " |      If `sizedim` is the size of dimension :attr:`dimension` for :attr:`self`, the size of\n",
      " |      dimension :attr:`dimension` in the returned tensor will be\n",
      " |      `(sizedim - size) / step + 1`.\n",
      " |\n",
      " |      An additional dimension of size :attr:`size` is appended in the returned tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          dimension (int): dimension in which unfolding happens\n",
      " |          size (int): the size of each slice that is unfolded\n",
      " |          step (int): the step between each slice\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.arange(1., 8)\n",
      " |          >>> x\n",
      " |          tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])\n",
      " |          >>> x.unfold(0, 2, 1)\n",
      " |          tensor([[ 1.,  2.],\n",
      " |                  [ 2.,  3.],\n",
      " |                  [ 3.,  4.],\n",
      " |                  [ 4.,  5.],\n",
      " |                  [ 5.,  6.],\n",
      " |                  [ 6.,  7.]])\n",
      " |          >>> x.unfold(0, 2, 2)\n",
      " |          tensor([[ 1.,  2.],\n",
      " |                  [ 3.,  4.],\n",
      " |                  [ 5.,  6.]])\n",
      " |\n",
      " |  uniform_(...)\n",
      " |      uniform_(from=0, to=1, *, generator=None) -> Tensor\n",
      " |\n",
      " |      Fills :attr:`self` tensor with numbers sampled from the continuous uniform\n",
      " |      distribution:\n",
      " |\n",
      " |      .. math::\n",
      " |          f(x) = \\dfrac{1}{\\text{to} - \\text{from}}\n",
      " |\n",
      " |  unsafe_chunk(...)\n",
      " |      unsafe_chunk(chunks, dim=0) -> List of Tensors\n",
      " |\n",
      " |      See :func:`torch.unsafe_chunk`\n",
      " |\n",
      " |  unsafe_split(...)\n",
      " |      unsafe_split(split_size, dim=0) -> List of Tensors\n",
      " |\n",
      " |      See :func:`torch.unsafe_split`\n",
      " |\n",
      " |  unsafe_split_with_sizes(...)\n",
      " |\n",
      " |  unsqueeze(...)\n",
      " |      unsqueeze(dim) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.unsqueeze`\n",
      " |\n",
      " |  unsqueeze_(...)\n",
      " |      unsqueeze_(dim) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.unsqueeze`\n",
      " |\n",
      " |  untyped_storage(...)\n",
      " |      untyped_storage() -> torch.UntypedStorage\n",
      " |\n",
      " |      Returns the underlying :class:`UntypedStorage`.\n",
      " |\n",
      " |  values(...)\n",
      " |      values() -> Tensor\n",
      " |\n",
      " |      Return the values tensor of a :ref:`sparse COO tensor <sparse-coo-docs>`.\n",
      " |\n",
      " |      .. warning::\n",
      " |        Throws an error if :attr:`self` is not a sparse COO tensor.\n",
      " |\n",
      " |      See also :meth:`Tensor.indices`.\n",
      " |\n",
      " |      .. note::\n",
      " |        This method can only be called on a coalesced sparse tensor. See\n",
      " |        :meth:`Tensor.coalesce` for details.\n",
      " |\n",
      " |  var(...)\n",
      " |      var(dim=None, *, correction=1, keepdim=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.var`\n",
      " |\n",
      " |  vdot(...)\n",
      " |      vdot(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.vdot`\n",
      " |\n",
      " |  view(...)\n",
      " |      view(*shape) -> Tensor\n",
      " |\n",
      " |      Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      " |      different :attr:`shape`.\n",
      " |\n",
      " |      The returned tensor shares the same data and must have the same number\n",
      " |      of elements, but may have a different size. For a tensor to be viewed, the new\n",
      " |      view size must be compatible with its original size and stride, i.e., each new\n",
      " |      view dimension must either be a subspace of an original dimension, or only span\n",
      " |      across original dimensions :math:`d, d+1, \\dots, d+k` that satisfy the following\n",
      " |      contiguity-like condition that :math:`\\forall i = d, \\dots, d+k-1`,\n",
      " |\n",
      " |      .. math::\n",
      " |\n",
      " |        \\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]\n",
      " |\n",
      " |      Otherwise, it will not be possible to view :attr:`self` tensor as :attr:`shape`\n",
      " |      without copying it (e.g., via :meth:`contiguous`). When it is unclear whether a\n",
      " |      :meth:`view` can be performed, it is advisable to use :meth:`reshape`, which\n",
      " |      returns a view if the shapes are compatible, and copies (equivalent to calling\n",
      " |      :meth:`contiguous`) otherwise.\n",
      " |\n",
      " |      Args:\n",
      " |          shape (torch.Size or int...): the desired size\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.randn(4, 4)\n",
      " |          >>> x.size()\n",
      " |          torch.Size([4, 4])\n",
      " |          >>> y = x.view(16)\n",
      " |          >>> y.size()\n",
      " |          torch.Size([16])\n",
      " |          >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
      " |          >>> z.size()\n",
      " |          torch.Size([2, 8])\n",
      " |\n",
      " |          >>> a = torch.randn(1, 2, 3, 4)\n",
      " |          >>> a.size()\n",
      " |          torch.Size([1, 2, 3, 4])\n",
      " |          >>> b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n",
      " |          >>> b.size()\n",
      " |          torch.Size([1, 3, 2, 4])\n",
      " |          >>> c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n",
      " |          >>> c.size()\n",
      " |          torch.Size([1, 3, 2, 4])\n",
      " |          >>> torch.equal(b, c)\n",
      " |          False\n",
      " |\n",
      " |\n",
      " |      .. method:: view(dtype) -> Tensor\n",
      " |         :noindex:\n",
      " |\n",
      " |      Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      " |      different :attr:`dtype`.\n",
      " |\n",
      " |      If the element size of :attr:`dtype` is different than that of ``self.dtype``,\n",
      " |      then the size of the last dimension of the output will be scaled\n",
      " |      proportionally.  For instance, if :attr:`dtype` element size is twice that of\n",
      " |      ``self.dtype``, then each pair of elements in the last dimension of\n",
      " |      :attr:`self` will be combined, and the size of the last dimension of the output\n",
      " |      will be half that of :attr:`self`. If :attr:`dtype` element size is half that\n",
      " |      of ``self.dtype``, then each element in the last dimension of :attr:`self` will\n",
      " |      be split in two, and the size of the last dimension of the output will be\n",
      " |      double that of :attr:`self`. For this to be possible, the following conditions\n",
      " |      must be true:\n",
      " |\n",
      " |          * ``self.dim()`` must be greater than 0.\n",
      " |          * ``self.stride(-1)`` must be 1.\n",
      " |\n",
      " |      Additionally, if the element size of :attr:`dtype` is greater than that of\n",
      " |      ``self.dtype``, the following conditions must be true as well:\n",
      " |\n",
      " |          * ``self.size(-1)`` must be divisible by the ratio between the element\n",
      " |            sizes of the dtypes.\n",
      " |          * ``self.storage_offset()`` must be divisible by the ratio between the\n",
      " |            element sizes of the dtypes.\n",
      " |          * The strides of all dimensions, except the last dimension, must be\n",
      " |            divisible by the ratio between the element sizes of the dtypes.\n",
      " |\n",
      " |      If any of the above conditions are not met, an error is thrown.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          This overload is not supported by TorchScript, and using it in a Torchscript\n",
      " |          program will cause undefined behavior.\n",
      " |\n",
      " |\n",
      " |      Args:\n",
      " |          dtype (:class:`torch.dtype`): the desired dtype\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.randn(4, 4)\n",
      " |          >>> x\n",
      " |          tensor([[ 0.9482, -0.0310,  1.4999, -0.5316],\n",
      " |                  [-0.1520,  0.7472,  0.5617, -0.8649],\n",
      " |                  [-2.4724, -0.0334, -0.2976, -0.8499],\n",
      " |                  [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
      " |          >>> x.dtype\n",
      " |          torch.float32\n",
      " |\n",
      " |          >>> y = x.view(torch.int32)\n",
      " |          >>> y\n",
      " |          tensor([[ 1064483442, -1124191867,  1069546515, -1089989247],\n",
      " |                  [-1105482831,  1061112040,  1057999968, -1084397505],\n",
      " |                  [-1071760287, -1123489973, -1097310419, -1084649136],\n",
      " |                  [-1101533110,  1073668768, -1082790149, -1088634448]],\n",
      " |              dtype=torch.int32)\n",
      " |          >>> y[0, 0] = 1000000000\n",
      " |          >>> x\n",
      " |          tensor([[ 0.0047, -0.0310,  1.4999, -0.5316],\n",
      " |                  [-0.1520,  0.7472,  0.5617, -0.8649],\n",
      " |                  [-2.4724, -0.0334, -0.2976, -0.8499],\n",
      " |                  [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
      " |\n",
      " |          >>> x.view(torch.cfloat)\n",
      " |          tensor([[ 0.0047-0.0310j,  1.4999-0.5316j],\n",
      " |                  [-0.1520+0.7472j,  0.5617-0.8649j],\n",
      " |                  [-2.4724-0.0334j, -0.2976-0.8499j],\n",
      " |                  [-0.2109+1.9913j, -0.9607-0.6123j]])\n",
      " |          >>> x.view(torch.cfloat).size()\n",
      " |          torch.Size([4, 2])\n",
      " |\n",
      " |          >>> x.view(torch.uint8)\n",
      " |          tensor([[  0, 202, 154,  59, 182, 243, 253, 188, 185, 252, 191,  63, 240,  22,\n",
      " |                     8, 191],\n",
      " |                  [227, 165,  27, 190, 128,  72,  63,  63, 146, 203,  15,  63,  22, 106,\n",
      " |                    93, 191],\n",
      " |                  [205,  59,  30, 192, 112, 206,   8, 189,   7,  95, 152, 190,  12, 147,\n",
      " |                    89, 191],\n",
      " |                  [ 43, 246,  87, 190, 235, 226, 254,  63, 111, 240, 117, 191, 177, 191,\n",
      " |                    28, 191]], dtype=torch.uint8)\n",
      " |          >>> x.view(torch.uint8).size()\n",
      " |          torch.Size([4, 16])\n",
      " |\n",
      " |  view_as(...)\n",
      " |      view_as(other) -> Tensor\n",
      " |\n",
      " |      View this tensor as the same size as :attr:`other`.\n",
      " |      ``self.view_as(other)`` is equivalent to ``self.view(other.size())``.\n",
      " |\n",
      " |      Please see :meth:`~Tensor.view` for more information about ``view``.\n",
      " |\n",
      " |      Args:\n",
      " |          other (:class:`torch.Tensor`): The result tensor has the same size\n",
      " |              as :attr:`other`.\n",
      " |\n",
      " |  vsplit(...)\n",
      " |      vsplit(split_size_or_sections) -> List of Tensors\n",
      " |\n",
      " |      See :func:`torch.vsplit`\n",
      " |\n",
      " |  where(...)\n",
      " |      where(condition, y) -> Tensor\n",
      " |\n",
      " |      ``self.where(condition, y)`` is equivalent to ``torch.where(condition, self, y)``.\n",
      " |      See :func:`torch.where`\n",
      " |\n",
      " |  xlogy(...)\n",
      " |      xlogy(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.xlogy`\n",
      " |\n",
      " |  xlogy_(...)\n",
      " |      xlogy_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.xlogy`\n",
      " |\n",
      " |  xpu(...)\n",
      " |      xpu(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      Returns a copy of this object in XPU memory.\n",
      " |\n",
      " |      If this object is already in XPU memory and on the correct device,\n",
      " |      then no copy is performed and the original object is returned.\n",
      " |\n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The destination XPU device.\n",
      " |              Defaults to the current XPU device.\n",
      " |          non_blocking (bool): If ``True`` and the source is in pinned memory,\n",
      " |              the copy will be asynchronous with respect to the host.\n",
      " |              Otherwise, the argument has no effect. Default: ``False``.\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  zero_(...)\n",
      " |      zero_() -> Tensor\n",
      " |\n",
      " |      Fills :attr:`self` tensor with zeros.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch._C.TensorBase:\n",
      " |\n",
      " |  H\n",
      " |      Returns a view of a matrix (2-D tensor) conjugated and transposed.\n",
      " |\n",
      " |      ``x.H`` is equivalent to ``x.transpose(0, 1).conj()`` for complex matrices and\n",
      " |      ``x.transpose(0, 1)`` for real matrices.\n",
      " |\n",
      " |      .. seealso::\n",
      " |\n",
      " |              :attr:`~.Tensor.mH`: An attribute that also works on batches of matrices.\n",
      " |\n",
      " |  T\n",
      " |      Returns a view of this tensor with its dimensions reversed.\n",
      " |\n",
      " |      If ``n`` is the number of dimensions in ``x``,\n",
      " |      ``x.T`` is equivalent to ``x.permute(n-1, n-2, ..., 0)``.\n",
      " |\n",
      " |      .. warning::\n",
      " |          The use of :func:`Tensor.T` on tensors of dimension other than 2 to reverse their shape\n",
      " |          is deprecated and it will throw an error in a future release. Consider :attr:`~.Tensor.mT`\n",
      " |          to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse\n",
      " |          the dimensions of a tensor.\n",
      " |\n",
      " |  data\n",
      " |\n",
      " |  device\n",
      " |      Is the :class:`torch.device` where this Tensor is.\n",
      " |\n",
      " |  dtype\n",
      " |\n",
      " |  grad\n",
      " |      This attribute is ``None`` by default and becomes a Tensor the first time a call to\n",
      " |      :func:`backward` computes gradients for ``self``.\n",
      " |      The attribute will then contain the gradients computed and future calls to\n",
      " |      :func:`backward` will accumulate (add) gradients into it.\n",
      " |\n",
      " |  grad_fn\n",
      " |\n",
      " |  imag\n",
      " |      Returns a new tensor containing imaginary values of the :attr:`self` tensor.\n",
      " |      The returned tensor and :attr:`self` share the same underlying storage.\n",
      " |\n",
      " |      .. warning::\n",
      " |          :func:`imag` is only supported for tensors with complex dtypes.\n",
      " |\n",
      " |      Example::\n",
      " |          >>> x=torch.randn(4, dtype=torch.cfloat)\n",
      " |          >>> x\n",
      " |          tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n",
      " |          >>> x.imag\n",
      " |          tensor([ 0.3553, -0.7896, -0.0633, -0.8119])\n",
      " |\n",
      " |  is_cpu\n",
      " |      Is ``True`` if the Tensor is stored on the CPU, ``False`` otherwise.\n",
      " |\n",
      " |  is_cuda\n",
      " |      Is ``True`` if the Tensor is stored on the GPU, ``False`` otherwise.\n",
      " |\n",
      " |  is_ipu\n",
      " |      Is ``True`` if the Tensor is stored on the IPU, ``False`` otherwise.\n",
      " |\n",
      " |  is_leaf\n",
      " |      All Tensors that have :attr:`requires_grad` which is ``False`` will be leaf Tensors by convention.\n",
      " |\n",
      " |      For Tensors that have :attr:`requires_grad` which is ``True``, they will be leaf Tensors if they were\n",
      " |      created by the user. This means that they are not the result of an operation and so\n",
      " |      :attr:`grad_fn` is None.\n",
      " |\n",
      " |      Only leaf Tensors will have their :attr:`grad` populated during a call to :func:`backward`.\n",
      " |      To get :attr:`grad` populated for non-leaf Tensors, you can use :func:`retain_grad`.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> a = torch.rand(10, requires_grad=True)\n",
      " |          >>> a.is_leaf\n",
      " |          True\n",
      " |          >>> b = torch.rand(10, requires_grad=True).cuda()\n",
      " |          >>> b.is_leaf\n",
      " |          False\n",
      " |          # b was created by the operation that cast a cpu Tensor into a cuda Tensor\n",
      " |          >>> c = torch.rand(10, requires_grad=True) + 2\n",
      " |          >>> c.is_leaf\n",
      " |          False\n",
      " |          # c was created by the addition operation\n",
      " |          >>> d = torch.rand(10).cuda()\n",
      " |          >>> d.is_leaf\n",
      " |          True\n",
      " |          # d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)\n",
      " |          >>> e = torch.rand(10).cuda().requires_grad_()\n",
      " |          >>> e.is_leaf\n",
      " |          True\n",
      " |          # e requires gradients and has no operations creating it\n",
      " |          >>> f = torch.rand(10, requires_grad=True, device=\"cuda\")\n",
      " |          >>> f.is_leaf\n",
      " |          True\n",
      " |          # f requires grad, has no operation creating it\n",
      " |\n",
      " |  is_maia\n",
      " |\n",
      " |  is_meta\n",
      " |      Is ``True`` if the Tensor is a meta tensor, ``False`` otherwise.  Meta tensors\n",
      " |      are like normal tensors, but they carry no data.\n",
      " |\n",
      " |  is_mkldnn\n",
      " |\n",
      " |  is_mps\n",
      " |      Is ``True`` if the Tensor is stored on the MPS device, ``False`` otherwise.\n",
      " |\n",
      " |  is_mtia\n",
      " |\n",
      " |  is_nested\n",
      " |\n",
      " |  is_quantized\n",
      " |      Is ``True`` if the Tensor is quantized, ``False`` otherwise.\n",
      " |\n",
      " |  is_sparse\n",
      " |      Is ``True`` if the Tensor uses sparse COO storage layout, ``False`` otherwise.\n",
      " |\n",
      " |  is_sparse_csr\n",
      " |      Is ``True`` if the Tensor uses sparse CSR storage layout, ``False`` otherwise.\n",
      " |\n",
      " |  is_vulkan\n",
      " |\n",
      " |  is_xla\n",
      " |      Is ``True`` if the Tensor is stored on an XLA device, ``False`` otherwise.\n",
      " |\n",
      " |  is_xpu\n",
      " |      Is ``True`` if the Tensor is stored on the XPU, ``False`` otherwise.\n",
      " |\n",
      " |  itemsize\n",
      " |      Alias for :meth:`~Tensor.element_size()`\n",
      " |\n",
      " |  layout\n",
      " |\n",
      " |  mH\n",
      " |      Accessing this property is equivalent to calling :func:`adjoint`.\n",
      " |\n",
      " |  mT\n",
      " |      Returns a view of this tensor with the last two dimensions transposed.\n",
      " |\n",
      " |      ``x.mT`` is equivalent to ``x.transpose(-2, -1)``.\n",
      " |\n",
      " |  name\n",
      " |\n",
      " |  names\n",
      " |      Stores names for each of this tensor's dimensions.\n",
      " |\n",
      " |      ``names[idx]`` corresponds to the name of tensor dimension ``idx``.\n",
      " |      Names are either a string if the dimension is named or ``None`` if the\n",
      " |      dimension is unnamed.\n",
      " |\n",
      " |      Dimension names may contain characters or underscore. Furthermore, a dimension\n",
      " |      name must be a valid Python variable name (i.e., does not start with underscore).\n",
      " |\n",
      " |      Tensors may not have two named dimensions with the same name.\n",
      " |\n",
      " |      .. warning::\n",
      " |          The named tensor API is experimental and subject to change.\n",
      " |\n",
      " |  nbytes\n",
      " |      Returns the number of bytes consumed by the \"view\" of elements of the Tensor\n",
      " |      if the Tensor does not use sparse storage layout.\n",
      " |      Defined to be :meth:`~Tensor.numel()` * :meth:`~Tensor.element_size()`\n",
      " |\n",
      " |  ndim\n",
      " |      Alias for :meth:`~Tensor.dim()`\n",
      " |\n",
      " |  output_nr\n",
      " |\n",
      " |  real\n",
      " |      Returns a new tensor containing real values of the :attr:`self` tensor for a complex-valued input tensor.\n",
      " |      The returned tensor and :attr:`self` share the same underlying storage.\n",
      " |\n",
      " |      Returns :attr:`self` if :attr:`self` is a real-valued tensor tensor.\n",
      " |\n",
      " |      Example::\n",
      " |          >>> x=torch.randn(4, dtype=torch.cfloat)\n",
      " |          >>> x\n",
      " |          tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n",
      " |          >>> x.real\n",
      " |          tensor([ 0.3100, -0.5445, -1.6492, -0.0638])\n",
      " |\n",
      " |  requires_grad\n",
      " |      Is ``True`` if gradients need to be computed for this Tensor, ``False`` otherwise.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          The fact that gradients need to be computed for a Tensor do not mean that the :attr:`grad`\n",
      " |          attribute will be populated, see :attr:`is_leaf` for more details.\n",
      " |\n",
      " |  retains_grad\n",
      " |      Is ``True`` if this Tensor is non-leaf and its :attr:`grad` is enabled to be\n",
      " |      populated during :func:`backward`, ``False`` otherwise.\n",
      " |\n",
      " |  shape\n",
      " |      shape() -> torch.Size\n",
      " |\n",
      " |      Returns the size of the :attr:`self` tensor. Alias for :attr:`size`.\n",
      " |\n",
      " |      See also :meth:`Tensor.size`.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> t = torch.empty(3, 4, 5)\n",
      " |          >>> t.size()\n",
      " |          torch.Size([3, 4, 5])\n",
      " |          >>> t.shape\n",
      " |          torch.Size([3, 4, 5])\n",
      " |\n",
      " |  volatile\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(param_store[\"AutoDiagonalNormal.loc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30beb9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Tensor in module torch object:\n",
      "\n",
      "class Tensor(torch._C.TensorBase)\n",
      " |  Method resolution order:\n",
      " |      Tensor\n",
      " |      torch._C.TensorBase\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __abs__ = abs(...)\n",
      " |\n",
      " |  __array__(self, dtype=None) from torch._tensor.Tensor\n",
      " |\n",
      " |  __array_wrap__(self, array) from torch._tensor.Tensor\n",
      " |      # Wrap Numpy array again in a suitable tensor when done, to support e.g.\n",
      " |      # `numpy.sin(tensor) -> tensor` or `numpy.greater(tensor, 0) -> ByteTensor`\n",
      " |\n",
      " |  __contains__(self, element: Any, /) -> bool from torch._tensor.Tensor\n",
      " |      Check if `element` is present in tensor\n",
      " |\n",
      " |      Args:\n",
      " |          element (Tensor or scalar): element to be checked\n",
      " |              for presence in current tensor\"\n",
      " |\n",
      " |  __deepcopy__(self, memo) from torch._tensor.Tensor\n",
      " |\n",
      " |  __dir__(self) from torch._tensor.Tensor\n",
      " |      Default dir() implementation.\n",
      " |\n",
      " |  __dlpack__(self, stream=None) from torch._tensor.Tensor\n",
      " |      Creates a DLpack `capsule https://data-apis.org/array-api/latest/design_topics/data_interchange.html#data-interchange`_\n",
      " |      of the current tensor to be exported to other libraries.\n",
      " |\n",
      " |      This function will be called from the `from_dlpack` method\n",
      " |      of the library that will consume the capsule. `from_dlpack` passes the current\n",
      " |      stream to this method as part of the specification.\n",
      " |\n",
      " |      Args:\n",
      " |          stream (integer or None): An optional Python integer representing a\n",
      " |          pointer to a CUDA stream. The current stream is synchronized with\n",
      " |          this stream before the capsule is created, and since the capsule\n",
      " |          shares its storage with the tensor this make it safe to access from\n",
      " |          both streams.  If None or -1 is passed then no synchronization is performed.\n",
      " |          If 1 (on CUDA) or 0 (on ROCM) then the default stream is used for\n",
      " |          synchronization.\n",
      " |\n",
      " |  __dlpack_device__(self) -> tuple[enum.IntEnum, int] from torch._tensor.Tensor\n",
      " |\n",
      " |  __floordiv__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __format__(self, format_spec) from torch._tensor.Tensor\n",
      " |      Default object formatter.\n",
      " |\n",
      " |      Return str(self) if format_spec is empty. Raise TypeError otherwise.\n",
      " |\n",
      " |  __hash__(self) from torch._tensor.Tensor\n",
      " |      Return hash(self).\n",
      " |\n",
      " |  __ipow__ = pow_(...) from torch._tensor.TensorBase\n",
      " |\n",
      " |  __iter__(self) from torch._tensor.Tensor\n",
      " |\n",
      " |  __itruediv__ = __idiv__(...)\n",
      " |\n",
      " |  __len__(self) from torch._tensor.Tensor\n",
      " |      Return len(self).\n",
      " |\n",
      " |  __neg__ = neg(...)\n",
      " |\n",
      " |  __pos__ = positive(...)\n",
      " |\n",
      " |  __pow__ = pow(...) from torch._tensor.TensorBase\n",
      " |\n",
      " |  __rdiv__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __reduce_ex__(self, proto) from torch._tensor.Tensor\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __repr__(self, *, tensor_contents=None) from torch._tensor.Tensor\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __reversed__(self) from torch._tensor.Tensor\n",
      " |      Reverses the tensor along dimension 0.\n",
      " |\n",
      " |  __rfloordiv__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __rlshift__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __rmatmul__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __rmod__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __rpow__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __rrshift__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __rsub__(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  __rtruediv__ = __rdiv__(self, other)\n",
      " |\n",
      " |  __setstate__(self, state) from torch._tensor.Tensor\n",
      " |\n",
      " |  align_to(self, *names) from torch._tensor.Tensor\n",
      " |      Permutes the dimensions of the :attr:`self` tensor to match the order\n",
      " |      specified in :attr:`names`, adding size-one dims for any new names.\n",
      " |\n",
      " |      All of the dims of :attr:`self` must be named in order to use this method.\n",
      " |      The resulting tensor is a view on the original tensor.\n",
      " |\n",
      " |      All dimension names of :attr:`self` must be present in :attr:`names`.\n",
      " |      :attr:`names` may contain additional names that are not in ``self.names``;\n",
      " |      the output tensor has a size-one dimension for each of those new names.\n",
      " |\n",
      " |      :attr:`names` may contain up to one Ellipsis (``...``).\n",
      " |      The Ellipsis is expanded to be equal to all dimension names of :attr:`self`\n",
      " |      that are not mentioned in :attr:`names`, in the order that they appear\n",
      " |      in :attr:`self`.\n",
      " |\n",
      " |      Python 2 does not support Ellipsis but one may use a string literal\n",
      " |      instead (``'...'``).\n",
      " |\n",
      " |      Args:\n",
      " |          names (iterable of str): The desired dimension ordering of the\n",
      " |              output tensor. May contain up to one Ellipsis that is expanded\n",
      " |              to all unmentioned dim names of :attr:`self`.\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |          >>> tensor = torch.randn(2, 2, 2, 2, 2, 2)\n",
      " |          >>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')\n",
      " |\n",
      " |          # Move the F and E dims to the front while keeping the rest in order\n",
      " |          >>> named_tensor.align_to('F', 'E', ...)\n",
      " |\n",
      " |      .. warning::\n",
      " |          The named tensor API is experimental and subject to change.\n",
      " |\n",
      " |  backward(self, gradient=None, retain_graph=None, create_graph=False, inputs=None) from torch._tensor.Tensor\n",
      " |      Computes the gradient of current tensor wrt graph leaves.\n",
      " |\n",
      " |      The graph is differentiated using the chain rule. If the tensor is\n",
      " |      non-scalar (i.e. its data has more than one element) and requires\n",
      " |      gradient, the function additionally requires specifying a ``gradient``.\n",
      " |      It should be a tensor of matching type and shape, that represents\n",
      " |      the gradient of the differentiated function w.r.t. ``self``.\n",
      " |\n",
      " |      This function accumulates gradients in the leaves - you might need to zero\n",
      " |      ``.grad`` attributes or set them to ``None`` before calling it.\n",
      " |      See :ref:`Default gradient layouts<default-grad-layouts>`\n",
      " |      for details on the memory layout of accumulated gradients.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          If you run any forward ops, create ``gradient``, and/or call ``backward``\n",
      " |          in a user-specified CUDA stream context, see\n",
      " |          :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          When ``inputs`` are provided and a given input is not a leaf,\n",
      " |          the current implementation will call its grad_fn (though it is not strictly needed to get this gradients).\n",
      " |          It is an implementation detail on which the user should not rely.\n",
      " |          See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.\n",
      " |\n",
      " |      Args:\n",
      " |          gradient (Tensor, optional): The gradient of the function\n",
      " |              being differentiated w.r.t. ``self``.\n",
      " |              This argument can be omitted if ``self`` is a scalar.\n",
      " |          retain_graph (bool, optional): If ``False``, the graph used to compute\n",
      " |              the grads will be freed. Note that in nearly all cases setting\n",
      " |              this option to True is not needed and often can be worked around\n",
      " |              in a much more efficient way. Defaults to the value of\n",
      " |              ``create_graph``.\n",
      " |          create_graph (bool, optional): If ``True``, graph of the derivative will\n",
      " |              be constructed, allowing to compute higher order derivative\n",
      " |              products. Defaults to ``False``.\n",
      " |          inputs (sequence of Tensor, optional): Inputs w.r.t. which the gradient will be\n",
      " |              accumulated into ``.grad``. All other tensors will be ignored. If not\n",
      " |              provided, the gradient is accumulated into all the leaf Tensors that were\n",
      " |              used to compute the :attr:`tensors`.\n",
      " |\n",
      " |  detach(...) from torch._C.TensorBase\n",
      " |      Returns a new Tensor, detached from the current graph.\n",
      " |\n",
      " |      The result will never require gradient.\n",
      " |\n",
      " |      This method also affects forward mode AD gradients and the result will never\n",
      " |      have forward mode AD gradients.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |        Returned Tensor shares the same storage with the original one.\n",
      " |        In-place modifications on either of them will be seen, and may trigger\n",
      " |        errors in correctness checks.\n",
      " |\n",
      " |  detach_(...) from torch._C.TensorBase\n",
      " |      Detaches the Tensor from the graph that created it, making it a leaf.\n",
      " |      Views cannot be detached in-place.\n",
      " |\n",
      " |      This method also affects forward mode AD gradients and the result will never\n",
      " |      have forward mode AD gradients.\n",
      " |\n",
      " |  dim_order(self, *, ambiguity_check: Union[bool, list[torch.memory_format]] = False) from torch._tensor.Tensor\n",
      " |      dim_order(ambiguity_check=False) -> tuple\n",
      " |\n",
      " |      Returns the uniquely determined tuple of int describing the dim order or\n",
      " |      physical layout of :attr:`self`.\n",
      " |\n",
      " |      The dim order represents how dimensions are laid out in memory of dense tensors,\n",
      " |      starting from the outermost to the innermost dimension.\n",
      " |\n",
      " |      Note that the dim order may not always be uniquely determined.\n",
      " |      If `ambiguity_check` is True, this function raises a RuntimeError when the dim order cannot be uniquely determined;\n",
      " |      If `ambiguity_check` is a list of memory formats, this function raises a RuntimeError when tensor can not be interpreted\n",
      " |      into exactly one of the given memory formats, or it cannot be uniquely determined.\n",
      " |      If `ambiguity_check` is False, it will return one of legal dim order(s) without checking its uniqueness.\n",
      " |      Otherwise, it will raise TypeError.\n",
      " |\n",
      " |      Args:\n",
      " |          ambiguity_check (bool or List[torch.memory_format]): The check method for ambiguity of dim order.\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |          >>> torch.empty((2, 3, 5, 7)).dim_order()\n",
      " |          (0, 1, 2, 3)\n",
      " |          >>> torch.empty((2, 3, 5, 7)).transpose(1, 2).dim_order()\n",
      " |          (0, 2, 1, 3)\n",
      " |          >>> torch.empty((2, 3, 5, 7), memory_format=torch.channels_last).dim_order()\n",
      " |          (0, 2, 3, 1)\n",
      " |          >>> torch.empty((1, 2, 3, 4)).dim_order()\n",
      " |          (0, 1, 2, 3)\n",
      " |          >>> try:\n",
      " |          ...     torch.empty((1, 2, 3, 4)).dim_order(ambiguity_check=True)\n",
      " |          ... except RuntimeError as e:\n",
      " |          ...     print(e)\n",
      " |          The tensor does not have unique dim order, or cannot map to exact one of the given memory formats.\n",
      " |          >>> torch.empty((1, 2, 3, 4)).dim_order(\n",
      " |          ...     ambiguity_check=[torch.contiguous_format, torch.channels_last]\n",
      " |          ... )  # It can be mapped to contiguous format\n",
      " |          (0, 1, 2, 3)\n",
      " |          >>> try:\n",
      " |          ...     torch.empty((1, 2, 3, 4)).dim_order(ambiguity_check=\"ILLEGAL\")\n",
      " |          ... except TypeError as e:\n",
      " |          ...     print(e)\n",
      " |          The ambiguity_check argument must be a bool or a list of memory formats.\n",
      " |\n",
      " |      .. warning::\n",
      " |          The dim_order tensor API is experimental and subject to change.\n",
      " |\n",
      " |  eig(self, eigenvectors=False) from torch._tensor.Tensor\n",
      " |\n",
      " |  is_shared(self) from torch._tensor.Tensor\n",
      " |      Checks if tensor is in shared memory.\n",
      " |\n",
      " |      This is always ``True`` for CUDA tensors.\n",
      " |\n",
      " |  istft(self, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: 'Optional[Tensor]' = None, center: bool = True, normalized: bool = False, onesided: Optional[bool] = None, length: Optional[int] = None, return_complex: bool = False) from torch._tensor.Tensor\n",
      " |      See :func:`torch.istft`\n",
      " |\n",
      " |  lstsq(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  lu(self, pivot=True, get_infos=False) from torch._tensor.Tensor\n",
      " |      See :func:`torch.lu`\n",
      " |\n",
      " |  module_load(self, other, assign=False) from torch._tensor.Tensor\n",
      " |      Defines how to transform ``other`` when loading it into ``self`` in :meth:`~nn.Module.load_state_dict`.\n",
      " |\n",
      " |      Used when :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\n",
      " |\n",
      " |      It is expected that ``self`` is a parameter or buffer in an ``nn.Module`` and ``other`` is the\n",
      " |      value in the state dictionary with the corresponding key, this method defines\n",
      " |      how ``other`` is remapped before being swapped with ``self`` via\n",
      " |      :func:`~torch.utils.swap_tensors` in :meth:`~nn.Module.load_state_dict`.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method should always return a new object that is not ``self`` or ``other``.\n",
      " |          For example, the default implementation returns ``self.copy_(other).detach()``\n",
      " |          if ``assign`` is ``False`` or ``other.detach()`` if ``assign`` is ``True``.\n",
      " |\n",
      " |      Args:\n",
      " |          other (Tensor): value in state dict with key corresponding to ``self``\n",
      " |          assign (bool): the assign argument passed to :meth:`nn.Module.load_state_dict`\n",
      " |\n",
      " |  norm(self, p: Union[float, str, NoneType] = 'fro', dim=None, keepdim=False, dtype=None) from torch._tensor.Tensor\n",
      " |      See :func:`torch.norm`\n",
      " |\n",
      " |  refine_names(self, *names) from torch._tensor.Tensor\n",
      " |      Refines the dimension names of :attr:`self` according to :attr:`names`.\n",
      " |\n",
      " |      Refining is a special case of renaming that \"lifts\" unnamed dimensions.\n",
      " |      A ``None`` dim can be refined to have any name; a named dim can only be\n",
      " |      refined to have the same name.\n",
      " |\n",
      " |      Because named tensors can coexist with unnamed tensors, refining names\n",
      " |      gives a nice way to write named-tensor-aware code that works with both\n",
      " |      named and unnamed tensors.\n",
      " |\n",
      " |      :attr:`names` may contain up to one Ellipsis (``...``).\n",
      " |      The Ellipsis is expanded greedily; it is expanded in-place to fill\n",
      " |      :attr:`names` to the same length as ``self.dim()`` using names from the\n",
      " |      corresponding indices of ``self.names``.\n",
      " |\n",
      " |      Python 2 does not support Ellipsis but one may use a string literal\n",
      " |      instead (``'...'``).\n",
      " |\n",
      " |      Args:\n",
      " |          names (iterable of str): The desired names of the output tensor. May\n",
      " |              contain up to one Ellipsis.\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |          >>> imgs = torch.randn(32, 3, 128, 128)\n",
      " |          >>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')\n",
      " |          >>> named_imgs.names\n",
      " |          ('N', 'C', 'H', 'W')\n",
      " |\n",
      " |          >>> tensor = torch.randn(2, 3, 5, 7, 11)\n",
      " |          >>> tensor = tensor.refine_names('A', ..., 'B', 'C')\n",
      " |          >>> tensor.names\n",
      " |          ('A', None, None, 'B', 'C')\n",
      " |\n",
      " |      .. warning::\n",
      " |          The named tensor API is experimental and subject to change.\n",
      " |\n",
      " |  register_hook(self, hook) from torch._tensor.Tensor\n",
      " |      Registers a backward hook.\n",
      " |\n",
      " |      The hook will be called every time a gradient with respect to the\n",
      " |      Tensor is computed. The hook should have the following signature::\n",
      " |\n",
      " |          hook(grad) -> Tensor or None\n",
      " |\n",
      " |\n",
      " |      The hook should not modify its argument, but it can optionally return\n",
      " |      a new gradient which will be used in place of :attr:`grad`.\n",
      " |\n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |\n",
      " |      .. note::\n",
      " |          See :ref:`backward-hooks-execution` for more information on how when this hook\n",
      " |          is executed, and how its execution is ordered relative to other hooks.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> v = torch.tensor([0., 0., 0.], requires_grad=True)\n",
      " |          >>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n",
      " |          >>> v.backward(torch.tensor([1., 2., 3.]))\n",
      " |          >>> v.grad\n",
      " |\n",
      " |           2\n",
      " |           4\n",
      " |           6\n",
      " |          [torch.FloatTensor of size (3,)]\n",
      " |\n",
      " |          >>> h.remove()  # removes the hook\n",
      " |\n",
      " |  register_post_accumulate_grad_hook(self, hook) from torch._tensor.Tensor\n",
      " |      Registers a backward hook that runs after grad accumulation.\n",
      " |\n",
      " |      The hook will be called after all gradients for a tensor have been accumulated,\n",
      " |      meaning that the .grad field has been updated on that tensor. The post\n",
      " |      accumulate grad hook is ONLY applicable for leaf tensors (tensors without a\n",
      " |      .grad_fn field). Registering this hook on a non-leaf tensor will error!\n",
      " |\n",
      " |      The hook should have the following signature::\n",
      " |\n",
      " |          hook(param: Tensor) -> None\n",
      " |\n",
      " |      Note that, unlike other autograd hooks, this hook operates on the tensor\n",
      " |      that requires grad and not the grad itself. The hook can in-place modify\n",
      " |      and access its Tensor argument, including its .grad field.\n",
      " |\n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |\n",
      " |      .. note::\n",
      " |          See :ref:`backward-hooks-execution` for more information on how when this hook\n",
      " |          is executed, and how its execution is ordered relative to other hooks. Since\n",
      " |          this hook runs during the backward pass, it will run in no_grad mode (unless\n",
      " |          create_graph is True). You can use torch.enable_grad() to re-enable autograd\n",
      " |          within the hook if you need it.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> v = torch.tensor([0., 0., 0.], requires_grad=True)\n",
      " |          >>> lr = 0.01\n",
      " |          >>> # simulate a simple SGD update\n",
      " |          >>> h = v.register_post_accumulate_grad_hook(lambda p: p.add_(p.grad, alpha=-lr))\n",
      " |          >>> v.backward(torch.tensor([1., 2., 3.]))\n",
      " |          >>> v\n",
      " |          tensor([-0.0100, -0.0200, -0.0300], requires_grad=True)\n",
      " |\n",
      " |          >>> h.remove()  # removes the hook\n",
      " |\n",
      " |  reinforce(self, reward) from torch._tensor.Tensor\n",
      " |\n",
      " |  rename(self, *names, **rename_map) from torch._tensor.Tensor\n",
      " |      Renames dimension names of :attr:`self`.\n",
      " |\n",
      " |      There are two main usages:\n",
      " |\n",
      " |      ``self.rename(**rename_map)`` returns a view on tensor that has dims\n",
      " |      renamed as specified in the mapping :attr:`rename_map`.\n",
      " |\n",
      " |      ``self.rename(*names)`` returns a view on tensor, renaming all\n",
      " |      dimensions positionally using :attr:`names`.\n",
      " |      Use ``self.rename(None)`` to drop names on a tensor.\n",
      " |\n",
      " |      One cannot specify both positional args :attr:`names` and keyword args\n",
      " |      :attr:`rename_map`.\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |          >>> imgs = torch.rand(2, 3, 5, 7, names=('N', 'C', 'H', 'W'))\n",
      " |          >>> renamed_imgs = imgs.rename(N='batch', C='channels')\n",
      " |          >>> renamed_imgs.names\n",
      " |          ('batch', 'channels', 'H', 'W')\n",
      " |\n",
      " |          >>> renamed_imgs = imgs.rename(None)\n",
      " |          >>> renamed_imgs.names\n",
      " |          (None, None, None, None)\n",
      " |\n",
      " |          >>> renamed_imgs = imgs.rename('batch', 'channel', 'height', 'width')\n",
      " |          >>> renamed_imgs.names\n",
      " |          ('batch', 'channel', 'height', 'width')\n",
      " |\n",
      " |      .. warning::\n",
      " |          The named tensor API is experimental and subject to change.\n",
      " |\n",
      " |  rename_(self, *names, **rename_map) from torch._tensor.Tensor\n",
      " |      In-place version of :meth:`~Tensor.rename`.\n",
      " |\n",
      " |  resize(self, *sizes) from torch._tensor.Tensor\n",
      " |\n",
      " |  resize_as(self, tensor) from torch._tensor.Tensor\n",
      " |\n",
      " |  share_memory_(self) from torch._tensor.Tensor\n",
      " |      Moves the underlying storage to shared memory.\n",
      " |\n",
      " |      This is a no-op if the underlying storage is already in shared memory\n",
      " |      and for CUDA tensors. Tensors in shared memory cannot be resized.\n",
      " |\n",
      " |      See :meth:`torch.UntypedStorage.share_memory_` for more details.\n",
      " |\n",
      " |  solve(self, other) from torch._tensor.Tensor\n",
      " |\n",
      " |  split(self, split_size, dim=0) from torch._tensor.Tensor\n",
      " |      See :func:`torch.split`\n",
      " |\n",
      " |  stft(self, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: 'Optional[Tensor]' = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: Optional[bool] = None, return_complex: Optional[bool] = None, align_to_window: Optional[bool] = None) from torch._tensor.Tensor\n",
      " |      See :func:`torch.stft`\n",
      " |\n",
      " |      .. warning::\n",
      " |        This function changed signature at version 0.4.1. Calling with\n",
      " |        the previous signature may cause error or return incorrect result.\n",
      " |\n",
      " |  storage(self) from torch._tensor.Tensor\n",
      " |      storage() -> torch.TypedStorage\n",
      " |\n",
      " |      Returns the underlying :class:`TypedStorage`.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          :class:`TypedStorage` is deprecated. It will be removed in the future, and\n",
      " |          :class:`UntypedStorage` will be the only storage class. To access the\n",
      " |          :class:`UntypedStorage` directly, use :attr:`Tensor.untyped_storage()`.\n",
      " |\n",
      " |  storage_type(self) from torch._tensor.Tensor\n",
      " |      storage_type() -> type\n",
      " |\n",
      " |      Returns the type of the underlying storage.\n",
      " |\n",
      " |  symeig(self, eigenvectors=False) from torch._tensor.Tensor\n",
      " |\n",
      " |  to_sparse_coo(self) from torch._tensor.Tensor\n",
      " |      Convert a tensor to :ref:`coordinate format <sparse-coo-docs>`.\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |           >>> dense = torch.randn(5, 5)\n",
      " |           >>> sparse = dense.to_sparse_coo()\n",
      " |           >>> sparse._nnz()\n",
      " |           25\n",
      " |\n",
      " |  unflatten(self, dim, sizes) from torch._tensor.Tensor\n",
      " |      unflatten(dim, sizes) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.unflatten`.\n",
      " |\n",
      " |  unique(self, sorted=True, return_inverse=False, return_counts=False, dim=None) from torch._tensor.Tensor\n",
      " |      Returns the unique elements of the input tensor.\n",
      " |\n",
      " |      See :func:`torch.unique`\n",
      " |\n",
      " |  unique_consecutive(self, return_inverse=False, return_counts=False, dim=None) from torch._tensor.Tensor\n",
      " |      Eliminates all but the first element from every consecutive group of equivalent elements.\n",
      " |\n",
      " |      See :func:`torch.unique_consecutive`\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  __torch_function__(func, types, args=(), kwargs=None) from torch._tensor.Tensor\n",
      " |      This __torch_function__ implementation wraps subclasses such that\n",
      " |      methods called on subclasses return a subclass instance instead of\n",
      " |      a ``torch.Tensor`` instance.\n",
      " |\n",
      " |      One corollary to this is that you need coverage for torch.Tensor\n",
      " |      methods if implementing __torch_function__ for subclasses.\n",
      " |\n",
      " |      We recommend always calling ``super().__torch_function__`` as the base\n",
      " |      case when doing the above.\n",
      " |\n",
      " |      While not mandatory, we recommend making `__torch_function__` a classmethod.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |\n",
      " |  __torch_dispatch__ = _disabled_torch_dispatch_impl(...)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  __cuda_array_interface__\n",
      " |      Array view description for cuda tensors.\n",
      " |\n",
      " |      See:\n",
      " |      https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'_is_param': <class 'bool'>}\n",
      " |\n",
      " |  __array_priority__ = 1000\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch._C.TensorBase:\n",
      " |\n",
      " |  __add__(...)\n",
      " |\n",
      " |  __and__(...)\n",
      " |\n",
      " |  __bool__(...)\n",
      " |\n",
      " |  __complex__(...)\n",
      " |\n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |\n",
      " |  __div__(...)\n",
      " |\n",
      " |  __eq__(...)\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __float__(...)\n",
      " |\n",
      " |  __ge__(...)\n",
      " |      Return self>=value.\n",
      " |\n",
      " |  __getitem__(self, key, /)\n",
      " |      Return self[key].\n",
      " |\n",
      " |  __gt__(...)\n",
      " |      Return self>value.\n",
      " |\n",
      " |  __iadd__(...)\n",
      " |\n",
      " |  __iand__(...)\n",
      " |\n",
      " |  __idiv__(...)\n",
      " |\n",
      " |  __ifloordiv__(...)\n",
      " |\n",
      " |  __ilshift__(...)\n",
      " |\n",
      " |  __imod__(...)\n",
      " |\n",
      " |  __imul__(...)\n",
      " |\n",
      " |  __index__(...)\n",
      " |\n",
      " |  __int__(...)\n",
      " |\n",
      " |  __invert__(...)\n",
      " |\n",
      " |  __ior__(...)\n",
      " |\n",
      " |  __irshift__(...)\n",
      " |\n",
      " |  __isub__(...)\n",
      " |\n",
      " |  __ixor__(...)\n",
      " |\n",
      " |  __le__(...)\n",
      " |      Return self<=value.\n",
      " |\n",
      " |  __long__(...)\n",
      " |\n",
      " |  __lshift__(...)\n",
      " |\n",
      " |  __lt__(...)\n",
      " |      Return self<value.\n",
      " |\n",
      " |  __matmul__(...)\n",
      " |\n",
      " |  __mod__(...)\n",
      " |\n",
      " |  __mul__(...)\n",
      " |\n",
      " |  __ne__(...)\n",
      " |      Return self!=value.\n",
      " |\n",
      " |  __nonzero__(...)\n",
      " |\n",
      " |  __or__(...)\n",
      " |      Return self|value.\n",
      " |\n",
      " |  __radd__(...)\n",
      " |\n",
      " |  __rand__(...)\n",
      " |\n",
      " |  __rmul__(...)\n",
      " |\n",
      " |  __ror__(...)\n",
      " |      Return value|self.\n",
      " |\n",
      " |  __rshift__(...)\n",
      " |\n",
      " |  __rxor__(...)\n",
      " |\n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |\n",
      " |  __sub__(...)\n",
      " |\n",
      " |  __truediv__(...)\n",
      " |\n",
      " |  __xor__(...)\n",
      " |\n",
      " |  abs(...)\n",
      " |      abs() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.abs`\n",
      " |\n",
      " |  abs_(...)\n",
      " |      abs_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.abs`\n",
      " |\n",
      " |  absolute(...)\n",
      " |      absolute() -> Tensor\n",
      " |\n",
      " |      Alias for :func:`abs`\n",
      " |\n",
      " |  absolute_(...)\n",
      " |      absolute_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.absolute`\n",
      " |      Alias for :func:`abs_`\n",
      " |\n",
      " |  acos(...)\n",
      " |      acos() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.acos`\n",
      " |\n",
      " |  acos_(...)\n",
      " |      acos_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.acos`\n",
      " |\n",
      " |  acosh(...)\n",
      " |      acosh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.acosh`\n",
      " |\n",
      " |  acosh_(...)\n",
      " |      acosh_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.acosh`\n",
      " |\n",
      " |  add(...)\n",
      " |      add(other, *, alpha=1) -> Tensor\n",
      " |\n",
      " |      Add a scalar or tensor to :attr:`self` tensor. If both :attr:`alpha`\n",
      " |      and :attr:`other` are specified, each element of :attr:`other` is scaled by\n",
      " |      :attr:`alpha` before being used.\n",
      " |\n",
      " |      When :attr:`other` is a tensor, the shape of :attr:`other` must be\n",
      " |      :ref:`broadcastable <broadcasting-semantics>` with the shape of the underlying\n",
      " |      tensor\n",
      " |\n",
      " |      See :func:`torch.add`\n",
      " |\n",
      " |  add_(...)\n",
      " |      add_(other, *, alpha=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.add`\n",
      " |\n",
      " |  addbmm(...)\n",
      " |      addbmm(batch1, batch2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.addbmm`\n",
      " |\n",
      " |  addbmm_(...)\n",
      " |      addbmm_(batch1, batch2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.addbmm`\n",
      " |\n",
      " |  addcdiv(...)\n",
      " |      addcdiv(tensor1, tensor2, *, value=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.addcdiv`\n",
      " |\n",
      " |  addcdiv_(...)\n",
      " |      addcdiv_(tensor1, tensor2, *, value=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.addcdiv`\n",
      " |\n",
      " |  addcmul(...)\n",
      " |      addcmul(tensor1, tensor2, *, value=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.addcmul`\n",
      " |\n",
      " |  addcmul_(...)\n",
      " |      addcmul_(tensor1, tensor2, *, value=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.addcmul`\n",
      " |\n",
      " |  addmm(...)\n",
      " |      addmm(mat1, mat2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.addmm`\n",
      " |\n",
      " |  addmm_(...)\n",
      " |      addmm_(mat1, mat2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.addmm`\n",
      " |\n",
      " |  addmv(...)\n",
      " |      addmv(mat, vec, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.addmv`\n",
      " |\n",
      " |  addmv_(...)\n",
      " |      addmv_(mat, vec, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.addmv`\n",
      " |\n",
      " |  addr(...)\n",
      " |      addr(vec1, vec2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.addr`\n",
      " |\n",
      " |  addr_(...)\n",
      " |      addr_(vec1, vec2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.addr`\n",
      " |\n",
      " |  adjoint(...)\n",
      " |      adjoint() -> Tensor\n",
      " |\n",
      " |      Alias for :func:`adjoint`\n",
      " |\n",
      " |  align_as(...)\n",
      " |      align_as(other) -> Tensor\n",
      " |\n",
      " |      Permutes the dimensions of the :attr:`self` tensor to match the dimension order\n",
      " |      in the :attr:`other` tensor, adding size-one dims for any new names.\n",
      " |\n",
      " |      This operation is useful for explicit broadcasting by names (see examples).\n",
      " |\n",
      " |      All of the dims of :attr:`self` must be named in order to use this method.\n",
      " |      The resulting tensor is a view on the original tensor.\n",
      " |\n",
      " |      All dimension names of :attr:`self` must be present in ``other.names``.\n",
      " |      :attr:`other` may contain named dimensions that are not in ``self.names``;\n",
      " |      the output tensor has a size-one dimension for each of those new names.\n",
      " |\n",
      " |      To align a tensor to a specific order, use :meth:`~Tensor.align_to`.\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |          # Example 1: Applying a mask\n",
      " |          >>> mask = torch.randint(2, [127, 128], dtype=torch.bool).refine_names('W', 'H')\n",
      " |          >>> imgs = torch.randn(32, 128, 127, 3, names=('N', 'H', 'W', 'C'))\n",
      " |          >>> imgs.masked_fill_(mask.align_as(imgs), 0)\n",
      " |\n",
      " |\n",
      " |          # Example 2: Applying a per-channel-scale\n",
      " |          >>> def scale_channels(input, scale):\n",
      " |          >>>    scale = scale.refine_names('C')\n",
      " |          >>>    return input * scale.align_as(input)\n",
      " |\n",
      " |          >>> num_channels = 3\n",
      " |          >>> scale = torch.randn(num_channels, names=('C',))\n",
      " |          >>> imgs = torch.rand(32, 128, 128, num_channels, names=('N', 'H', 'W', 'C'))\n",
      " |          >>> more_imgs = torch.rand(32, num_channels, 128, 128, names=('N', 'C', 'H', 'W'))\n",
      " |          >>> videos = torch.randn(3, num_channels, 128, 128, 128, names=('N', 'C', 'H', 'W', 'D'))\n",
      " |\n",
      " |          # scale_channels is agnostic to the dimension order of the input\n",
      " |          >>> scale_channels(imgs, scale)\n",
      " |          >>> scale_channels(more_imgs, scale)\n",
      " |          >>> scale_channels(videos, scale)\n",
      " |\n",
      " |      .. warning::\n",
      " |          The named tensor API is experimental and subject to change.\n",
      " |\n",
      " |  all(...)\n",
      " |      all(dim=None, keepdim=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.all`\n",
      " |\n",
      " |  allclose(...)\n",
      " |      allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.allclose`\n",
      " |\n",
      " |  amax(...)\n",
      " |      amax(dim=None, keepdim=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.amax`\n",
      " |\n",
      " |  amin(...)\n",
      " |      amin(dim=None, keepdim=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.amin`\n",
      " |\n",
      " |  aminmax(...)\n",
      " |      aminmax(*, dim=None, keepdim=False) -> (Tensor min, Tensor max)\n",
      " |\n",
      " |      See :func:`torch.aminmax`\n",
      " |\n",
      " |  angle(...)\n",
      " |      angle() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.angle`\n",
      " |\n",
      " |  any(...)\n",
      " |      any(dim=None, keepdim=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.any`\n",
      " |\n",
      " |  apply_(...)\n",
      " |      apply_(callable) -> Tensor\n",
      " |\n",
      " |      Applies the function :attr:`callable` to each element in the tensor, replacing\n",
      " |      each element with the value returned by :attr:`callable`.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          This function only works with CPU tensors and should not be used in code\n",
      " |          sections that require high performance.\n",
      " |\n",
      " |  arccos(...)\n",
      " |      arccos() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.arccos`\n",
      " |\n",
      " |  arccos_(...)\n",
      " |      arccos_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.arccos`\n",
      " |\n",
      " |  arccosh(...)\n",
      " |      acosh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.arccosh`\n",
      " |\n",
      " |  arccosh_(...)\n",
      " |      acosh_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.arccosh`\n",
      " |\n",
      " |  arcsin(...)\n",
      " |      arcsin() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.arcsin`\n",
      " |\n",
      " |  arcsin_(...)\n",
      " |      arcsin_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.arcsin`\n",
      " |\n",
      " |  arcsinh(...)\n",
      " |      arcsinh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.arcsinh`\n",
      " |\n",
      " |  arcsinh_(...)\n",
      " |      arcsinh_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.arcsinh`\n",
      " |\n",
      " |  arctan(...)\n",
      " |      arctan() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.arctan`\n",
      " |\n",
      " |  arctan2(...)\n",
      " |      arctan2(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.arctan2`\n",
      " |\n",
      " |  arctan2_(...)\n",
      " |      atan2_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.arctan2`\n",
      " |\n",
      " |  arctan_(...)\n",
      " |      arctan_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.arctan`\n",
      " |\n",
      " |  arctanh(...)\n",
      " |      arctanh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.arctanh`\n",
      " |\n",
      " |  arctanh_(...)\n",
      " |      arctanh_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.arctanh`\n",
      " |\n",
      " |  argmax(...)\n",
      " |      argmax(dim=None, keepdim=False) -> LongTensor\n",
      " |\n",
      " |      See :func:`torch.argmax`\n",
      " |\n",
      " |  argmin(...)\n",
      " |      argmin(dim=None, keepdim=False) -> LongTensor\n",
      " |\n",
      " |      See :func:`torch.argmin`\n",
      " |\n",
      " |  argsort(...)\n",
      " |      argsort(dim=-1, descending=False) -> LongTensor\n",
      " |\n",
      " |      See :func:`torch.argsort`\n",
      " |\n",
      " |  argwhere(...)\n",
      " |      argwhere() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.argwhere`\n",
      " |\n",
      " |  as_strided(...)\n",
      " |      as_strided(size, stride, storage_offset=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.as_strided`\n",
      " |\n",
      " |  as_strided_(...)\n",
      " |      as_strided_(size, stride, storage_offset=None) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.as_strided`\n",
      " |\n",
      " |  as_strided_scatter(...)\n",
      " |      as_strided_scatter(src, size, stride, storage_offset=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.as_strided_scatter`\n",
      " |\n",
      " |  as_subclass(...)\n",
      " |      as_subclass(cls) -> Tensor\n",
      " |\n",
      " |      Makes a ``cls`` instance with the same data pointer as ``self``. Changes\n",
      " |      in the output mirror changes in ``self``, and the output stays attached\n",
      " |      to the autograd graph. ``cls`` must be a subclass of ``Tensor``.\n",
      " |\n",
      " |  asin(...)\n",
      " |      asin() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.asin`\n",
      " |\n",
      " |  asin_(...)\n",
      " |      asin_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.asin`\n",
      " |\n",
      " |  asinh(...)\n",
      " |      asinh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.asinh`\n",
      " |\n",
      " |  asinh_(...)\n",
      " |      asinh_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.asinh`\n",
      " |\n",
      " |  atan(...)\n",
      " |      atan() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.atan`\n",
      " |\n",
      " |  atan2(...)\n",
      " |      atan2(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.atan2`\n",
      " |\n",
      " |  atan2_(...)\n",
      " |      atan2_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.atan2`\n",
      " |\n",
      " |  atan_(...)\n",
      " |      atan_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.atan`\n",
      " |\n",
      " |  atanh(...)\n",
      " |      atanh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.atanh`\n",
      " |\n",
      " |  atanh_(...)\n",
      " |      atanh_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.atanh`\n",
      " |\n",
      " |  baddbmm(...)\n",
      " |      baddbmm(batch1, batch2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.baddbmm`\n",
      " |\n",
      " |  baddbmm_(...)\n",
      " |      baddbmm_(batch1, batch2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.baddbmm`\n",
      " |\n",
      " |  bernoulli(...)\n",
      " |      bernoulli(*, generator=None) -> Tensor\n",
      " |\n",
      " |      Returns a result tensor where each :math:`\\texttt{result[i]}` is independently\n",
      " |      sampled from :math:`\\text{Bernoulli}(\\texttt{self[i]})`. :attr:`self` must have\n",
      " |      floating point ``dtype``, and the result will have the same ``dtype``.\n",
      " |\n",
      " |      See :func:`torch.bernoulli`\n",
      " |\n",
      " |  bernoulli_(...)\n",
      " |      bernoulli_(p=0.5, *, generator=None) -> Tensor\n",
      " |\n",
      " |      Fills each location of :attr:`self` with an independent sample from\n",
      " |      :math:`\\text{Bernoulli}(\\texttt{p})`. :attr:`self` can have integral\n",
      " |      ``dtype``.\n",
      " |\n",
      " |      :attr:`p` should either be a scalar or tensor containing probabilities to be\n",
      " |      used for drawing the binary random number.\n",
      " |\n",
      " |      If it is a tensor, the :math:`\\text{i}^{th}` element of :attr:`self` tensor\n",
      " |      will be set to a value sampled from\n",
      " |      :math:`\\text{Bernoulli}(\\texttt{p\\_tensor[i]})`. In this case `p` must have\n",
      " |      floating point ``dtype``.\n",
      " |\n",
      " |      See also :meth:`~Tensor.bernoulli` and :func:`torch.bernoulli`\n",
      " |\n",
      " |  bfloat16(...)\n",
      " |      bfloat16(memory_format=torch.preserve_format) -> Tensor\n",
      " |      ``self.bfloat16()`` is equivalent to ``self.to(torch.bfloat16)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  bincount(...)\n",
      " |      bincount(weights=None, minlength=0) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.bincount`\n",
      " |\n",
      " |  bitwise_and(...)\n",
      " |      bitwise_and() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.bitwise_and`\n",
      " |\n",
      " |  bitwise_and_(...)\n",
      " |      bitwise_and_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.bitwise_and`\n",
      " |\n",
      " |  bitwise_left_shift(...)\n",
      " |      bitwise_left_shift(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.bitwise_left_shift`\n",
      " |\n",
      " |  bitwise_left_shift_(...)\n",
      " |      bitwise_left_shift_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.bitwise_left_shift`\n",
      " |\n",
      " |  bitwise_not(...)\n",
      " |      bitwise_not() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.bitwise_not`\n",
      " |\n",
      " |  bitwise_not_(...)\n",
      " |      bitwise_not_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.bitwise_not`\n",
      " |\n",
      " |  bitwise_or(...)\n",
      " |      bitwise_or() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.bitwise_or`\n",
      " |\n",
      " |  bitwise_or_(...)\n",
      " |      bitwise_or_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.bitwise_or`\n",
      " |\n",
      " |  bitwise_right_shift(...)\n",
      " |      bitwise_right_shift(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.bitwise_right_shift`\n",
      " |\n",
      " |  bitwise_right_shift_(...)\n",
      " |      bitwise_right_shift_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.bitwise_right_shift`\n",
      " |\n",
      " |  bitwise_xor(...)\n",
      " |      bitwise_xor() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.bitwise_xor`\n",
      " |\n",
      " |  bitwise_xor_(...)\n",
      " |      bitwise_xor_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.bitwise_xor`\n",
      " |\n",
      " |  bmm(...)\n",
      " |      bmm(batch2) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.bmm`\n",
      " |\n",
      " |  bool(...)\n",
      " |      bool(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.bool()`` is equivalent to ``self.to(torch.bool)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  broadcast_to(...)\n",
      " |      broadcast_to(shape) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.broadcast_to`.\n",
      " |\n",
      " |  byte(...)\n",
      " |      byte(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.byte()`` is equivalent to ``self.to(torch.uint8)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  cauchy_(...)\n",
      " |      cauchy_(median=0, sigma=1, *, generator=None) -> Tensor\n",
      " |\n",
      " |      Fills the tensor with numbers drawn from the Cauchy distribution:\n",
      " |\n",
      " |      .. math::\n",
      " |\n",
      " |          f(x) = \\dfrac{1}{\\pi} \\dfrac{\\sigma}{(x - \\text{median})^2 + \\sigma^2}\n",
      " |\n",
      " |      .. note::\n",
      " |        Sigma (:math:`\\sigma`) is used to denote the scale parameter in Cauchy distribution.\n",
      " |\n",
      " |  ccol_indices(...)\n",
      " |\n",
      " |  cdouble(...)\n",
      " |      cdouble(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.cdouble()`` is equivalent to ``self.to(torch.complex128)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  ceil(...)\n",
      " |      ceil() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.ceil`\n",
      " |\n",
      " |  ceil_(...)\n",
      " |      ceil_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.ceil`\n",
      " |\n",
      " |  cfloat(...)\n",
      " |      cfloat(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.cfloat()`` is equivalent to ``self.to(torch.complex64)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  chalf(...)\n",
      " |      chalf(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.chalf()`` is equivalent to ``self.to(torch.complex32)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |           memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  char(...)\n",
      " |      char(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.char()`` is equivalent to ``self.to(torch.int8)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  cholesky(...)\n",
      " |      cholesky(upper=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cholesky`\n",
      " |\n",
      " |  cholesky_inverse(...)\n",
      " |      cholesky_inverse(upper=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cholesky_inverse`\n",
      " |\n",
      " |  cholesky_solve(...)\n",
      " |      cholesky_solve(input2, upper=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cholesky_solve`\n",
      " |\n",
      " |  chunk(...)\n",
      " |      chunk(chunks, dim=0) -> List of Tensors\n",
      " |\n",
      " |      See :func:`torch.chunk`\n",
      " |\n",
      " |  clamp(...)\n",
      " |      clamp(min=None, max=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.clamp`\n",
      " |\n",
      " |  clamp_(...)\n",
      " |      clamp_(min=None, max=None) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.clamp`\n",
      " |\n",
      " |  clamp_max(...)\n",
      " |\n",
      " |  clamp_max_(...)\n",
      " |\n",
      " |  clamp_min(...)\n",
      " |\n",
      " |  clamp_min_(...)\n",
      " |\n",
      " |  clip(...)\n",
      " |      clip(min=None, max=None) -> Tensor\n",
      " |\n",
      " |      Alias for :meth:`~Tensor.clamp`.\n",
      " |\n",
      " |  clip_(...)\n",
      " |      clip_(min=None, max=None) -> Tensor\n",
      " |\n",
      " |      Alias for :meth:`~Tensor.clamp_`.\n",
      " |\n",
      " |  clone(...)\n",
      " |      clone(*, memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.clone`\n",
      " |\n",
      " |  coalesce(...)\n",
      " |      coalesce() -> Tensor\n",
      " |\n",
      " |      Returns a coalesced copy of :attr:`self` if :attr:`self` is an\n",
      " |      :ref:`uncoalesced tensor <sparse-uncoalesced-coo-docs>`.\n",
      " |\n",
      " |      Returns :attr:`self` if :attr:`self` is a coalesced tensor.\n",
      " |\n",
      " |      .. warning::\n",
      " |        Throws an error if :attr:`self` is not a sparse COO tensor.\n",
      " |\n",
      " |  col_indices(...)\n",
      " |      col_indices() -> IntTensor\n",
      " |\n",
      " |      Returns the tensor containing the column indices of the :attr:`self`\n",
      " |      tensor when :attr:`self` is a sparse CSR tensor of layout ``sparse_csr``.\n",
      " |      The ``col_indices`` tensor is strictly of shape (:attr:`self`.nnz())\n",
      " |      and of type ``int32`` or ``int64``.  When using MKL routines such as sparse\n",
      " |      matrix multiplication, it is necessary to use ``int32`` indexing in order\n",
      " |      to avoid downcasting and potentially losing information.\n",
      " |\n",
      " |      Example::\n",
      " |          >>> csr = torch.eye(5,5).to_sparse_csr()\n",
      " |          >>> csr.col_indices()\n",
      " |          tensor([0, 1, 2, 3, 4], dtype=torch.int32)\n",
      " |\n",
      " |  conj(...)\n",
      " |      conj() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.conj`\n",
      " |\n",
      " |  conj_physical(...)\n",
      " |      conj_physical() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.conj_physical`\n",
      " |\n",
      " |  conj_physical_(...)\n",
      " |      conj_physical_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.conj_physical`\n",
      " |\n",
      " |  contiguous(...)\n",
      " |      contiguous(memory_format=torch.contiguous_format) -> Tensor\n",
      " |\n",
      " |      Returns a contiguous in memory tensor containing the same data as :attr:`self` tensor. If\n",
      " |      :attr:`self` tensor is already in the specified memory format, this function returns the\n",
      " |      :attr:`self` tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.contiguous_format``.\n",
      " |\n",
      " |  copy_(...)\n",
      " |      copy_(src, non_blocking=False) -> Tensor\n",
      " |\n",
      " |      Copies the elements from :attr:`src` into :attr:`self` tensor and returns\n",
      " |      :attr:`self`.\n",
      " |\n",
      " |      The :attr:`src` tensor must be :ref:`broadcastable <broadcasting-semantics>`\n",
      " |      with the :attr:`self` tensor. It may be of a different data type or reside on a\n",
      " |      different device.\n",
      " |\n",
      " |      Args:\n",
      " |          src (Tensor): the source tensor to copy from\n",
      " |          non_blocking (bool): if ``True`` and this copy is between CPU and GPU,\n",
      " |              the copy may occur asynchronously with respect to the host. For other\n",
      " |              cases, this argument has no effect.\n",
      " |\n",
      " |  copysign(...)\n",
      " |      copysign(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.copysign`\n",
      " |\n",
      " |  copysign_(...)\n",
      " |      copysign_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.copysign`\n",
      " |\n",
      " |  corrcoef(...)\n",
      " |      corrcoef() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.corrcoef`\n",
      " |\n",
      " |  cos(...)\n",
      " |      cos() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cos`\n",
      " |\n",
      " |  cos_(...)\n",
      " |      cos_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.cos`\n",
      " |\n",
      " |  cosh(...)\n",
      " |      cosh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cosh`\n",
      " |\n",
      " |  cosh_(...)\n",
      " |      cosh_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.cosh`\n",
      " |\n",
      " |  count_nonzero(...)\n",
      " |      count_nonzero(dim=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.count_nonzero`\n",
      " |\n",
      " |  cov(...)\n",
      " |      cov(*, correction=1, fweights=None, aweights=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cov`\n",
      " |\n",
      " |  cpu(...)\n",
      " |      cpu(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      Returns a copy of this object in CPU memory.\n",
      " |\n",
      " |      If this object is already in CPU memory,\n",
      " |      then no copy is performed and the original object is returned.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  cross(...)\n",
      " |      cross(other, dim=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cross`\n",
      " |\n",
      " |  crow_indices(...)\n",
      " |      crow_indices() -> IntTensor\n",
      " |\n",
      " |      Returns the tensor containing the compressed row indices of the :attr:`self`\n",
      " |      tensor when :attr:`self` is a sparse CSR tensor of layout ``sparse_csr``.\n",
      " |      The ``crow_indices`` tensor is strictly of shape (:attr:`self`.size(0) + 1)\n",
      " |      and of type ``int32`` or ``int64``. When using MKL routines such as sparse\n",
      " |      matrix multiplication, it is necessary to use ``int32`` indexing in order\n",
      " |      to avoid downcasting and potentially losing information.\n",
      " |\n",
      " |      Example::\n",
      " |          >>> csr = torch.eye(5,5).to_sparse_csr()\n",
      " |          >>> csr.crow_indices()\n",
      " |          tensor([0, 1, 2, 3, 4, 5], dtype=torch.int32)\n",
      " |\n",
      " |  cuda(...)\n",
      " |      cuda(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      Returns a copy of this object in CUDA memory.\n",
      " |\n",
      " |      If this object is already in CUDA memory and on the correct device,\n",
      " |      then no copy is performed and the original object is returned.\n",
      " |\n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The destination GPU device.\n",
      " |              Defaults to the current CUDA device.\n",
      " |          non_blocking (bool): If ``True`` and the source is in pinned memory,\n",
      " |              the copy will be asynchronous with respect to the host.\n",
      " |              Otherwise, the argument has no effect. Default: ``False``.\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  cummax(...)\n",
      " |      cummax(dim) -> (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.cummax`\n",
      " |\n",
      " |  cummin(...)\n",
      " |      cummin(dim) -> (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.cummin`\n",
      " |\n",
      " |  cumprod(...)\n",
      " |      cumprod(dim, dtype=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cumprod`\n",
      " |\n",
      " |  cumprod_(...)\n",
      " |      cumprod_(dim, dtype=None) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.cumprod`\n",
      " |\n",
      " |  cumsum(...)\n",
      " |      cumsum(dim, dtype=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.cumsum`\n",
      " |\n",
      " |  cumsum_(...)\n",
      " |      cumsum_(dim, dtype=None) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.cumsum`\n",
      " |\n",
      " |  data_ptr(...)\n",
      " |      data_ptr() -> int\n",
      " |\n",
      " |      Returns the address of the first element of :attr:`self` tensor.\n",
      " |\n",
      " |  deg2rad(...)\n",
      " |      deg2rad() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.deg2rad`\n",
      " |\n",
      " |  deg2rad_(...)\n",
      " |      deg2rad_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.deg2rad`\n",
      " |\n",
      " |  dense_dim(...)\n",
      " |      dense_dim() -> int\n",
      " |\n",
      " |      Return the number of dense dimensions in a :ref:`sparse tensor <sparse-docs>` :attr:`self`.\n",
      " |\n",
      " |      .. note::\n",
      " |        Returns ``len(self.shape)`` if :attr:`self` is not a sparse tensor.\n",
      " |\n",
      " |      See also :meth:`Tensor.sparse_dim` and :ref:`hybrid tensors <sparse-hybrid-coo-docs>`.\n",
      " |\n",
      " |  dequantize(...)\n",
      " |      dequantize() -> Tensor\n",
      " |\n",
      " |      Given a quantized Tensor, dequantize it and return the dequantized float Tensor.\n",
      " |\n",
      " |  det(...)\n",
      " |      det() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.det`\n",
      " |\n",
      " |  diag(...)\n",
      " |      diag(diagonal=0) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.diag`\n",
      " |\n",
      " |  diag_embed(...)\n",
      " |      diag_embed(offset=0, dim1=-2, dim2=-1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.diag_embed`\n",
      " |\n",
      " |  diagflat(...)\n",
      " |      diagflat(offset=0) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.diagflat`\n",
      " |\n",
      " |  diagonal(...)\n",
      " |      diagonal(offset=0, dim1=0, dim2=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.diagonal`\n",
      " |\n",
      " |  diagonal_scatter(...)\n",
      " |      diagonal_scatter(src, offset=0, dim1=0, dim2=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.diagonal_scatter`\n",
      " |\n",
      " |  diff(...)\n",
      " |      diff(n=1, dim=-1, prepend=None, append=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.diff`\n",
      " |\n",
      " |  digamma(...)\n",
      " |      digamma() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.digamma`\n",
      " |\n",
      " |  digamma_(...)\n",
      " |      digamma_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.digamma`\n",
      " |\n",
      " |  dim(...)\n",
      " |      dim() -> int\n",
      " |\n",
      " |      Returns the number of dimensions of :attr:`self` tensor.\n",
      " |\n",
      " |  dist(...)\n",
      " |      dist(other, p=2) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.dist`\n",
      " |\n",
      " |  div(...)\n",
      " |      div(value, *, rounding_mode=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.div`\n",
      " |\n",
      " |  div_(...)\n",
      " |      div_(value, *, rounding_mode=None) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.div`\n",
      " |\n",
      " |  divide(...)\n",
      " |      divide(value, *, rounding_mode=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.divide`\n",
      " |\n",
      " |  divide_(...)\n",
      " |      divide_(value, *, rounding_mode=None) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.divide`\n",
      " |\n",
      " |  dot(...)\n",
      " |      dot(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.dot`\n",
      " |\n",
      " |  double(...)\n",
      " |      double(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.double()`` is equivalent to ``self.to(torch.float64)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  dsplit(...)\n",
      " |      dsplit(split_size_or_sections) -> List of Tensors\n",
      " |\n",
      " |      See :func:`torch.dsplit`\n",
      " |\n",
      " |  element_size(...)\n",
      " |      element_size() -> int\n",
      " |\n",
      " |      Returns the size in bytes of an individual element.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> torch.tensor([]).element_size()\n",
      " |          4\n",
      " |          >>> torch.tensor([], dtype=torch.uint8).element_size()\n",
      " |          1\n",
      " |\n",
      " |  eq(...)\n",
      " |      eq(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.eq`\n",
      " |\n",
      " |  eq_(...)\n",
      " |      eq_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.eq`\n",
      " |\n",
      " |  equal(...)\n",
      " |      equal(other) -> bool\n",
      " |\n",
      " |      See :func:`torch.equal`\n",
      " |\n",
      " |  erf(...)\n",
      " |      erf() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.erf`\n",
      " |\n",
      " |  erf_(...)\n",
      " |      erf_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.erf`\n",
      " |\n",
      " |  erfc(...)\n",
      " |      erfc() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.erfc`\n",
      " |\n",
      " |  erfc_(...)\n",
      " |      erfc_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.erfc`\n",
      " |\n",
      " |  erfinv(...)\n",
      " |      erfinv() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.erfinv`\n",
      " |\n",
      " |  erfinv_(...)\n",
      " |      erfinv_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.erfinv`\n",
      " |\n",
      " |  exp(...)\n",
      " |      exp() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.exp`\n",
      " |\n",
      " |  exp2(...)\n",
      " |      exp2() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.exp2`\n",
      " |\n",
      " |  exp2_(...)\n",
      " |      exp2_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.exp2`\n",
      " |\n",
      " |  exp_(...)\n",
      " |      exp_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.exp`\n",
      " |\n",
      " |  expand(...)\n",
      " |      expand(*sizes) -> Tensor\n",
      " |\n",
      " |      Returns a new view of the :attr:`self` tensor with singleton dimensions expanded\n",
      " |      to a larger size.\n",
      " |\n",
      " |      Passing -1 as the size for a dimension means not changing the size of\n",
      " |      that dimension.\n",
      " |\n",
      " |      Tensor can be also expanded to a larger number of dimensions, and the\n",
      " |      new ones will be appended at the front. For the new dimensions, the\n",
      " |      size cannot be set to -1.\n",
      " |\n",
      " |      Expanding a tensor does not allocate new memory, but only creates a\n",
      " |      new view on the existing tensor where a dimension of size one is\n",
      " |      expanded to a larger size by setting the ``stride`` to 0. Any dimension\n",
      " |      of size 1 can be expanded to an arbitrary value without allocating new\n",
      " |      memory.\n",
      " |\n",
      " |      Args:\n",
      " |          *sizes (torch.Size or int...): the desired expanded size\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          More than one element of an expanded tensor may refer to a single\n",
      " |          memory location. As a result, in-place operations (especially ones that\n",
      " |          are vectorized) may result in incorrect behavior. If you need to write\n",
      " |          to the tensors, please clone them first.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.tensor([[1], [2], [3]])\n",
      " |          >>> x.size()\n",
      " |          torch.Size([3, 1])\n",
      " |          >>> x.expand(3, 4)\n",
      " |          tensor([[ 1,  1,  1,  1],\n",
      " |                  [ 2,  2,  2,  2],\n",
      " |                  [ 3,  3,  3,  3]])\n",
      " |          >>> x.expand(-1, 4)   # -1 means not changing the size of that dimension\n",
      " |          tensor([[ 1,  1,  1,  1],\n",
      " |                  [ 2,  2,  2,  2],\n",
      " |                  [ 3,  3,  3,  3]])\n",
      " |\n",
      " |  expand_as(...)\n",
      " |      expand_as(other) -> Tensor\n",
      " |\n",
      " |      Expand this tensor to the same size as :attr:`other`.\n",
      " |      ``self.expand_as(other)`` is equivalent to ``self.expand(other.size())``.\n",
      " |\n",
      " |      Please see :meth:`~Tensor.expand` for more information about ``expand``.\n",
      " |\n",
      " |      Args:\n",
      " |          other (:class:`torch.Tensor`): The result tensor has the same size\n",
      " |              as :attr:`other`.\n",
      " |\n",
      " |  expm1(...)\n",
      " |      expm1() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.expm1`\n",
      " |\n",
      " |  expm1_(...)\n",
      " |      expm1_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.expm1`\n",
      " |\n",
      " |  exponential_(...)\n",
      " |      exponential_(lambd=1, *, generator=None) -> Tensor\n",
      " |\n",
      " |      Fills :attr:`self` tensor with elements drawn from the PDF (probability density function):\n",
      " |\n",
      " |      .. math::\n",
      " |\n",
      " |          f(x) = \\lambda e^{-\\lambda x}, x > 0\n",
      " |\n",
      " |      .. note::\n",
      " |        In probability theory, exponential distribution is supported on interval [0, :math:`\\inf`) (i.e., :math:`x >= 0`)\n",
      " |        implying that zero can be sampled from the exponential distribution.\n",
      " |        However, :func:`torch.Tensor.exponential_` does not sample zero,\n",
      " |        which means that its actual support is the interval (0, :math:`\\inf`).\n",
      " |\n",
      " |        Note that :func:`torch.distributions.exponential.Exponential` is supported on the interval [0, :math:`\\inf`) and can sample zero.\n",
      " |\n",
      " |  fill_(...)\n",
      " |      fill_(value) -> Tensor\n",
      " |\n",
      " |      Fills :attr:`self` tensor with the specified value.\n",
      " |\n",
      " |  fill_diagonal_(...)\n",
      " |      fill_diagonal_(fill_value, wrap=False) -> Tensor\n",
      " |\n",
      " |      Fill the main diagonal of a tensor that has at least 2-dimensions.\n",
      " |      When dims>2, all dimensions of input must be of equal length.\n",
      " |      This function modifies the input tensor in-place, and returns the input tensor.\n",
      " |\n",
      " |      Arguments:\n",
      " |          fill_value (Scalar): the fill value\n",
      " |          wrap (bool): the diagonal 'wrapped' after N columns for tall matrices.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> a = torch.zeros(3, 3)\n",
      " |          >>> a.fill_diagonal_(5)\n",
      " |          tensor([[5., 0., 0.],\n",
      " |                  [0., 5., 0.],\n",
      " |                  [0., 0., 5.]])\n",
      " |          >>> b = torch.zeros(7, 3)\n",
      " |          >>> b.fill_diagonal_(5)\n",
      " |          tensor([[5., 0., 0.],\n",
      " |                  [0., 5., 0.],\n",
      " |                  [0., 0., 5.],\n",
      " |                  [0., 0., 0.],\n",
      " |                  [0., 0., 0.],\n",
      " |                  [0., 0., 0.],\n",
      " |                  [0., 0., 0.]])\n",
      " |          >>> c = torch.zeros(7, 3)\n",
      " |          >>> c.fill_diagonal_(5, wrap=True)\n",
      " |          tensor([[5., 0., 0.],\n",
      " |                  [0., 5., 0.],\n",
      " |                  [0., 0., 5.],\n",
      " |                  [0., 0., 0.],\n",
      " |                  [5., 0., 0.],\n",
      " |                  [0., 5., 0.],\n",
      " |                  [0., 0., 5.]])\n",
      " |\n",
      " |  fix(...)\n",
      " |      fix() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.fix`.\n",
      " |\n",
      " |  fix_(...)\n",
      " |      fix_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.fix`\n",
      " |\n",
      " |  flatten(...)\n",
      " |      flatten(start_dim=0, end_dim=-1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.flatten`\n",
      " |\n",
      " |  flip(...)\n",
      " |      flip(dims) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.flip`\n",
      " |\n",
      " |  fliplr(...)\n",
      " |      fliplr() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.fliplr`\n",
      " |\n",
      " |  flipud(...)\n",
      " |      flipud() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.flipud`\n",
      " |\n",
      " |  float(...)\n",
      " |      float(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.float()`` is equivalent to ``self.to(torch.float32)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  float_power(...)\n",
      " |      float_power(exponent) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.float_power`\n",
      " |\n",
      " |  float_power_(...)\n",
      " |      float_power_(exponent) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.float_power`\n",
      " |\n",
      " |  floor(...)\n",
      " |      floor() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.floor`\n",
      " |\n",
      " |  floor_(...)\n",
      " |      floor_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.floor`\n",
      " |\n",
      " |  floor_divide(...)\n",
      " |      floor_divide(value) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.floor_divide`\n",
      " |\n",
      " |  floor_divide_(...)\n",
      " |      floor_divide_(value) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.floor_divide`\n",
      " |\n",
      " |  fmax(...)\n",
      " |      fmax(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.fmax`\n",
      " |\n",
      " |  fmin(...)\n",
      " |      fmin(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.fmin`\n",
      " |\n",
      " |  fmod(...)\n",
      " |      fmod(divisor) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.fmod`\n",
      " |\n",
      " |  fmod_(...)\n",
      " |      fmod_(divisor) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.fmod`\n",
      " |\n",
      " |  frac(...)\n",
      " |      frac() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.frac`\n",
      " |\n",
      " |  frac_(...)\n",
      " |      frac_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.frac`\n",
      " |\n",
      " |  frexp(...)\n",
      " |      frexp(input) -> (Tensor mantissa, Tensor exponent)\n",
      " |\n",
      " |      See :func:`torch.frexp`\n",
      " |\n",
      " |  gather(...)\n",
      " |      gather(dim, index) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.gather`\n",
      " |\n",
      " |  gcd(...)\n",
      " |      gcd(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.gcd`\n",
      " |\n",
      " |  gcd_(...)\n",
      " |      gcd_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.gcd`\n",
      " |\n",
      " |  ge(...)\n",
      " |      ge(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.ge`.\n",
      " |\n",
      " |  ge_(...)\n",
      " |      ge_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.ge`.\n",
      " |\n",
      " |  geometric_(...)\n",
      " |      geometric_(p, *, generator=None) -> Tensor\n",
      " |\n",
      " |      Fills :attr:`self` tensor with elements drawn from the geometric distribution:\n",
      " |\n",
      " |      .. math::\n",
      " |\n",
      " |          P(X=k) = (1 - p)^{k - 1} p, k = 1, 2, ...\n",
      " |\n",
      " |      .. note::\n",
      " |        :func:`torch.Tensor.geometric_` `k`-th trial is the first success hence draws samples in :math:`\\{1, 2, \\ldots\\}`, whereas\n",
      " |        :func:`torch.distributions.geometric.Geometric` :math:`(k+1)`-th trial is the first success\n",
      " |        hence draws samples in :math:`\\{0, 1, \\ldots\\}`.\n",
      " |\n",
      " |  geqrf(...)\n",
      " |      geqrf() -> (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.geqrf`\n",
      " |\n",
      " |  ger(...)\n",
      " |      ger(vec2) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.ger`\n",
      " |\n",
      " |  get_device(...)\n",
      " |      get_device() -> Device ordinal (Integer)\n",
      " |\n",
      " |      For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.\n",
      " |      For CPU tensors, this function returns `-1`.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.randn(3, 4, 5, device='cuda:0')\n",
      " |          >>> x.get_device()\n",
      " |          0\n",
      " |          >>> x.cpu().get_device()\n",
      " |          -1\n",
      " |\n",
      " |  greater(...)\n",
      " |      greater(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.greater`.\n",
      " |\n",
      " |  greater_(...)\n",
      " |      greater_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.greater`.\n",
      " |\n",
      " |  greater_equal(...)\n",
      " |      greater_equal(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.greater_equal`.\n",
      " |\n",
      " |  greater_equal_(...)\n",
      " |      greater_equal_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.greater_equal`.\n",
      " |\n",
      " |  gt(...)\n",
      " |      gt(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.gt`.\n",
      " |\n",
      " |  gt_(...)\n",
      " |      gt_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.gt`.\n",
      " |\n",
      " |  half(...)\n",
      " |      half(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.half()`` is equivalent to ``self.to(torch.float16)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  hardshrink(...)\n",
      " |      hardshrink(lambd=0.5) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.nn.functional.hardshrink`\n",
      " |\n",
      " |  has_names(...)\n",
      " |      Is ``True`` if any of this tensor's dimensions are named. Otherwise, is ``False``.\n",
      " |\n",
      " |  heaviside(...)\n",
      " |      heaviside(values) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.heaviside`\n",
      " |\n",
      " |  heaviside_(...)\n",
      " |      heaviside_(values) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.heaviside`\n",
      " |\n",
      " |  histc(...)\n",
      " |      histc(bins=100, min=0, max=0) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.histc`\n",
      " |\n",
      " |  histogram(...)\n",
      " |      histogram(input, bins, *, range=None, weight=None, density=False) -> (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.histogram`\n",
      " |\n",
      " |  hsplit(...)\n",
      " |      hsplit(split_size_or_sections) -> List of Tensors\n",
      " |\n",
      " |      See :func:`torch.hsplit`\n",
      " |\n",
      " |  hypot(...)\n",
      " |      hypot(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.hypot`\n",
      " |\n",
      " |  hypot_(...)\n",
      " |      hypot_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.hypot`\n",
      " |\n",
      " |  i0(...)\n",
      " |      i0() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.i0`\n",
      " |\n",
      " |  i0_(...)\n",
      " |      i0_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.i0`\n",
      " |\n",
      " |  igamma(...)\n",
      " |      igamma(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.igamma`\n",
      " |\n",
      " |  igamma_(...)\n",
      " |      igamma_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.igamma`\n",
      " |\n",
      " |  igammac(...)\n",
      " |      igammac(other) -> Tensor\n",
      " |      See :func:`torch.igammac`\n",
      " |\n",
      " |  igammac_(...)\n",
      " |      igammac_(other) -> Tensor\n",
      " |      In-place version of :meth:`~Tensor.igammac`\n",
      " |\n",
      " |  index_add(...)\n",
      " |      index_add(dim, index, source, *, alpha=1) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.index_add_`.\n",
      " |\n",
      " |  index_add_(...)\n",
      " |      index_add_(dim, index, source, *, alpha=1) -> Tensor\n",
      " |\n",
      " |      Accumulate the elements of :attr:`alpha` times ``source`` into the :attr:`self`\n",
      " |      tensor by adding to the indices in the order given in :attr:`index`. For example,\n",
      " |      if ``dim == 0``, ``index[i] == j``, and ``alpha=-1``, then the ``i``\\ th row of\n",
      " |      ``source`` is subtracted from the ``j``\\ th row of :attr:`self`.\n",
      " |\n",
      " |      The :attr:`dim`\\ th dimension of ``source`` must have the same size as the\n",
      " |      length of :attr:`index` (which must be a vector), and all other dimensions must\n",
      " |      match :attr:`self`, or an error will be raised.\n",
      " |\n",
      " |      For a 3-D tensor the output is given as::\n",
      " |\n",
      " |          self[index[i], :, :] += alpha * src[i, :, :]  # if dim == 0\n",
      " |          self[:, index[i], :] += alpha * src[:, i, :]  # if dim == 1\n",
      " |          self[:, :, index[i]] += alpha * src[:, :, i]  # if dim == 2\n",
      " |\n",
      " |      Note:\n",
      " |          This operation may behave nondeterministically when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int): dimension along which to index\n",
      " |          index (Tensor): indices of ``source`` to select from,\n",
      " |                  should have dtype either `torch.int64` or `torch.int32`\n",
      " |          source (Tensor): the tensor containing values to add\n",
      " |\n",
      " |      Keyword args:\n",
      " |          alpha (Number): the scalar multiplier for ``source``\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.ones(5, 3)\n",
      " |          >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
      " |          >>> index = torch.tensor([0, 4, 2])\n",
      " |          >>> x.index_add_(0, index, t)\n",
      " |          tensor([[  2.,   3.,   4.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  8.,   9.,  10.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  5.,   6.,   7.]])\n",
      " |          >>> x.index_add_(0, index, t, alpha=-1)\n",
      " |          tensor([[  1.,   1.,   1.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  1.,   1.,   1.]])\n",
      " |\n",
      " |  index_copy(...)\n",
      " |      index_copy(dim, index, tensor2) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.index_copy_`.\n",
      " |\n",
      " |  index_copy_(...)\n",
      " |      index_copy_(dim, index, tensor) -> Tensor\n",
      " |\n",
      " |      Copies the elements of :attr:`tensor` into the :attr:`self` tensor by selecting\n",
      " |      the indices in the order given in :attr:`index`. For example, if ``dim == 0``\n",
      " |      and ``index[i] == j``, then the ``i``\\ th row of :attr:`tensor` is copied to the\n",
      " |      ``j``\\ th row of :attr:`self`.\n",
      " |\n",
      " |      The :attr:`dim`\\ th dimension of :attr:`tensor` must have the same size as the\n",
      " |      length of :attr:`index` (which must be a vector), and all other dimensions must\n",
      " |      match :attr:`self`, or an error will be raised.\n",
      " |\n",
      " |      .. note::\n",
      " |          If :attr:`index` contains duplicate entries, multiple elements from\n",
      " |          :attr:`tensor` will be copied to the same index of :attr:`self`. The result\n",
      " |          is nondeterministic since it depends on which copy occurs last.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int): dimension along which to index\n",
      " |          index (LongTensor): indices of :attr:`tensor` to select from\n",
      " |          tensor (Tensor): the tensor containing values to copy\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.zeros(5, 3)\n",
      " |          >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
      " |          >>> index = torch.tensor([0, 4, 2])\n",
      " |          >>> x.index_copy_(0, index, t)\n",
      " |          tensor([[ 1.,  2.,  3.],\n",
      " |                  [ 0.,  0.,  0.],\n",
      " |                  [ 7.,  8.,  9.],\n",
      " |                  [ 0.,  0.,  0.],\n",
      " |                  [ 4.,  5.,  6.]])\n",
      " |\n",
      " |  index_fill(...)\n",
      " |      index_fill(dim, index, value) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.index_fill_`.\n",
      " |\n",
      " |  index_fill_(...)\n",
      " |      index_fill_(dim, index, value) -> Tensor\n",
      " |\n",
      " |      Fills the elements of the :attr:`self` tensor with value :attr:`value` by\n",
      " |      selecting the indices in the order given in :attr:`index`.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int): dimension along which to index\n",
      " |          index (LongTensor): indices of :attr:`self` tensor to fill in\n",
      " |          value (float): the value to fill with\n",
      " |\n",
      " |      Example::\n",
      " |          >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
      " |          >>> index = torch.tensor([0, 2])\n",
      " |          >>> x.index_fill_(1, index, -1)\n",
      " |          tensor([[-1.,  2., -1.],\n",
      " |                  [-1.,  5., -1.],\n",
      " |                  [-1.,  8., -1.]])\n",
      " |\n",
      " |  index_put(...)\n",
      " |      index_put(indices, values, accumulate=False) -> Tensor\n",
      " |\n",
      " |      Out-place version of :meth:`~Tensor.index_put_`.\n",
      " |\n",
      " |  index_put_(...)\n",
      " |      index_put_(indices, values, accumulate=False) -> Tensor\n",
      " |\n",
      " |      Puts values from the tensor :attr:`values` into the tensor :attr:`self` using\n",
      " |      the indices specified in :attr:`indices` (which is a tuple of Tensors). The\n",
      " |      expression ``tensor.index_put_(indices, values)`` is equivalent to\n",
      " |      ``tensor[indices] = values``. Returns :attr:`self`.\n",
      " |\n",
      " |      If :attr:`accumulate` is ``True``, the elements in :attr:`values` are added to\n",
      " |      :attr:`self`. If accumulate is ``False``, the behavior is undefined if indices\n",
      " |      contain duplicate elements.\n",
      " |\n",
      " |      Args:\n",
      " |          indices (tuple of LongTensor): tensors used to index into `self`.\n",
      " |          values (Tensor): tensor of same dtype as `self`.\n",
      " |          accumulate (bool): whether to accumulate into self\n",
      " |\n",
      " |  index_reduce(...)\n",
      " |\n",
      " |  index_reduce_(...)\n",
      " |      index_reduce_(dim, index, source, reduce, *, include_self=True) -> Tensor\n",
      " |\n",
      " |      Accumulate the elements of ``source`` into the :attr:`self`\n",
      " |      tensor by accumulating to the indices in the order given in :attr:`index`\n",
      " |      using the reduction given by the ``reduce`` argument. For example, if ``dim == 0``,\n",
      " |      ``index[i] == j``, ``reduce == prod`` and ``include_self == True`` then the ``i``\\ th\n",
      " |      row of ``source`` is multiplied by the ``j``\\ th row of :attr:`self`. If\n",
      " |      :obj:`include_self=\"True\"`, the values in the :attr:`self` tensor are included\n",
      " |      in the reduction, otherwise, rows in the :attr:`self` tensor that are accumulated\n",
      " |      to are treated as if they were filled with the reduction identites.\n",
      " |\n",
      " |      The :attr:`dim`\\ th dimension of ``source`` must have the same size as the\n",
      " |      length of :attr:`index` (which must be a vector), and all other dimensions must\n",
      " |      match :attr:`self`, or an error will be raised.\n",
      " |\n",
      " |      For a 3-D tensor with :obj:`reduce=\"prod\"` and :obj:`include_self=True` the\n",
      " |      output is given as::\n",
      " |\n",
      " |          self[index[i], :, :] *= src[i, :, :]  # if dim == 0\n",
      " |          self[:, index[i], :] *= src[:, i, :]  # if dim == 1\n",
      " |          self[:, :, index[i]] *= src[:, :, i]  # if dim == 2\n",
      " |\n",
      " |      Note:\n",
      " |          This operation may behave nondeterministically when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          This function only supports floating point tensors.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          This function is in beta and may change in the near future.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int): dimension along which to index\n",
      " |          index (Tensor): indices of ``source`` to select from,\n",
      " |              should have dtype either `torch.int64` or `torch.int32`\n",
      " |          source (FloatTensor): the tensor containing values to accumulate\n",
      " |          reduce (str): the reduction operation to apply\n",
      " |              (:obj:`\"prod\"`, :obj:`\"mean\"`, :obj:`\"amax\"`, :obj:`\"amin\"`)\n",
      " |\n",
      " |      Keyword args:\n",
      " |          include_self (bool): whether the elements from the ``self`` tensor are\n",
      " |              included in the reduction\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.empty(5, 3).fill_(2)\n",
      " |          >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], dtype=torch.float)\n",
      " |          >>> index = torch.tensor([0, 4, 2, 0])\n",
      " |          >>> x.index_reduce_(0, index, t, 'prod')\n",
      " |          tensor([[20., 44., 72.],\n",
      " |                  [ 2.,  2.,  2.],\n",
      " |                  [14., 16., 18.],\n",
      " |                  [ 2.,  2.,  2.],\n",
      " |                  [ 8., 10., 12.]])\n",
      " |          >>> x = torch.empty(5, 3).fill_(2)\n",
      " |          >>> x.index_reduce_(0, index, t, 'prod', include_self=False)\n",
      " |          tensor([[10., 22., 36.],\n",
      " |                  [ 2.,  2.,  2.],\n",
      " |                  [ 7.,  8.,  9.],\n",
      " |                  [ 2.,  2.,  2.],\n",
      " |                  [ 4.,  5.,  6.]])\n",
      " |\n",
      " |  index_select(...)\n",
      " |      index_select(dim, index) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.index_select`\n",
      " |\n",
      " |  indices(...)\n",
      " |      indices() -> Tensor\n",
      " |\n",
      " |      Return the indices tensor of a :ref:`sparse COO tensor <sparse-coo-docs>`.\n",
      " |\n",
      " |      .. warning::\n",
      " |        Throws an error if :attr:`self` is not a sparse COO tensor.\n",
      " |\n",
      " |      See also :meth:`Tensor.values`.\n",
      " |\n",
      " |      .. note::\n",
      " |        This method can only be called on a coalesced sparse tensor. See\n",
      " |        :meth:`Tensor.coalesce` for details.\n",
      " |\n",
      " |  inner(...)\n",
      " |      inner(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.inner`.\n",
      " |\n",
      " |  int(...)\n",
      " |      int(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.int()`` is equivalent to ``self.to(torch.int32)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  int_repr(...)\n",
      " |      int_repr() -> Tensor\n",
      " |\n",
      " |      Given a quantized Tensor,\n",
      " |      ``self.int_repr()`` returns a CPU Tensor with uint8_t as data type that stores the\n",
      " |      underlying uint8_t values of the given Tensor.\n",
      " |\n",
      " |  inverse(...)\n",
      " |      inverse() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.inverse`\n",
      " |\n",
      " |  ipu(...)\n",
      " |      ipu(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      Returns a copy of this object in IPU memory.\n",
      " |\n",
      " |      If this object is already in IPU memory and on the correct device,\n",
      " |      then no copy is performed and the original object is returned.\n",
      " |\n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The destination IPU device.\n",
      " |              Defaults to the current IPU device.\n",
      " |          non_blocking (bool): If ``True`` and the source is in pinned memory,\n",
      " |              the copy will be asynchronous with respect to the host.\n",
      " |              Otherwise, the argument has no effect. Default: ``False``.\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  is_coalesced(...)\n",
      " |      is_coalesced() -> bool\n",
      " |\n",
      " |      Returns ``True`` if :attr:`self` is a :ref:`sparse COO tensor\n",
      " |      <sparse-coo-docs>` that is coalesced, ``False`` otherwise.\n",
      " |\n",
      " |      .. warning::\n",
      " |        Throws an error if :attr:`self` is not a sparse COO tensor.\n",
      " |\n",
      " |      See :meth:`coalesce` and :ref:`uncoalesced tensors <sparse-uncoalesced-coo-docs>`.\n",
      " |\n",
      " |  is_complex(...)\n",
      " |      is_complex() -> bool\n",
      " |\n",
      " |      Returns True if the data type of :attr:`self` is a complex data type.\n",
      " |\n",
      " |  is_conj(...)\n",
      " |      is_conj() -> bool\n",
      " |\n",
      " |      Returns True if the conjugate bit of :attr:`self` is set to true.\n",
      " |\n",
      " |  is_contiguous(...)\n",
      " |      is_contiguous(memory_format=torch.contiguous_format) -> bool\n",
      " |\n",
      " |      Returns True if :attr:`self` tensor is contiguous in memory in the order specified\n",
      " |      by memory format.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): Specifies memory allocation\n",
      " |              order. Default: ``torch.contiguous_format``.\n",
      " |\n",
      " |  is_distributed(...)\n",
      " |\n",
      " |  is_floating_point(...)\n",
      " |      is_floating_point() -> bool\n",
      " |\n",
      " |      Returns True if the data type of :attr:`self` is a floating point data type.\n",
      " |\n",
      " |  is_inference(...)\n",
      " |      is_inference() -> bool\n",
      " |\n",
      " |      See :func:`torch.is_inference`\n",
      " |\n",
      " |  is_neg(...)\n",
      " |      is_neg() -> bool\n",
      " |\n",
      " |      Returns True if the negative bit of :attr:`self` is set to true.\n",
      " |\n",
      " |  is_nonzero(...)\n",
      " |\n",
      " |  is_pinned(...)\n",
      " |      Returns true if this tensor resides in pinned memory.\n",
      " |      By default, the device pinned memory on will be the current :ref:`accelerator<accelerators>`.\n",
      " |\n",
      " |  is_same_size(...)\n",
      " |\n",
      " |  is_set_to(...)\n",
      " |      is_set_to(tensor) -> bool\n",
      " |\n",
      " |      Returns True if both tensors are pointing to the exact same memory (same\n",
      " |      storage, offset, size and stride).\n",
      " |\n",
      " |  is_signed(...)\n",
      " |      is_signed() -> bool\n",
      " |\n",
      " |      Returns True if the data type of :attr:`self` is a signed data type.\n",
      " |\n",
      " |  isclose(...)\n",
      " |      isclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.isclose`\n",
      " |\n",
      " |  isfinite(...)\n",
      " |      isfinite() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.isfinite`\n",
      " |\n",
      " |  isinf(...)\n",
      " |      isinf() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.isinf`\n",
      " |\n",
      " |  isnan(...)\n",
      " |      isnan() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.isnan`\n",
      " |\n",
      " |  isneginf(...)\n",
      " |      isneginf() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.isneginf`\n",
      " |\n",
      " |  isposinf(...)\n",
      " |      isposinf() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.isposinf`\n",
      " |\n",
      " |  isreal(...)\n",
      " |      isreal() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.isreal`\n",
      " |\n",
      " |  item(...)\n",
      " |      item() -> number\n",
      " |\n",
      " |      Returns the value of this tensor as a standard Python number. This only works\n",
      " |      for tensors with one element. For other cases, see :meth:`~Tensor.tolist`.\n",
      " |\n",
      " |      This operation is not differentiable.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.tensor([1.0])\n",
      " |          >>> x.item()\n",
      " |          1.0\n",
      " |\n",
      " |  kron(...)\n",
      " |      kron(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.kron`\n",
      " |\n",
      " |  kthvalue(...)\n",
      " |      kthvalue(k, dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
      " |\n",
      " |      See :func:`torch.kthvalue`\n",
      " |\n",
      " |  lcm(...)\n",
      " |      lcm(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.lcm`\n",
      " |\n",
      " |  lcm_(...)\n",
      " |      lcm_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.lcm`\n",
      " |\n",
      " |  ldexp(...)\n",
      " |      ldexp(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.ldexp`\n",
      " |\n",
      " |  ldexp_(...)\n",
      " |      ldexp_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.ldexp`\n",
      " |\n",
      " |  le(...)\n",
      " |      le(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.le`.\n",
      " |\n",
      " |  le_(...)\n",
      " |      le_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.le`.\n",
      " |\n",
      " |  lerp(...)\n",
      " |      lerp(end, weight) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.lerp`\n",
      " |\n",
      " |  lerp_(...)\n",
      " |      lerp_(end, weight) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.lerp`\n",
      " |\n",
      " |  less(...)\n",
      " |      lt(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.less`.\n",
      " |\n",
      " |  less_(...)\n",
      " |      less_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.less`.\n",
      " |\n",
      " |  less_equal(...)\n",
      " |      less_equal(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.less_equal`.\n",
      " |\n",
      " |  less_equal_(...)\n",
      " |      less_equal_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.less_equal`.\n",
      " |\n",
      " |  lgamma(...)\n",
      " |      lgamma() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.lgamma`\n",
      " |\n",
      " |  lgamma_(...)\n",
      " |      lgamma_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.lgamma`\n",
      " |\n",
      " |  log(...)\n",
      " |      log() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.log`\n",
      " |\n",
      " |  log10(...)\n",
      " |      log10() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.log10`\n",
      " |\n",
      " |  log10_(...)\n",
      " |      log10_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.log10`\n",
      " |\n",
      " |  log1p(...)\n",
      " |      log1p() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.log1p`\n",
      " |\n",
      " |  log1p_(...)\n",
      " |      log1p_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.log1p`\n",
      " |\n",
      " |  log2(...)\n",
      " |      log2() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.log2`\n",
      " |\n",
      " |  log2_(...)\n",
      " |      log2_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.log2`\n",
      " |\n",
      " |  log_(...)\n",
      " |      log_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.log`\n",
      " |\n",
      " |  log_normal_(...)\n",
      " |      log_normal_(mean=1, std=2, *, generator=None)\n",
      " |\n",
      " |      Fills :attr:`self` tensor with numbers samples from the log-normal distribution\n",
      " |      parameterized by the given mean :math:`\\mu` and standard deviation\n",
      " |      :math:`\\sigma`. Note that :attr:`mean` and :attr:`std` are the mean and\n",
      " |      standard deviation of the underlying normal distribution, and not of the\n",
      " |      returned distribution:\n",
      " |\n",
      " |      .. math::\n",
      " |\n",
      " |          f(x) = \\dfrac{1}{x \\sigma \\sqrt{2\\pi}}\\ e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}}\n",
      " |\n",
      " |  log_softmax(...)\n",
      " |\n",
      " |  logaddexp(...)\n",
      " |      logaddexp(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logaddexp`\n",
      " |\n",
      " |  logaddexp2(...)\n",
      " |      logaddexp2(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logaddexp2`\n",
      " |\n",
      " |  logcumsumexp(...)\n",
      " |      logcumsumexp(dim) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logcumsumexp`\n",
      " |\n",
      " |  logdet(...)\n",
      " |      logdet() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logdet`\n",
      " |\n",
      " |  logical_and(...)\n",
      " |      logical_and() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logical_and`\n",
      " |\n",
      " |  logical_and_(...)\n",
      " |      logical_and_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.logical_and`\n",
      " |\n",
      " |  logical_not(...)\n",
      " |      logical_not() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logical_not`\n",
      " |\n",
      " |  logical_not_(...)\n",
      " |      logical_not_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.logical_not`\n",
      " |\n",
      " |  logical_or(...)\n",
      " |      logical_or() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logical_or`\n",
      " |\n",
      " |  logical_or_(...)\n",
      " |      logical_or_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.logical_or`\n",
      " |\n",
      " |  logical_xor(...)\n",
      " |      logical_xor() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logical_xor`\n",
      " |\n",
      " |  logical_xor_(...)\n",
      " |      logical_xor_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.logical_xor`\n",
      " |\n",
      " |  logit(...)\n",
      " |      logit() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logit`\n",
      " |\n",
      " |  logit_(...)\n",
      " |      logit_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.logit`\n",
      " |\n",
      " |  logsumexp(...)\n",
      " |      logsumexp(dim, keepdim=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.logsumexp`\n",
      " |\n",
      " |  long(...)\n",
      " |      long(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.long()`` is equivalent to ``self.to(torch.int64)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  lt(...)\n",
      " |      lt(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.lt`.\n",
      " |\n",
      " |  lt_(...)\n",
      " |      lt_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.lt`.\n",
      " |\n",
      " |  lu_solve(...)\n",
      " |      lu_solve(LU_data, LU_pivots) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.lu_solve`\n",
      " |\n",
      " |  map2_(...)\n",
      " |\n",
      " |  map_(...)\n",
      " |      map_(tensor, callable)\n",
      " |\n",
      " |      Applies :attr:`callable` for each element in :attr:`self` tensor and the given\n",
      " |      :attr:`tensor` and stores the results in :attr:`self` tensor. :attr:`self` tensor and\n",
      " |      the given :attr:`tensor` must be :ref:`broadcastable <broadcasting-semantics>`.\n",
      " |\n",
      " |      The :attr:`callable` should have the signature::\n",
      " |\n",
      " |          def callable(a, b) -> number\n",
      " |\n",
      " |  masked_fill(...)\n",
      " |      masked_fill(mask, value) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.masked_fill_`\n",
      " |\n",
      " |  masked_fill_(...)\n",
      " |      masked_fill_(mask, value)\n",
      " |\n",
      " |      Fills elements of :attr:`self` tensor with :attr:`value` where :attr:`mask` is\n",
      " |      True. The shape of :attr:`mask` must be\n",
      " |      :ref:`broadcastable <broadcasting-semantics>` with the shape of the underlying\n",
      " |      tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          mask (BoolTensor): the boolean mask\n",
      " |          value (float): the value to fill in with\n",
      " |\n",
      " |  masked_scatter(...)\n",
      " |      masked_scatter(mask, tensor) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.masked_scatter_`\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          The inputs :attr:`self` and :attr:`mask`\n",
      " |          :ref:`broadcast <broadcasting-semantics>`.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          >>> self = torch.tensor([0, 0, 0, 0, 0])\n",
      " |          >>> mask = torch.tensor([[0, 0, 0, 1, 1], [1, 1, 0, 1, 1]], dtype=torch.bool)\n",
      " |          >>> source = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n",
      " |          >>> self.masked_scatter(mask, source)\n",
      " |          tensor([[0, 0, 0, 0, 1],\n",
      " |                  [2, 3, 0, 4, 5]])\n",
      " |\n",
      " |  masked_scatter_(...)\n",
      " |      masked_scatter_(mask, source)\n",
      " |\n",
      " |      Copies elements from :attr:`source` into :attr:`self` tensor at positions where\n",
      " |      the :attr:`mask` is True. Elements from :attr:`source` are copied into :attr:`self`\n",
      " |      starting at position 0 of :attr:`source` and continuing in order one-by-one for each\n",
      " |      occurrence of :attr:`mask` being True.\n",
      " |      The shape of :attr:`mask` must be :ref:`broadcastable <broadcasting-semantics>`\n",
      " |      with the shape of the underlying tensor. The :attr:`source` should have at least\n",
      " |      as many elements as the number of ones in :attr:`mask`.\n",
      " |\n",
      " |      Args:\n",
      " |          mask (BoolTensor): the boolean mask\n",
      " |          source (Tensor): the tensor to copy from\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          The :attr:`mask` operates on the :attr:`self` tensor, not on the given\n",
      " |          :attr:`source` tensor.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          >>> self = torch.tensor([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]])\n",
      " |          >>> mask = torch.tensor([[0, 0, 0, 1, 1], [1, 1, 0, 1, 1]], dtype=torch.bool)\n",
      " |          >>> source = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n",
      " |          >>> self.masked_scatter_(mask, source)\n",
      " |          tensor([[0, 0, 0, 0, 1],\n",
      " |                  [2, 3, 0, 4, 5]])\n",
      " |\n",
      " |  masked_select(...)\n",
      " |      masked_select(mask) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.masked_select`\n",
      " |\n",
      " |  matmul(...)\n",
      " |      matmul(tensor2) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.matmul`\n",
      " |\n",
      " |  matrix_exp(...)\n",
      " |      matrix_exp() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.matrix_exp`\n",
      " |\n",
      " |  matrix_power(...)\n",
      " |      matrix_power(n) -> Tensor\n",
      " |\n",
      " |      .. note:: :meth:`~Tensor.matrix_power` is deprecated, use :func:`torch.linalg.matrix_power` instead.\n",
      " |\n",
      " |      Alias for :func:`torch.linalg.matrix_power`\n",
      " |\n",
      " |  max(...)\n",
      " |      max(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.max`\n",
      " |\n",
      " |  maximum(...)\n",
      " |      maximum(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.maximum`\n",
      " |\n",
      " |  mean(...)\n",
      " |      mean(dim=None, keepdim=False, *, dtype=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.mean`\n",
      " |\n",
      " |  median(...)\n",
      " |      median(dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
      " |\n",
      " |      See :func:`torch.median`\n",
      " |\n",
      " |  min(...)\n",
      " |      min(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.min`\n",
      " |\n",
      " |  minimum(...)\n",
      " |      minimum(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.minimum`\n",
      " |\n",
      " |  mm(...)\n",
      " |      mm(mat2) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.mm`\n",
      " |\n",
      " |  mode(...)\n",
      " |      mode(dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
      " |\n",
      " |      See :func:`torch.mode`\n",
      " |\n",
      " |  moveaxis(...)\n",
      " |      moveaxis(source, destination) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.moveaxis`\n",
      " |\n",
      " |  movedim(...)\n",
      " |      movedim(source, destination) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.movedim`\n",
      " |\n",
      " |  msort(...)\n",
      " |      msort() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.msort`\n",
      " |\n",
      " |  mtia(...)\n",
      " |      mtia(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      Returns a copy of this object in MTIA memory.\n",
      " |\n",
      " |      If this object is already in MTIA memory and on the correct device,\n",
      " |      then no copy is performed and the original object is returned.\n",
      " |\n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The destination MTIA device.\n",
      " |              Defaults to the current MTIA device.\n",
      " |          non_blocking (bool): If ``True`` and the source is in pinned memory,\n",
      " |              the copy will be asynchronous with respect to the host.\n",
      " |              Otherwise, the argument has no effect. Default: ``False``.\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  mul(...)\n",
      " |      mul(value) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.mul`.\n",
      " |\n",
      " |  mul_(...)\n",
      " |      mul_(value) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.mul`.\n",
      " |\n",
      " |  multinomial(...)\n",
      " |      multinomial(num_samples, replacement=False, *, generator=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.multinomial`\n",
      " |\n",
      " |  multiply(...)\n",
      " |      multiply(value) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.multiply`.\n",
      " |\n",
      " |  multiply_(...)\n",
      " |      multiply_(value) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.multiply`.\n",
      " |\n",
      " |  mv(...)\n",
      " |      mv(vec) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.mv`\n",
      " |\n",
      " |  mvlgamma(...)\n",
      " |      mvlgamma(p) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.mvlgamma`\n",
      " |\n",
      " |  mvlgamma_(...)\n",
      " |      mvlgamma_(p) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.mvlgamma`\n",
      " |\n",
      " |  nan_to_num(...)\n",
      " |      nan_to_num(nan=0.0, posinf=None, neginf=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.nan_to_num`.\n",
      " |\n",
      " |  nan_to_num_(...)\n",
      " |      nan_to_num_(nan=0.0, posinf=None, neginf=None) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.nan_to_num`.\n",
      " |\n",
      " |  nanmean(...)\n",
      " |      nanmean(dim=None, keepdim=False, *, dtype=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.nanmean`\n",
      " |\n",
      " |  nanmedian(...)\n",
      " |      nanmedian(dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
      " |\n",
      " |      See :func:`torch.nanmedian`\n",
      " |\n",
      " |  nanquantile(...)\n",
      " |      nanquantile(q, dim=None, keepdim=False, *, interpolation='linear') -> Tensor\n",
      " |\n",
      " |      See :func:`torch.nanquantile`\n",
      " |\n",
      " |  nansum(...)\n",
      " |      nansum(dim=None, keepdim=False, dtype=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.nansum`\n",
      " |\n",
      " |  narrow(...)\n",
      " |      narrow(dimension, start, length) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.narrow`.\n",
      " |\n",
      " |  narrow_copy(...)\n",
      " |      narrow_copy(dimension, start, length) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.narrow_copy`.\n",
      " |\n",
      " |  ndimension(...)\n",
      " |      ndimension() -> int\n",
      " |\n",
      " |      Alias for :meth:`~Tensor.dim()`\n",
      " |\n",
      " |  ne(...)\n",
      " |      ne(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.ne`.\n",
      " |\n",
      " |  ne_(...)\n",
      " |      ne_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.ne`.\n",
      " |\n",
      " |  neg(...)\n",
      " |      neg() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.neg`\n",
      " |\n",
      " |  neg_(...)\n",
      " |      neg_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.neg`\n",
      " |\n",
      " |  negative(...)\n",
      " |      negative() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.negative`\n",
      " |\n",
      " |  negative_(...)\n",
      " |      negative_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.negative`\n",
      " |\n",
      " |  nelement(...)\n",
      " |      nelement() -> int\n",
      " |\n",
      " |      Alias for :meth:`~Tensor.numel`\n",
      " |\n",
      " |  new(...)\n",
      " |\n",
      " |  new_empty(...)\n",
      " |      new_empty(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |\n",
      " |\n",
      " |      Returns a Tensor of size :attr:`size` filled with uninitialized data.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
      " |              shape of the output tensor.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> tensor = torch.ones(())\n",
      " |          >>> tensor.new_empty((2, 3))\n",
      " |          tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],\n",
      " |                  [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])\n",
      " |\n",
      " |  new_empty_strided(...)\n",
      " |      new_empty_strided(size, stride, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |\n",
      " |\n",
      " |      Returns a Tensor of size :attr:`size` and strides :attr:`stride` filled with\n",
      " |      uninitialized data. By default, the returned Tensor has the same\n",
      " |      :class:`torch.dtype` and :class:`torch.device` as this tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
      " |              shape of the output tensor.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> tensor = torch.ones(())\n",
      " |          >>> tensor.new_empty_strided((2, 3), (3, 1))\n",
      " |          tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],\n",
      " |                  [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])\n",
      " |\n",
      " |  new_full(...)\n",
      " |      new_full(size, fill_value, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |\n",
      " |\n",
      " |      Returns a Tensor of size :attr:`size` filled with :attr:`fill_value`.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          fill_value (scalar): the number to fill the output tensor with.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> tensor = torch.ones((2,), dtype=torch.float64)\n",
      " |          >>> tensor.new_full((3, 4), 3.141592)\n",
      " |          tensor([[ 3.1416,  3.1416,  3.1416,  3.1416],\n",
      " |                  [ 3.1416,  3.1416,  3.1416,  3.1416],\n",
      " |                  [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)\n",
      " |\n",
      " |  new_ones(...)\n",
      " |      new_ones(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |\n",
      " |\n",
      " |      Returns a Tensor of size :attr:`size` filled with ``1``.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
      " |              shape of the output tensor.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> tensor = torch.tensor((), dtype=torch.int32)\n",
      " |          >>> tensor.new_ones((2, 3))\n",
      " |          tensor([[ 1,  1,  1],\n",
      " |                  [ 1,  1,  1]], dtype=torch.int32)\n",
      " |\n",
      " |  new_tensor(...)\n",
      " |      new_tensor(data, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |\n",
      " |\n",
      " |      Returns a new Tensor with :attr:`data` as the tensor data.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          :func:`new_tensor` always copies :attr:`data`. If you have a Tensor\n",
      " |          ``data`` and want to avoid a copy, use :func:`torch.Tensor.requires_grad_`\n",
      " |          or :func:`torch.Tensor.detach`.\n",
      " |          If you have a numpy array and want to avoid a copy, use\n",
      " |          :func:`torch.from_numpy`.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          When data is a tensor `x`, :func:`new_tensor()` reads out 'the data' from whatever it is passed,\n",
      " |          and constructs a leaf variable. Therefore ``tensor.new_tensor(x)`` is equivalent to ``x.detach().clone()``\n",
      " |          and ``tensor.new_tensor(x, requires_grad=True)`` is equivalent to ``x.detach().clone().requires_grad_(True)``.\n",
      " |          The equivalents using ``detach()`` and ``clone()`` are recommended.\n",
      " |\n",
      " |      Args:\n",
      " |          data (array_like): The returned Tensor copies :attr:`data`.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> tensor = torch.ones((2,), dtype=torch.int8)\n",
      " |          >>> data = [[0, 1], [2, 3]]\n",
      " |          >>> tensor.new_tensor(data)\n",
      " |          tensor([[ 0,  1],\n",
      " |                  [ 2,  3]], dtype=torch.int8)\n",
      " |\n",
      " |  new_zeros(...)\n",
      " |      new_zeros(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |\n",
      " |\n",
      " |      Returns a Tensor of size :attr:`size` filled with ``0``.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
      " |              shape of the output tensor.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> tensor = torch.tensor((), dtype=torch.float64)\n",
      " |          >>> tensor.new_zeros((2, 3))\n",
      " |          tensor([[ 0.,  0.,  0.],\n",
      " |                  [ 0.,  0.,  0.]], dtype=torch.float64)\n",
      " |\n",
      " |  nextafter(...)\n",
      " |      nextafter(other) -> Tensor\n",
      " |      See :func:`torch.nextafter`\n",
      " |\n",
      " |  nextafter_(...)\n",
      " |      nextafter_(other) -> Tensor\n",
      " |      In-place version of :meth:`~Tensor.nextafter`\n",
      " |\n",
      " |  nonzero(...)\n",
      " |      nonzero() -> LongTensor\n",
      " |\n",
      " |      See :func:`torch.nonzero`\n",
      " |\n",
      " |  nonzero_static(...)\n",
      " |      nonzero_static(input, *, size, fill_value=-1) -> Tensor\n",
      " |\n",
      " |      Returns a 2-D tensor where each row is the index for a non-zero value.\n",
      " |      The returned Tensor has the same `torch.dtype` as `torch.nonzero()`.\n",
      " |\n",
      " |      Args:\n",
      " |          input (Tensor): the input tensor to count non-zero elements.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          size (int): the size of non-zero elements expected to be included in the out\n",
      " |              tensor. Pad the out tensor with `fill_value` if the `size` is larger\n",
      " |              than total number of non-zero elements, truncate out tensor if `size`\n",
      " |              is smaller. The size must be a non-negative integer.\n",
      " |          fill_value (int): the value to fill the output tensor with when `size` is larger\n",
      " |              than the total number of non-zero elements. Default is `-1` to represent\n",
      " |              invalid index.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          # Example 1: Padding\n",
      " |          >>> input_tensor = torch.tensor([[1, 0], [3, 2]])\n",
      " |          >>> static_size = 4\n",
      " |          >>> t = torch.nonzero_static(input_tensor, size = static_size)\n",
      " |          tensor([[  0,   0],\n",
      " |                  [  1,   0],\n",
      " |                  [  1,   1],\n",
      " |                  [  -1, -1]], dtype=torch.int64)\n",
      " |\n",
      " |          # Example 2: Truncating\n",
      " |          >>> input_tensor = torch.tensor([[1, 0], [3, 2]])\n",
      " |          >>> static_size = 2\n",
      " |          >>> t = torch.nonzero_static(input_tensor, size = static_size)\n",
      " |          tensor([[  0,   0],\n",
      " |                  [  1,   0]], dtype=torch.int64)\n",
      " |\n",
      " |          # Example 3: 0 size\n",
      " |          >>> input_tensor = torch.tensor([10])\n",
      " |          >>> static_size = 0\n",
      " |          >>> t = torch.nonzero_static(input_tensor, size = static_size)\n",
      " |          tensor([], size=(0, 1), dtype=torch.int64)\n",
      " |\n",
      " |          # Example 4: 0 rank input\n",
      " |          >>> input_tensor = torch.tensor(10)\n",
      " |          >>> static_size = 2\n",
      " |          >>> t = torch.nonzero_static(input_tensor, size = static_size)\n",
      " |          tensor([], size=(2, 0), dtype=torch.int64)\n",
      " |\n",
      " |  normal_(...)\n",
      " |      normal_(mean=0, std=1, *, generator=None) -> Tensor\n",
      " |\n",
      " |      Fills :attr:`self` tensor with elements samples from the normal distribution\n",
      " |      parameterized by :attr:`mean` and :attr:`std`.\n",
      " |\n",
      " |  not_equal(...)\n",
      " |      not_equal(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.not_equal`.\n",
      " |\n",
      " |  not_equal_(...)\n",
      " |      not_equal_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.not_equal`.\n",
      " |\n",
      " |  numel(...)\n",
      " |      numel() -> int\n",
      " |\n",
      " |      See :func:`torch.numel`\n",
      " |\n",
      " |  numpy(...)\n",
      " |      numpy(*, force=False) -> numpy.ndarray\n",
      " |\n",
      " |      Returns the tensor as a NumPy :class:`ndarray`.\n",
      " |\n",
      " |      If :attr:`force` is ``False`` (the default), the conversion\n",
      " |      is performed only if the tensor is on the CPU, does not require grad,\n",
      " |      does not have its conjugate bit set, and is a dtype and layout that\n",
      " |      NumPy supports. The returned ndarray and the tensor will share their\n",
      " |      storage, so changes to the tensor will be reflected in the ndarray\n",
      " |      and vice versa.\n",
      " |\n",
      " |      If :attr:`force` is ``True`` this is equivalent to\n",
      " |      calling ``t.detach().cpu().resolve_conj().resolve_neg().numpy()``.\n",
      " |      If the tensor isn't on the CPU or the conjugate or negative bit is set,\n",
      " |      the tensor won't share its storage with the returned ndarray.\n",
      " |      Setting :attr:`force` to ``True`` can be a useful shorthand.\n",
      " |\n",
      " |      Args:\n",
      " |          force (bool): if ``True``, the ndarray may be a copy of the tensor\n",
      " |                     instead of always sharing memory, defaults to ``False``.\n",
      " |\n",
      " |  orgqr(...)\n",
      " |      orgqr(input2) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.orgqr`\n",
      " |\n",
      " |  ormqr(...)\n",
      " |      ormqr(input2, input3, left=True, transpose=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.ormqr`\n",
      " |\n",
      " |  outer(...)\n",
      " |      outer(vec2) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.outer`.\n",
      " |\n",
      " |  permute(...)\n",
      " |      permute(*dims) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.permute`\n",
      " |\n",
      " |  pin_memory(...)\n",
      " |      pin_memory() -> Tensor\n",
      " |\n",
      " |      Copies the tensor to pinned memory, if it's not already pinned.\n",
      " |      By default, the device pinned memory on will be the current :ref:`accelerator<accelerators>`.\n",
      " |\n",
      " |  pinverse(...)\n",
      " |      pinverse() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.pinverse`\n",
      " |\n",
      " |  polygamma(...)\n",
      " |      polygamma(n) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.polygamma`\n",
      " |\n",
      " |  polygamma_(...)\n",
      " |      polygamma_(n) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.polygamma`\n",
      " |\n",
      " |  positive(...)\n",
      " |      positive() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.positive`\n",
      " |\n",
      " |  pow(...)\n",
      " |      pow(exponent) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.pow`\n",
      " |\n",
      " |  pow_(...)\n",
      " |      pow_(exponent) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.pow`\n",
      " |\n",
      " |  prelu(...)\n",
      " |\n",
      " |  prod(...)\n",
      " |      prod(dim=None, keepdim=False, dtype=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.prod`\n",
      " |\n",
      " |  put(...)\n",
      " |      put(input, index, source, accumulate=False) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.put_`.\n",
      " |      `input` corresponds to `self` in :meth:`torch.Tensor.put_`.\n",
      " |\n",
      " |  put_(...)\n",
      " |      put_(index, source, accumulate=False) -> Tensor\n",
      " |\n",
      " |      Copies the elements from :attr:`source` into the positions specified by\n",
      " |      :attr:`index`. For the purpose of indexing, the :attr:`self` tensor is treated as if\n",
      " |      it were a 1-D tensor.\n",
      " |\n",
      " |      :attr:`index` and :attr:`source` need to have the same number of elements, but not necessarily\n",
      " |      the same shape.\n",
      " |\n",
      " |      If :attr:`accumulate` is ``True``, the elements in :attr:`source` are added to\n",
      " |      :attr:`self`. If accumulate is ``False``, the behavior is undefined if :attr:`index`\n",
      " |      contain duplicate elements.\n",
      " |\n",
      " |      Args:\n",
      " |          index (LongTensor): the indices into self\n",
      " |          source (Tensor): the tensor containing values to copy from\n",
      " |          accumulate (bool): whether to accumulate into self\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> src = torch.tensor([[4, 3, 5],\n",
      " |          ...                     [6, 7, 8]])\n",
      " |          >>> src.put_(torch.tensor([1, 3]), torch.tensor([9, 10]))\n",
      " |          tensor([[  4,   9,   5],\n",
      " |                  [ 10,   7,   8]])\n",
      " |\n",
      " |  q_per_channel_axis(...)\n",
      " |      q_per_channel_axis() -> int\n",
      " |\n",
      " |      Given a Tensor quantized by linear (affine) per-channel quantization,\n",
      " |      returns the index of dimension on which per-channel quantization is applied.\n",
      " |\n",
      " |  q_per_channel_scales(...)\n",
      " |      q_per_channel_scales() -> Tensor\n",
      " |\n",
      " |      Given a Tensor quantized by linear (affine) per-channel quantization,\n",
      " |      returns a Tensor of scales of the underlying quantizer. It has the number of\n",
      " |      elements that matches the corresponding dimensions (from q_per_channel_axis) of\n",
      " |      the tensor.\n",
      " |\n",
      " |  q_per_channel_zero_points(...)\n",
      " |      q_per_channel_zero_points() -> Tensor\n",
      " |\n",
      " |      Given a Tensor quantized by linear (affine) per-channel quantization,\n",
      " |      returns a tensor of zero_points of the underlying quantizer. It has the number of\n",
      " |      elements that matches the corresponding dimensions (from q_per_channel_axis) of\n",
      " |      the tensor.\n",
      " |\n",
      " |  q_scale(...)\n",
      " |      q_scale() -> float\n",
      " |\n",
      " |      Given a Tensor quantized by linear(affine) quantization,\n",
      " |      returns the scale of the underlying quantizer().\n",
      " |\n",
      " |  q_zero_point(...)\n",
      " |      q_zero_point() -> int\n",
      " |\n",
      " |      Given a Tensor quantized by linear(affine) quantization,\n",
      " |      returns the zero_point of the underlying quantizer().\n",
      " |\n",
      " |  qr(...)\n",
      " |      qr(some=True) -> (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.qr`\n",
      " |\n",
      " |  qscheme(...)\n",
      " |      qscheme() -> torch.qscheme\n",
      " |\n",
      " |      Returns the quantization scheme of a given QTensor.\n",
      " |\n",
      " |  quantile(...)\n",
      " |      quantile(q, dim=None, keepdim=False, *, interpolation='linear') -> Tensor\n",
      " |\n",
      " |      See :func:`torch.quantile`\n",
      " |\n",
      " |  rad2deg(...)\n",
      " |      rad2deg() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.rad2deg`\n",
      " |\n",
      " |  rad2deg_(...)\n",
      " |      rad2deg_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.rad2deg`\n",
      " |\n",
      " |  random_(...)\n",
      " |      random_(from=0, to=None, *, generator=None) -> Tensor\n",
      " |\n",
      " |      Fills :attr:`self` tensor with numbers sampled from the discrete uniform\n",
      " |      distribution over ``[from, to - 1]``. If not specified, the values are usually\n",
      " |      only bounded by :attr:`self` tensor's data type. However, for floating point\n",
      " |      types, if unspecified, range will be ``[0, 2^mantissa]`` to ensure that every\n",
      " |      value is representable. For example, `torch.tensor(1, dtype=torch.double).random_()`\n",
      " |      will be uniform in ``[0, 2^53]``.\n",
      " |\n",
      " |  ravel(...)\n",
      " |      ravel() -> Tensor\n",
      " |\n",
      " |      see :func:`torch.ravel`\n",
      " |\n",
      " |  reciprocal(...)\n",
      " |      reciprocal() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.reciprocal`\n",
      " |\n",
      " |  reciprocal_(...)\n",
      " |      reciprocal_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.reciprocal`\n",
      " |\n",
      " |  record_stream(...)\n",
      " |      record_stream(stream)\n",
      " |\n",
      " |      Marks the tensor as having been used by this stream.  When the tensor\n",
      " |      is deallocated, ensure the tensor memory is not reused for another tensor\n",
      " |      until all work queued on :attr:`stream` at the time of deallocation is\n",
      " |      complete.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          The caching allocator is aware of only the stream where a tensor was\n",
      " |          allocated. Due to the awareness, it already correctly manages the life\n",
      " |          cycle of tensors on only one stream. But if a tensor is used on a stream\n",
      " |          different from the stream of origin, the allocator might reuse the memory\n",
      " |          unexpectedly. Calling this method lets the allocator know which streams\n",
      " |          have used the tensor.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          This method is most suitable for use cases where you are providing a\n",
      " |          function that created a tensor on a side stream, and want users to be able\n",
      " |          to make use of the tensor without having to think carefully about stream\n",
      " |          safety when making use of them.  These safety guarantees come at some\n",
      " |          performance and predictability cost (analogous to the tradeoff between GC\n",
      " |          and manual memory management), so if you are in a situation where\n",
      " |          you manage the full lifetime of your tensors, you may consider instead\n",
      " |          manually managing CUDA events so that calling this method is not necessary.\n",
      " |          In particular, when you call this method, on later allocations the\n",
      " |          allocator will poll the recorded stream to see if all operations have\n",
      " |          completed yet; you can potentially race with side stream computation and\n",
      " |          non-deterministically reuse or fail to reuse memory for an allocation.\n",
      " |\n",
      " |          You can safely use tensors allocated on side streams without\n",
      " |          :meth:`~Tensor.record_stream`; you must manually ensure that\n",
      " |          any non-creation stream uses of a tensor are synced back to the creation\n",
      " |          stream before you deallocate the tensor.  As the CUDA caching allocator\n",
      " |          guarantees that the memory will only be reused with the same creation stream,\n",
      " |          this is sufficient to ensure that writes to future reallocations of the\n",
      " |          memory will be delayed until non-creation stream uses are done.\n",
      " |          (Counterintuitively, you may observe that on the CPU side we have already\n",
      " |          reallocated the tensor, even though CUDA kernels on the old tensor are\n",
      " |          still in progress.  This is fine, because CUDA operations on the new\n",
      " |          tensor will appropriately wait for the old operations to complete, as they\n",
      " |          are all on the same stream.)\n",
      " |\n",
      " |          Concretely, this looks like this::\n",
      " |\n",
      " |              with torch.cuda.stream(s0):\n",
      " |                  x = torch.zeros(N)\n",
      " |\n",
      " |              s1.wait_stream(s0)\n",
      " |              with torch.cuda.stream(s1):\n",
      " |                  y = some_comm_op(x)\n",
      " |\n",
      " |              ... some compute on s0 ...\n",
      " |\n",
      " |              # synchronize creation stream s0 to side stream s1\n",
      " |              # before deallocating x\n",
      " |              s0.wait_stream(s1)\n",
      " |              del x\n",
      " |\n",
      " |          Note that some discretion is required when deciding when to perform\n",
      " |          ``s0.wait_stream(s1)``.  In particular, if we were to wait immediately\n",
      " |          after ``some_comm_op``, there wouldn't be any point in having the side\n",
      " |          stream; it would be equivalent to have run ``some_comm_op`` on ``s0``.\n",
      " |          Instead, the synchronization must be placed at some appropriate, later\n",
      " |          point in time where you expect the side stream ``s1`` to have finished\n",
      " |          work.  This location is typically identified via profiling, e.g., using\n",
      " |          Chrome traces produced\n",
      " |          :meth:`torch.autograd.profiler.profile.export_chrome_trace`.  If you\n",
      " |          place the wait too early, work on s0 will block until ``s1`` has finished,\n",
      " |          preventing further overlapping of communication and computation.  If you\n",
      " |          place the wait too late, you will use more memory than is strictly\n",
      " |          necessary (as you are keeping ``x`` live for longer.)  For a concrete\n",
      " |          example of how this guidance can be applied in practice, see this post:\n",
      " |          `FSDP and CUDACachingAllocator\n",
      " |          <https://dev-discuss.pytorch.org/t/fsdp-cudacachingallocator-an-outsider-newb-perspective/1486>`_.\n",
      " |\n",
      " |  relu(...)\n",
      " |\n",
      " |  relu_(...)\n",
      " |\n",
      " |  remainder(...)\n",
      " |      remainder(divisor) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.remainder`\n",
      " |\n",
      " |  remainder_(...)\n",
      " |      remainder_(divisor) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.remainder`\n",
      " |\n",
      " |  renorm(...)\n",
      " |      renorm(p, dim, maxnorm) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.renorm`\n",
      " |\n",
      " |  renorm_(...)\n",
      " |      renorm_(p, dim, maxnorm) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.renorm`\n",
      " |\n",
      " |  repeat(...)\n",
      " |      repeat(*repeats) -> Tensor\n",
      " |\n",
      " |      Repeats this tensor along the specified dimensions.\n",
      " |\n",
      " |      Unlike :meth:`~Tensor.expand`, this function copies the tensor's data.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          :meth:`~Tensor.repeat` behaves differently from\n",
      " |          `numpy.repeat <https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html>`_,\n",
      " |          but is more similar to\n",
      " |          `numpy.tile <https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html>`_.\n",
      " |          For the operator similar to `numpy.repeat`, see :func:`torch.repeat_interleave`.\n",
      " |\n",
      " |      Args:\n",
      " |          repeat (torch.Size, int..., tuple of int or list of int): The number of times to repeat this tensor along each dimension\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.tensor([1, 2, 3])\n",
      " |          >>> x.repeat(4, 2)\n",
      " |          tensor([[ 1,  2,  3,  1,  2,  3],\n",
      " |                  [ 1,  2,  3,  1,  2,  3],\n",
      " |                  [ 1,  2,  3,  1,  2,  3],\n",
      " |                  [ 1,  2,  3,  1,  2,  3]])\n",
      " |          >>> x.repeat(4, 2, 1).size()\n",
      " |          torch.Size([4, 2, 3])\n",
      " |\n",
      " |  repeat_interleave(...)\n",
      " |      repeat_interleave(repeats, dim=None, *, output_size=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.repeat_interleave`.\n",
      " |\n",
      " |  requires_grad_(...)\n",
      " |      requires_grad_(requires_grad=True) -> Tensor\n",
      " |\n",
      " |      Change if autograd should record operations on this tensor: sets this tensor's\n",
      " |      :attr:`requires_grad` attribute in-place. Returns this tensor.\n",
      " |\n",
      " |      :func:`requires_grad_`'s main use case is to tell autograd to begin recording\n",
      " |      operations on a Tensor ``tensor``. If ``tensor`` has ``requires_grad=False``\n",
      " |      (because it was obtained through a DataLoader, or required preprocessing or\n",
      " |      initialization), ``tensor.requires_grad_()`` makes it so that autograd will\n",
      " |      begin to record operations on ``tensor``.\n",
      " |\n",
      " |      Args:\n",
      " |          requires_grad (bool): If autograd should record operations on this tensor.\n",
      " |              Default: ``True``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # Let's say we want to preprocess some saved weights and use\n",
      " |          >>> # the result as new weights.\n",
      " |          >>> saved_weights = [0.1, 0.2, 0.3, 0.25]\n",
      " |          >>> loaded_weights = torch.tensor(saved_weights)\n",
      " |          >>> weights = preprocess(loaded_weights)  # some function\n",
      " |          >>> weights\n",
      " |          tensor([-0.5503,  0.4926, -2.1158, -0.8303])\n",
      " |\n",
      " |          >>> # Now, start to record operations done to weights\n",
      " |          >>> weights.requires_grad_()\n",
      " |          >>> out = weights.pow(2).sum()\n",
      " |          >>> out.backward()\n",
      " |          >>> weights.grad\n",
      " |          tensor([-1.1007,  0.9853, -4.2316, -1.6606])\n",
      " |\n",
      " |  reshape(...)\n",
      " |      reshape(*shape) -> Tensor\n",
      " |\n",
      " |      Returns a tensor with the same data and number of elements as :attr:`self`\n",
      " |      but with the specified shape. This method returns a view if :attr:`shape` is\n",
      " |      compatible with the current shape. See :meth:`torch.Tensor.view` on when it is\n",
      " |      possible to return a view.\n",
      " |\n",
      " |      See :func:`torch.reshape`\n",
      " |\n",
      " |      Args:\n",
      " |          shape (tuple of ints or int...): the desired shape\n",
      " |\n",
      " |  reshape_as(...)\n",
      " |      reshape_as(other) -> Tensor\n",
      " |\n",
      " |      Returns this tensor as the same shape as :attr:`other`.\n",
      " |      ``self.reshape_as(other)`` is equivalent to ``self.reshape(other.sizes())``.\n",
      " |      This method returns a view if ``other.sizes()`` is compatible with the current\n",
      " |      shape. See :meth:`torch.Tensor.view` on when it is possible to return a view.\n",
      " |\n",
      " |      Please see :meth:`reshape` for more information about ``reshape``.\n",
      " |\n",
      " |      Args:\n",
      " |          other (:class:`torch.Tensor`): The result tensor has the same shape\n",
      " |              as :attr:`other`.\n",
      " |\n",
      " |  resize_(...)\n",
      " |      resize_(*sizes, memory_format=torch.contiguous_format) -> Tensor\n",
      " |\n",
      " |      Resizes :attr:`self` tensor to the specified size. If the number of elements is\n",
      " |      larger than the current storage size, then the underlying storage is resized\n",
      " |      to fit the new number of elements. If the number of elements is smaller, the\n",
      " |      underlying storage is not changed. Existing elements are preserved but any new\n",
      " |      memory is uninitialized.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          This is a low-level method. The storage is reinterpreted as C-contiguous,\n",
      " |          ignoring the current strides (unless the target size equals the current\n",
      " |          size, in which case the tensor is left unchanged). For most purposes, you\n",
      " |          will instead want to use :meth:`~Tensor.view()`, which checks for\n",
      " |          contiguity, or :meth:`~Tensor.reshape()`, which copies data if needed. To\n",
      " |          change the size in-place with custom strides, see :meth:`~Tensor.set_()`.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          If :func:`torch.use_deterministic_algorithms()` and\n",
      " |          :attr:`torch.utils.deterministic.fill_uninitialized_memory` are both set to\n",
      " |          ``True``, new elements are initialized to prevent nondeterministic behavior\n",
      " |          from using the result as an input to an operation. Floating point and\n",
      " |          complex values are set to NaN, and integer values are set to the maximum\n",
      " |          value.\n",
      " |\n",
      " |      Args:\n",
      " |          sizes (torch.Size or int...): the desired size\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              Tensor. Default: ``torch.contiguous_format``. Note that memory format of\n",
      " |              :attr:`self` is going to be unaffected if ``self.size()`` matches ``sizes``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
      " |          >>> x.resize_(2, 2)\n",
      " |          tensor([[ 1,  2],\n",
      " |                  [ 3,  4]])\n",
      " |\n",
      " |  resize_as_(...)\n",
      " |      resize_as_(tensor, memory_format=torch.contiguous_format) -> Tensor\n",
      " |\n",
      " |      Resizes the :attr:`self` tensor to be the same size as the specified\n",
      " |      :attr:`tensor`. This is equivalent to ``self.resize_(tensor.size())``.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              Tensor. Default: ``torch.contiguous_format``. Note that memory format of\n",
      " |              :attr:`self` is going to be unaffected if ``self.size()`` matches ``tensor.size()``.\n",
      " |\n",
      " |  resize_as_sparse_(...)\n",
      " |\n",
      " |  resolve_conj(...)\n",
      " |      resolve_conj() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.resolve_conj`\n",
      " |\n",
      " |  resolve_neg(...)\n",
      " |      resolve_neg() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.resolve_neg`\n",
      " |\n",
      " |  retain_grad(...)\n",
      " |      retain_grad() -> None\n",
      " |\n",
      " |      Enables this Tensor to have their :attr:`grad` populated during\n",
      " |      :func:`backward`. This is a no-op for leaf tensors.\n",
      " |\n",
      " |  roll(...)\n",
      " |      roll(shifts, dims) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.roll`\n",
      " |\n",
      " |  rot90(...)\n",
      " |      rot90(k, dims) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.rot90`\n",
      " |\n",
      " |  round(...)\n",
      " |      round(decimals=0) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.round`\n",
      " |\n",
      " |  round_(...)\n",
      " |      round_(decimals=0) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.round`\n",
      " |\n",
      " |  row_indices(...)\n",
      " |\n",
      " |  rsqrt(...)\n",
      " |      rsqrt() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.rsqrt`\n",
      " |\n",
      " |  rsqrt_(...)\n",
      " |      rsqrt_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.rsqrt`\n",
      " |\n",
      " |  scatter(...)\n",
      " |      scatter(dim, index, src) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.scatter_`\n",
      " |\n",
      " |  scatter_(...)\n",
      " |      scatter_(dim, index, src, *, reduce=None) -> Tensor\n",
      " |\n",
      " |      Writes all values from the tensor :attr:`src` into :attr:`self` at the indices\n",
      " |      specified in the :attr:`index` tensor. For each value in :attr:`src`, its output\n",
      " |      index is specified by its index in :attr:`src` for ``dimension != dim`` and by\n",
      " |      the corresponding value in :attr:`index` for ``dimension = dim``.\n",
      " |\n",
      " |      For a 3-D tensor, :attr:`self` is updated as::\n",
      " |\n",
      " |          self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\n",
      " |          self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\n",
      " |          self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n",
      " |\n",
      " |      This is the reverse operation of the manner described in :meth:`~Tensor.gather`.\n",
      " |\n",
      " |      :attr:`self`, :attr:`index` and :attr:`src` (if it is a Tensor) should all have\n",
      " |      the same number of dimensions. It is also required that\n",
      " |      ``index.size(d) <= src.size(d)`` for all dimensions ``d``, and that\n",
      " |      ``index.size(d) <= self.size(d)`` for all dimensions ``d != dim``.\n",
      " |      Note that ``index`` and ``src`` do not broadcast.\n",
      " |\n",
      " |      Moreover, as for :meth:`~Tensor.gather`, the values of :attr:`index` must be\n",
      " |      between ``0`` and ``self.size(dim) - 1`` inclusive.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          When indices are not unique, the behavior is non-deterministic (one of the\n",
      " |          values from ``src`` will be picked arbitrarily) and the gradient will be\n",
      " |          incorrect (it will be propagated to all locations in the source that\n",
      " |          correspond to the same index)!\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          The backward pass is implemented only for ``src.shape == index.shape``.\n",
      " |\n",
      " |      Additionally accepts an optional :attr:`reduce` argument that allows\n",
      " |      specification of an optional reduction operation, which is applied to all\n",
      " |      values in the tensor :attr:`src` into :attr:`self` at the indices\n",
      " |      specified in the :attr:`index`. For each value in :attr:`src`, the reduction\n",
      " |      operation is applied to an index in :attr:`self` which is specified by\n",
      " |      its index in :attr:`src` for ``dimension != dim`` and by the corresponding\n",
      " |      value in :attr:`index` for ``dimension = dim``.\n",
      " |\n",
      " |      Given a 3-D tensor and reduction using the multiplication operation, :attr:`self`\n",
      " |      is updated as::\n",
      " |\n",
      " |          self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0\n",
      " |          self[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1\n",
      " |          self[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2\n",
      " |\n",
      " |      Reducing with the addition operation is the same as using\n",
      " |      :meth:`~torch.Tensor.scatter_add_`.\n",
      " |\n",
      " |      .. warning::\n",
      " |          The reduce argument with Tensor ``src`` is deprecated and will be removed in\n",
      " |          a future PyTorch release. Please use :meth:`~torch.Tensor.scatter_reduce_`\n",
      " |          instead for more reduction options.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int): the axis along which to index\n",
      " |          index (LongTensor): the indices of elements to scatter, can be either empty\n",
      " |              or of the same dimensionality as ``src``. When empty, the operation\n",
      " |              returns ``self`` unchanged.\n",
      " |          src (Tensor): the source element(s) to scatter.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          reduce (str, optional): reduction operation to apply, can be either\n",
      " |              ``'add'`` or ``'multiply'``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> src = torch.arange(1, 11).reshape((2, 5))\n",
      " |          >>> src\n",
      " |          tensor([[ 1,  2,  3,  4,  5],\n",
      " |                  [ 6,  7,  8,  9, 10]])\n",
      " |          >>> index = torch.tensor([[0, 1, 2, 0]])\n",
      " |          >>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)\n",
      " |          tensor([[1, 0, 0, 4, 0],\n",
      " |                  [0, 2, 0, 0, 0],\n",
      " |                  [0, 0, 3, 0, 0]])\n",
      " |          >>> index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n",
      " |          >>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)\n",
      " |          tensor([[1, 2, 3, 0, 0],\n",
      " |                  [6, 7, 0, 0, 8],\n",
      " |                  [0, 0, 0, 0, 0]])\n",
      " |\n",
      " |          >>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
      " |          ...            1.23, reduce='multiply')\n",
      " |          tensor([[2.0000, 2.0000, 2.4600, 2.0000],\n",
      " |                  [2.0000, 2.0000, 2.0000, 2.4600]])\n",
      " |          >>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
      " |          ...            1.23, reduce='add')\n",
      " |          tensor([[2.0000, 2.0000, 3.2300, 2.0000],\n",
      " |                  [2.0000, 2.0000, 2.0000, 3.2300]])\n",
      " |\n",
      " |      .. function:: scatter_(dim, index, value, *, reduce=None) -> Tensor:\n",
      " |         :noindex:\n",
      " |\n",
      " |      Writes the value from :attr:`value` into :attr:`self` at the indices\n",
      " |      specified in the :attr:`index` tensor.  This operation is equivalent to the previous version,\n",
      " |      with the :attr:`src` tensor filled entirely with :attr:`value`.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int): the axis along which to index\n",
      " |          index (LongTensor): the indices of elements to scatter, can be either empty\n",
      " |              or of the same dimensionality as ``src``. When empty, the operation\n",
      " |              returns ``self`` unchanged.\n",
      " |          value (Scalar): the value to scatter.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          reduce (str, optional): reduction operation to apply, can be either\n",
      " |              ``'add'`` or ``'multiply'``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> index = torch.tensor([[0, 1]])\n",
      " |          >>> value = 2\n",
      " |          >>> torch.zeros(3, 5).scatter_(0, index, value)\n",
      " |          tensor([[2., 0., 0., 0., 0.],\n",
      " |                  [0., 2., 0., 0., 0.],\n",
      " |                  [0., 0., 0., 0., 0.]])\n",
      " |\n",
      " |  scatter_add(...)\n",
      " |      scatter_add(dim, index, src) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.scatter_add_`\n",
      " |\n",
      " |  scatter_add_(...)\n",
      " |      scatter_add_(dim, index, src) -> Tensor\n",
      " |\n",
      " |      Adds all values from the tensor :attr:`src` into :attr:`self` at the indices\n",
      " |      specified in the :attr:`index` tensor in a similar fashion as\n",
      " |      :meth:`~torch.Tensor.scatter_`. For each value in :attr:`src`, it is added to\n",
      " |      an index in :attr:`self` which is specified by its index in :attr:`src`\n",
      " |      for ``dimension != dim`` and by the corresponding value in :attr:`index` for\n",
      " |      ``dimension = dim``.\n",
      " |\n",
      " |      For a 3-D tensor, :attr:`self` is updated as::\n",
      " |\n",
      " |          self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\n",
      " |          self[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\n",
      " |          self[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2\n",
      " |\n",
      " |      :attr:`self`, :attr:`index` and :attr:`src` should have same number of\n",
      " |      dimensions. It is also required that ``index.size(d) <= src.size(d)`` for all\n",
      " |      dimensions ``d``, and that ``index.size(d) <= self.size(d)`` for all dimensions\n",
      " |      ``d != dim``. Note that ``index`` and ``src`` do not broadcast.\n",
      " |\n",
      " |      Note:\n",
      " |          This operation may behave nondeterministically when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          The backward pass is implemented only for ``src.shape == index.shape``.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int): the axis along which to index\n",
      " |          index (LongTensor): the indices of elements to scatter and add, can be\n",
      " |              either empty or of the same dimensionality as ``src``. When empty, the\n",
      " |              operation returns ``self`` unchanged.\n",
      " |          src (Tensor): the source elements to scatter and add\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> src = torch.ones((2, 5))\n",
      " |          >>> index = torch.tensor([[0, 1, 2, 0, 0]])\n",
      " |          >>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\n",
      " |          tensor([[1., 0., 0., 1., 1.],\n",
      " |                  [0., 1., 0., 0., 0.],\n",
      " |                  [0., 0., 1., 0., 0.]])\n",
      " |          >>> index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]])\n",
      " |          >>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\n",
      " |          tensor([[2., 0., 0., 1., 1.],\n",
      " |                  [0., 2., 0., 0., 0.],\n",
      " |                  [0., 0., 2., 1., 1.]])\n",
      " |\n",
      " |  scatter_reduce(...)\n",
      " |      scatter_reduce(dim, index, src, reduce, *, include_self=True) -> Tensor\n",
      " |\n",
      " |      Out-of-place version of :meth:`torch.Tensor.scatter_reduce_`\n",
      " |\n",
      " |  scatter_reduce_(...)\n",
      " |      scatter_reduce_(dim, index, src, reduce, *, include_self=True) -> Tensor\n",
      " |\n",
      " |      Reduces all values from the :attr:`src` tensor to the indices specified in\n",
      " |      the :attr:`index` tensor in the :attr:`self` tensor using the applied reduction\n",
      " |      defined via the :attr:`reduce` argument (:obj:`\"sum\"`, :obj:`\"prod\"`, :obj:`\"mean\"`,\n",
      " |      :obj:`\"amax\"`, :obj:`\"amin\"`). For each value in :attr:`src`, it is reduced to an\n",
      " |      index in :attr:`self` which is specified by its index in :attr:`src` for\n",
      " |      ``dimension != dim`` and by the corresponding value in :attr:`index` for\n",
      " |      ``dimension = dim``. If :obj:`include_self=\"True\"`, the values in the :attr:`self`\n",
      " |      tensor are included in the reduction.\n",
      " |\n",
      " |      :attr:`self`, :attr:`index` and :attr:`src` should all have\n",
      " |      the same number of dimensions. It is also required that\n",
      " |      ``index.size(d) <= src.size(d)`` for all dimensions ``d``, and that\n",
      " |      ``index.size(d) <= self.size(d)`` for all dimensions ``d != dim``.\n",
      " |      Note that ``index`` and ``src`` do not broadcast.\n",
      " |\n",
      " |      For a 3-D tensor with :obj:`reduce=\"sum\"` and :obj:`include_self=True` the\n",
      " |      output is given as::\n",
      " |\n",
      " |          self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\n",
      " |          self[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\n",
      " |          self[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2\n",
      " |\n",
      " |      Note:\n",
      " |          This operation may behave nondeterministically when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          The backward pass is implemented only for ``src.shape == index.shape``.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          This function is in beta and may change in the near future.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int): the axis along which to index\n",
      " |          index (LongTensor): the indices of elements to scatter and reduce.\n",
      " |          src (Tensor): the source elements to scatter and reduce\n",
      " |          reduce (str): the reduction operation to apply for non-unique indices\n",
      " |              (:obj:`\"sum\"`, :obj:`\"prod\"`, :obj:`\"mean\"`, :obj:`\"amax\"`, :obj:`\"amin\"`)\n",
      " |          include_self (bool): whether elements from the :attr:`self` tensor are\n",
      " |              included in the reduction\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> src = torch.tensor([1., 2., 3., 4., 5., 6.])\n",
      " |          >>> index = torch.tensor([0, 1, 0, 1, 2, 1])\n",
      " |          >>> input = torch.tensor([1., 2., 3., 4.])\n",
      " |          >>> input.scatter_reduce(0, index, src, reduce=\"sum\")\n",
      " |          tensor([5., 14., 8., 4.])\n",
      " |          >>> input.scatter_reduce(0, index, src, reduce=\"sum\", include_self=False)\n",
      " |          tensor([4., 12., 5., 4.])\n",
      " |          >>> input2 = torch.tensor([5., 4., 3., 2.])\n",
      " |          >>> input2.scatter_reduce(0, index, src, reduce=\"amax\")\n",
      " |          tensor([5., 6., 5., 2.])\n",
      " |          >>> input2.scatter_reduce(0, index, src, reduce=\"amax\", include_self=False)\n",
      " |          tensor([3., 6., 5., 2.])\n",
      " |\n",
      " |  select(...)\n",
      " |      select(dim, index) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.select`\n",
      " |\n",
      " |  select_scatter(...)\n",
      " |      select_scatter(src, dim, index) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.select_scatter`\n",
      " |\n",
      " |  set_(...)\n",
      " |      set_(source=None, storage_offset=0, size=None, stride=None) -> Tensor\n",
      " |\n",
      " |      Sets the underlying storage, size, and strides. If :attr:`source` is a tensor,\n",
      " |      :attr:`self` tensor will share the same storage and have the same size and\n",
      " |      strides as :attr:`source`. Changes to elements in one tensor will be reflected\n",
      " |      in the other.\n",
      " |\n",
      " |      If :attr:`source` is a :class:`~torch.Storage`, the method sets the underlying\n",
      " |      storage, offset, size, and stride.\n",
      " |\n",
      " |      Args:\n",
      " |          source (Tensor or Storage): the tensor or storage to use\n",
      " |          storage_offset (int, optional): the offset in the storage\n",
      " |          size (torch.Size, optional): the desired size. Defaults to the size of the source.\n",
      " |          stride (tuple, optional): the desired stride. Defaults to C-contiguous strides.\n",
      " |\n",
      " |  sgn(...)\n",
      " |      sgn() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sgn`\n",
      " |\n",
      " |  sgn_(...)\n",
      " |      sgn_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.sgn`\n",
      " |\n",
      " |  short(...)\n",
      " |      short(memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      ``self.short()`` is equivalent to ``self.to(torch.int16)``. See :func:`to`.\n",
      " |\n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  sigmoid(...)\n",
      " |      sigmoid() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sigmoid`\n",
      " |\n",
      " |  sigmoid_(...)\n",
      " |      sigmoid_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.sigmoid`\n",
      " |\n",
      " |  sign(...)\n",
      " |      sign() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sign`\n",
      " |\n",
      " |  sign_(...)\n",
      " |      sign_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.sign`\n",
      " |\n",
      " |  signbit(...)\n",
      " |      signbit() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.signbit`\n",
      " |\n",
      " |  sin(...)\n",
      " |      sin() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sin`\n",
      " |\n",
      " |  sin_(...)\n",
      " |      sin_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.sin`\n",
      " |\n",
      " |  sinc(...)\n",
      " |      sinc() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sinc`\n",
      " |\n",
      " |  sinc_(...)\n",
      " |      sinc_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.sinc`\n",
      " |\n",
      " |  sinh(...)\n",
      " |      sinh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sinh`\n",
      " |\n",
      " |  sinh_(...)\n",
      " |      sinh_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.sinh`\n",
      " |\n",
      " |  size(...)\n",
      " |      size(dim=None) -> torch.Size or int\n",
      " |\n",
      " |      Returns the size of the :attr:`self` tensor. If ``dim`` is not specified,\n",
      " |      the returned value is a :class:`torch.Size`, a subclass of :class:`tuple`.\n",
      " |      If ``dim`` is specified, returns an int holding the size of that dimension.\n",
      " |\n",
      " |      Args:\n",
      " |        dim (int, optional): The dimension for which to retrieve the size.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> t = torch.empty(3, 4, 5)\n",
      " |          >>> t.size()\n",
      " |          torch.Size([3, 4, 5])\n",
      " |          >>> t.size(dim=1)\n",
      " |          4\n",
      " |\n",
      " |  slice_inverse(...)\n",
      " |\n",
      " |  slice_scatter(...)\n",
      " |      slice_scatter(src, dim=0, start=None, end=None, step=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.slice_scatter`\n",
      " |\n",
      " |  slogdet(...)\n",
      " |      slogdet() -> (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.slogdet`\n",
      " |\n",
      " |  smm(...)\n",
      " |      smm(mat) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.smm`\n",
      " |\n",
      " |  softmax(...)\n",
      " |      softmax(dim) -> Tensor\n",
      " |\n",
      " |      Alias for :func:`torch.nn.functional.softmax`.\n",
      " |\n",
      " |  sort(...)\n",
      " |      sort(dim=-1, descending=False) -> (Tensor, LongTensor)\n",
      " |\n",
      " |      See :func:`torch.sort`\n",
      " |\n",
      " |  sparse_dim(...)\n",
      " |      sparse_dim() -> int\n",
      " |\n",
      " |      Return the number of sparse dimensions in a :ref:`sparse tensor <sparse-docs>` :attr:`self`.\n",
      " |\n",
      " |      .. note::\n",
      " |        Returns ``0`` if :attr:`self` is not a sparse tensor.\n",
      " |\n",
      " |      See also :meth:`Tensor.dense_dim` and :ref:`hybrid tensors <sparse-hybrid-coo-docs>`.\n",
      " |\n",
      " |  sparse_mask(...)\n",
      " |      sparse_mask(mask) -> Tensor\n",
      " |\n",
      " |      Returns a new :ref:`sparse tensor <sparse-docs>` with values from a\n",
      " |      strided tensor :attr:`self` filtered by the indices of the sparse\n",
      " |      tensor :attr:`mask`. The values of :attr:`mask` sparse tensor are\n",
      " |      ignored. :attr:`self` and :attr:`mask` tensors must have the same\n",
      " |      shape.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |        The returned sparse tensor might contain duplicate values if :attr:`mask`\n",
      " |        is not coalesced. It is therefore advisable to pass ``mask.coalesce()``\n",
      " |        if such behavior is not desired.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |        The returned sparse tensor has the same indices as the sparse tensor\n",
      " |        :attr:`mask`, even when the corresponding values in :attr:`self` are\n",
      " |        zeros.\n",
      " |\n",
      " |      Args:\n",
      " |          mask (Tensor): a sparse tensor whose indices are used as a filter\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> nse = 5\n",
      " |          >>> dims = (5, 5, 2, 2)\n",
      " |          >>> I = torch.cat([torch.randint(0, dims[0], size=(nse,)),\n",
      " |          ...                torch.randint(0, dims[1], size=(nse,))], 0).reshape(2, nse)\n",
      " |          >>> V = torch.randn(nse, dims[2], dims[3])\n",
      " |          >>> S = torch.sparse_coo_tensor(I, V, dims).coalesce()\n",
      " |          >>> D = torch.randn(dims)\n",
      " |          >>> D.sparse_mask(S)\n",
      " |          tensor(indices=tensor([[0, 0, 0, 2],\n",
      " |                                 [0, 1, 4, 3]]),\n",
      " |                 values=tensor([[[ 1.6550,  0.2397],\n",
      " |                                 [-0.1611, -0.0779]],\n",
      " |\n",
      " |                                [[ 0.2326, -1.0558],\n",
      " |                                 [ 1.4711,  1.9678]],\n",
      " |\n",
      " |                                [[-0.5138, -0.0411],\n",
      " |                                 [ 1.9417,  0.5158]],\n",
      " |\n",
      " |                                [[ 0.0793,  0.0036],\n",
      " |                                 [-0.2569, -0.1055]]]),\n",
      " |                 size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)\n",
      " |\n",
      " |  sparse_resize_(...)\n",
      " |      sparse_resize_(size, sparse_dim, dense_dim) -> Tensor\n",
      " |\n",
      " |      Resizes :attr:`self` :ref:`sparse tensor <sparse-docs>` to the desired\n",
      " |      size and the number of sparse and dense dimensions.\n",
      " |\n",
      " |      .. note::\n",
      " |        If the number of specified elements in :attr:`self` is zero, then\n",
      " |        :attr:`size`, :attr:`sparse_dim`, and :attr:`dense_dim` can be any\n",
      " |        size and positive integers such that ``len(size) == sparse_dim +\n",
      " |        dense_dim``.\n",
      " |\n",
      " |        If :attr:`self` specifies one or more elements, however, then each\n",
      " |        dimension in :attr:`size` must not be smaller than the corresponding\n",
      " |        dimension of :attr:`self`, :attr:`sparse_dim` must equal the number\n",
      " |        of sparse dimensions in :attr:`self`, and :attr:`dense_dim` must\n",
      " |        equal the number of dense dimensions in :attr:`self`.\n",
      " |\n",
      " |      .. warning::\n",
      " |        Throws an error if :attr:`self` is not a sparse tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          size (torch.Size): the desired size. If :attr:`self` is non-empty\n",
      " |            sparse tensor, the desired size cannot be smaller than the\n",
      " |            original size.\n",
      " |          sparse_dim (int): the number of sparse dimensions\n",
      " |          dense_dim (int): the number of dense dimensions\n",
      " |\n",
      " |  sparse_resize_and_clear_(...)\n",
      " |      sparse_resize_and_clear_(size, sparse_dim, dense_dim) -> Tensor\n",
      " |\n",
      " |      Removes all specified elements from a :ref:`sparse tensor\n",
      " |      <sparse-docs>` :attr:`self` and resizes :attr:`self` to the desired\n",
      " |      size and the number of sparse and dense dimensions.\n",
      " |\n",
      " |      .. warning:\n",
      " |        Throws an error if :attr:`self` is not a sparse tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          size (torch.Size): the desired size.\n",
      " |          sparse_dim (int): the number of sparse dimensions\n",
      " |          dense_dim (int): the number of dense dimensions\n",
      " |\n",
      " |  split_with_sizes(...)\n",
      " |\n",
      " |  sqrt(...)\n",
      " |      sqrt() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sqrt`\n",
      " |\n",
      " |  sqrt_(...)\n",
      " |      sqrt_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.sqrt`\n",
      " |\n",
      " |  square(...)\n",
      " |      square() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.square`\n",
      " |\n",
      " |  square_(...)\n",
      " |      square_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.square`\n",
      " |\n",
      " |  squeeze(...)\n",
      " |      squeeze(dim=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.squeeze`\n",
      " |\n",
      " |  squeeze_(...)\n",
      " |      squeeze_(dim=None) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.squeeze`\n",
      " |\n",
      " |  sspaddmm(...)\n",
      " |      sspaddmm(mat1, mat2, *, beta=1, alpha=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sspaddmm`\n",
      " |\n",
      " |  std(...)\n",
      " |      std(dim=None, *, correction=1, keepdim=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.std`\n",
      " |\n",
      " |  storage_offset(...)\n",
      " |      storage_offset() -> int\n",
      " |\n",
      " |      Returns :attr:`self` tensor's offset in the underlying storage in terms of\n",
      " |      number of storage elements (not bytes).\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.tensor([1, 2, 3, 4, 5])\n",
      " |          >>> x.storage_offset()\n",
      " |          0\n",
      " |          >>> x[3:].storage_offset()\n",
      " |          3\n",
      " |\n",
      " |  stride(...)\n",
      " |      stride(dim) -> tuple or int\n",
      " |\n",
      " |      Returns the stride of :attr:`self` tensor.\n",
      " |\n",
      " |      Stride is the jump necessary to go from one element to the next one in the\n",
      " |      specified dimension :attr:`dim`. A tuple of all strides is returned when no\n",
      " |      argument is passed in. Otherwise, an integer value is returned as the stride in\n",
      " |      the particular dimension :attr:`dim`.\n",
      " |\n",
      " |      Args:\n",
      " |          dim (int, optional): the desired dimension in which stride is required\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n",
      " |          >>> x.stride()\n",
      " |          (5, 1)\n",
      " |          >>> x.stride(0)\n",
      " |          5\n",
      " |          >>> x.stride(-1)\n",
      " |          1\n",
      " |\n",
      " |  sub(...)\n",
      " |      sub(other, *, alpha=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sub`.\n",
      " |\n",
      " |  sub_(...)\n",
      " |      sub_(other, *, alpha=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.sub`\n",
      " |\n",
      " |  subtract(...)\n",
      " |      subtract(other, *, alpha=1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.subtract`.\n",
      " |\n",
      " |  subtract_(...)\n",
      " |      subtract_(other, *, alpha=1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.subtract`.\n",
      " |\n",
      " |  sum(...)\n",
      " |      sum(dim=None, keepdim=False, dtype=None) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.sum`\n",
      " |\n",
      " |  sum_to_size(...)\n",
      " |      sum_to_size(*size) -> Tensor\n",
      " |\n",
      " |      Sum ``this`` tensor to :attr:`size`.\n",
      " |      :attr:`size` must be broadcastable to ``this`` tensor size.\n",
      " |\n",
      " |      Args:\n",
      " |          size (int...): a sequence of integers defining the shape of the output tensor.\n",
      " |\n",
      " |  svd(...)\n",
      " |      svd(some=True, compute_uv=True) -> (Tensor, Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.svd`\n",
      " |\n",
      " |  swapaxes(...)\n",
      " |      swapaxes(axis0, axis1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.swapaxes`\n",
      " |\n",
      " |  swapaxes_(...)\n",
      " |      swapaxes_(axis0, axis1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.swapaxes`\n",
      " |\n",
      " |  swapdims(...)\n",
      " |      swapdims(dim0, dim1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.swapdims`\n",
      " |\n",
      " |  swapdims_(...)\n",
      " |      swapdims_(dim0, dim1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.swapdims`\n",
      " |\n",
      " |  t(...)\n",
      " |      t() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.t`\n",
      " |\n",
      " |  t_(...)\n",
      " |      t_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.t`\n",
      " |\n",
      " |  take(...)\n",
      " |      take(indices) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.take`\n",
      " |\n",
      " |  take_along_dim(...)\n",
      " |      take_along_dim(indices, dim) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.take_along_dim`\n",
      " |\n",
      " |  tan(...)\n",
      " |      tan() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.tan`\n",
      " |\n",
      " |  tan_(...)\n",
      " |      tan_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.tan`\n",
      " |\n",
      " |  tanh(...)\n",
      " |      tanh() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.tanh`\n",
      " |\n",
      " |  tanh_(...)\n",
      " |      tanh_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.tanh`\n",
      " |\n",
      " |  tensor_split(...)\n",
      " |      tensor_split(indices_or_sections, dim=0) -> List of Tensors\n",
      " |\n",
      " |      See :func:`torch.tensor_split`\n",
      " |\n",
      " |  tile(...)\n",
      " |      tile(dims) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.tile`\n",
      " |\n",
      " |  to(...)\n",
      " |      to(*args, **kwargs) -> Tensor\n",
      " |\n",
      " |      Performs Tensor dtype and/or device conversion. A :class:`torch.dtype` and :class:`torch.device` are\n",
      " |      inferred from the arguments of ``self.to(*args, **kwargs)``.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          If the ``self`` Tensor already\n",
      " |          has the correct :class:`torch.dtype` and :class:`torch.device`, then ``self`` is returned.\n",
      " |          Otherwise, the returned tensor is a copy of ``self`` with the desired\n",
      " |          :class:`torch.dtype` and :class:`torch.device`.\n",
      " |\n",
      " |      Here are the ways to call ``to``:\n",
      " |\n",
      " |      .. method:: to(dtype, non_blocking=False, copy=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |         :noindex:\n",
      " |\n",
      " |          Returns a Tensor with the specified :attr:`dtype`\n",
      " |\n",
      " |          Args:\n",
      " |              memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |      .. method:: to(device=None, dtype=None, non_blocking=False, copy=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |         :noindex:\n",
      " |\n",
      " |          Returns a Tensor with the specified :attr:`device` and (optional)\n",
      " |          :attr:`dtype`. If :attr:`dtype` is ``None`` it is inferred to be ``self.dtype``.\n",
      " |          When :attr:`non_blocking` is set to ``True``, the function attempts to perform\n",
      " |          the conversion asynchronously with respect to the host, if possible. This\n",
      " |          asynchronous behavior applies to both pinned and pageable memory. However,\n",
      " |          caution is advised when using this feature. For more information, refer to the\n",
      " |          `tutorial on good usage of non_blocking and pin_memory <https://pytorch.org/tutorials/intermediate/pinmem_nonblock.html>`__.\n",
      " |          When :attr:`copy` is set, a new Tensor is created even when the Tensor\n",
      " |          already matches the desired conversion.\n",
      " |\n",
      " |          Args:\n",
      " |              memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |      .. method:: to(other, non_blocking=False, copy=False) -> Tensor\n",
      " |         :noindex:\n",
      " |\n",
      " |          Returns a Tensor with same :class:`torch.dtype` and :class:`torch.device` as\n",
      " |          the Tensor :attr:`other`.\n",
      " |          When :attr:`non_blocking` is set to ``True``, the function attempts to perform\n",
      " |          the conversion asynchronously with respect to the host, if possible. This\n",
      " |          asynchronous behavior applies to both pinned and pageable memory. However,\n",
      " |          caution is advised when using this feature. For more information, refer to the\n",
      " |          `tutorial on good usage of non_blocking and pin_memory <https://pytorch.org/tutorials/intermediate/pinmem_nonblock.html>`__.\n",
      " |          When :attr:`copy` is set, a new Tensor is created even when the Tensor\n",
      " |          already matches the desired conversion.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu\n",
      " |          >>> tensor.to(torch.float64)\n",
      " |          tensor([[-0.5044,  0.0005],\n",
      " |                  [ 0.3310, -0.0584]], dtype=torch.float64)\n",
      " |\n",
      " |          >>> cuda0 = torch.device('cuda:0')\n",
      " |          >>> tensor.to(cuda0)\n",
      " |          tensor([[-0.5044,  0.0005],\n",
      " |                  [ 0.3310, -0.0584]], device='cuda:0')\n",
      " |\n",
      " |          >>> tensor.to(cuda0, dtype=torch.float64)\n",
      " |          tensor([[-0.5044,  0.0005],\n",
      " |                  [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n",
      " |\n",
      " |          >>> other = torch.randn((), dtype=torch.float64, device=cuda0)\n",
      " |          >>> tensor.to(other, non_blocking=True)\n",
      " |          tensor([[-0.5044,  0.0005],\n",
      " |                  [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n",
      " |\n",
      " |  to_dense(...)\n",
      " |      to_dense(dtype=None, *, masked_grad=True) -> Tensor\n",
      " |\n",
      " |      Creates a strided copy of :attr:`self` if :attr:`self` is not a strided tensor, otherwise returns :attr:`self`.\n",
      " |\n",
      " |      Keyword args:\n",
      " |          {dtype}\n",
      " |          masked_grad (bool, optional): If set to ``True`` (default) and\n",
      " |            :attr:`self` has a sparse layout then the backward of\n",
      " |            :meth:`to_dense` returns ``grad.sparse_mask(self)``.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> s = torch.sparse_coo_tensor(\n",
      " |          ...        torch.tensor([[1, 1],\n",
      " |          ...                      [0, 2]]),\n",
      " |          ...        torch.tensor([9, 10]),\n",
      " |          ...        size=(3, 3))\n",
      " |          >>> s.to_dense()\n",
      " |          tensor([[ 0,  0,  0],\n",
      " |                  [ 9,  0, 10],\n",
      " |                  [ 0,  0,  0]])\n",
      " |\n",
      " |  to_mkldnn(...)\n",
      " |      to_mkldnn() -> Tensor\n",
      " |      Returns a copy of the tensor in ``torch.mkldnn`` layout.\n",
      " |\n",
      " |  to_padded_tensor(...)\n",
      " |      to_padded_tensor(padding, output_size=None) -> Tensor\n",
      " |      See :func:`to_padded_tensor`\n",
      " |\n",
      " |  to_sparse(...)\n",
      " |      to_sparse(sparseDims) -> Tensor\n",
      " |\n",
      " |      Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in\n",
      " |      :ref:`coordinate format <sparse-coo-docs>`.\n",
      " |\n",
      " |      Args:\n",
      " |          sparseDims (int, optional): the number of sparse dimensions to include in the new sparse tensor\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]])\n",
      " |          >>> d\n",
      " |          tensor([[ 0,  0,  0],\n",
      " |                  [ 9,  0, 10],\n",
      " |                  [ 0,  0,  0]])\n",
      " |          >>> d.to_sparse()\n",
      " |          tensor(indices=tensor([[1, 1],\n",
      " |                                 [0, 2]]),\n",
      " |                 values=tensor([ 9, 10]),\n",
      " |                 size=(3, 3), nnz=2, layout=torch.sparse_coo)\n",
      " |          >>> d.to_sparse(1)\n",
      " |          tensor(indices=tensor([[1]]),\n",
      " |                 values=tensor([[ 9,  0, 10]]),\n",
      " |                 size=(3, 3), nnz=1, layout=torch.sparse_coo)\n",
      " |\n",
      " |      .. method:: to_sparse(*, layout=None, blocksize=None, dense_dim=None) -> Tensor\n",
      " |         :noindex:\n",
      " |\n",
      " |      Returns a sparse tensor with the specified layout and blocksize.  If\n",
      " |      the :attr:`self` is strided, the number of dense dimensions could be\n",
      " |      specified, and a hybrid sparse tensor will be created, with\n",
      " |      `dense_dim` dense dimensions and `self.dim() - 2 - dense_dim` batch\n",
      " |      dimension.\n",
      " |\n",
      " |      .. note:: If the :attr:`self` layout and blocksize parameters match\n",
      " |                with the specified layout and blocksize, return\n",
      " |                :attr:`self`. Otherwise, return a sparse tensor copy of\n",
      " |                :attr:`self`.\n",
      " |\n",
      " |      Args:\n",
      " |\n",
      " |          layout (:class:`torch.layout`, optional): The desired sparse\n",
      " |            layout. One of ``torch.sparse_coo``, ``torch.sparse_csr``,\n",
      " |            ``torch.sparse_csc``, ``torch.sparse_bsr``, or\n",
      " |            ``torch.sparse_bsc``. Default: if ``None``,\n",
      " |            ``torch.sparse_coo``.\n",
      " |\n",
      " |          blocksize (list, tuple, :class:`torch.Size`, optional): Block size\n",
      " |            of the resulting BSR or BSC tensor. For other layouts,\n",
      " |            specifying the block size that is not ``None`` will result in a\n",
      " |            RuntimeError exception.  A block size must be a tuple of length\n",
      " |            two such that its items evenly divide the two sparse dimensions.\n",
      " |\n",
      " |          dense_dim (int, optional): Number of dense dimensions of the\n",
      " |            resulting CSR, CSC, BSR or BSC tensor.  This argument should be\n",
      " |            used only if :attr:`self` is a strided tensor, and must be a\n",
      " |            value between 0 and dimension of :attr:`self` tensor minus two.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.tensor([[1, 0], [0, 0], [2, 3]])\n",
      " |          >>> x.to_sparse(layout=torch.sparse_coo)\n",
      " |          tensor(indices=tensor([[0, 2, 2],\n",
      " |                                 [0, 0, 1]]),\n",
      " |                 values=tensor([1, 2, 3]),\n",
      " |                 size=(3, 2), nnz=3, layout=torch.sparse_coo)\n",
      " |          >>> x.to_sparse(layout=torch.sparse_bsr, blocksize=(1, 2))\n",
      " |          tensor(crow_indices=tensor([0, 1, 1, 2]),\n",
      " |                 col_indices=tensor([0, 0]),\n",
      " |                 values=tensor([[[1, 0]],\n",
      " |                                [[2, 3]]]), size=(3, 2), nnz=2, layout=torch.sparse_bsr)\n",
      " |          >>> x.to_sparse(layout=torch.sparse_bsr, blocksize=(2, 1))\n",
      " |          RuntimeError: Tensor size(-2) 3 needs to be divisible by blocksize[0] 2\n",
      " |          >>> x.to_sparse(layout=torch.sparse_csr, blocksize=(3, 1))\n",
      " |          RuntimeError: to_sparse for Strided to SparseCsr conversion does not use specified blocksize\n",
      " |\n",
      " |          >>> x = torch.tensor([[[1], [0]], [[0], [0]], [[2], [3]]])\n",
      " |          >>> x.to_sparse(layout=torch.sparse_csr, dense_dim=1)\n",
      " |          tensor(crow_indices=tensor([0, 1, 1, 3]),\n",
      " |                 col_indices=tensor([0, 0, 1]),\n",
      " |                 values=tensor([[1],\n",
      " |                                [2],\n",
      " |                                [3]]), size=(3, 2, 1), nnz=3, layout=torch.sparse_csr)\n",
      " |\n",
      " |  to_sparse_bsc(...)\n",
      " |      to_sparse_bsc(blocksize, dense_dim) -> Tensor\n",
      " |\n",
      " |      Convert a tensor to a block sparse column (BSC) storage format of\n",
      " |      given blocksize.  If the :attr:`self` is strided, then the number of\n",
      " |      dense dimensions could be specified, and a hybrid BSC tensor will be\n",
      " |      created, with `dense_dim` dense dimensions and `self.dim() - 2 -\n",
      " |      dense_dim` batch dimension.\n",
      " |\n",
      " |      Args:\n",
      " |\n",
      " |          blocksize (list, tuple, :class:`torch.Size`, optional): Block size\n",
      " |            of the resulting BSC tensor. A block size must be a tuple of\n",
      " |            length two such that its items evenly divide the two sparse\n",
      " |            dimensions.\n",
      " |\n",
      " |          dense_dim (int, optional): Number of dense dimensions of the\n",
      " |            resulting BSC tensor.  This argument should be used only if\n",
      " |            :attr:`self` is a strided tensor, and must be a value between 0\n",
      " |            and dimension of :attr:`self` tensor minus two.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> dense = torch.randn(10, 10)\n",
      " |          >>> sparse = dense.to_sparse_csr()\n",
      " |          >>> sparse_bsc = sparse.to_sparse_bsc((5, 5))\n",
      " |          >>> sparse_bsc.row_indices()\n",
      " |          tensor([0, 1, 0, 1])\n",
      " |\n",
      " |          >>> dense = torch.zeros(4, 3, 1)\n",
      " |          >>> dense[0:2, 0] = dense[0:2, 2] = dense[2:4, 1] = 1\n",
      " |          >>> dense.to_sparse_bsc((2, 1), 1)\n",
      " |          tensor(ccol_indices=tensor([0, 1, 2, 3]),\n",
      " |                 row_indices=tensor([0, 1, 0]),\n",
      " |                 values=tensor([[[[1.]],\n",
      " |\n",
      " |                                 [[1.]]],\n",
      " |\n",
      " |\n",
      " |                                [[[1.]],\n",
      " |\n",
      " |                                 [[1.]]],\n",
      " |\n",
      " |\n",
      " |                                [[[1.]],\n",
      " |\n",
      " |                                 [[1.]]]]), size=(4, 3, 1), nnz=3,\n",
      " |                 layout=torch.sparse_bsc)\n",
      " |\n",
      " |  to_sparse_bsr(...)\n",
      " |      to_sparse_bsr(blocksize, dense_dim) -> Tensor\n",
      " |\n",
      " |      Convert a tensor to a block sparse row (BSR) storage format of given\n",
      " |      blocksize.  If the :attr:`self` is strided, then the number of dense\n",
      " |      dimensions could be specified, and a hybrid BSR tensor will be\n",
      " |      created, with `dense_dim` dense dimensions and `self.dim() - 2 -\n",
      " |      dense_dim` batch dimension.\n",
      " |\n",
      " |      Args:\n",
      " |\n",
      " |          blocksize (list, tuple, :class:`torch.Size`, optional): Block size\n",
      " |            of the resulting BSR tensor. A block size must be a tuple of\n",
      " |            length two such that its items evenly divide the two sparse\n",
      " |            dimensions.\n",
      " |\n",
      " |          dense_dim (int, optional): Number of dense dimensions of the\n",
      " |            resulting BSR tensor.  This argument should be used only if\n",
      " |            :attr:`self` is a strided tensor, and must be a value between 0\n",
      " |            and dimension of :attr:`self` tensor minus two.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> dense = torch.randn(10, 10)\n",
      " |          >>> sparse = dense.to_sparse_csr()\n",
      " |          >>> sparse_bsr = sparse.to_sparse_bsr((5, 5))\n",
      " |          >>> sparse_bsr.col_indices()\n",
      " |          tensor([0, 1, 0, 1])\n",
      " |\n",
      " |          >>> dense = torch.zeros(4, 3, 1)\n",
      " |          >>> dense[0:2, 0] = dense[0:2, 2] = dense[2:4, 1] = 1\n",
      " |          >>> dense.to_sparse_bsr((2, 1), 1)\n",
      " |          tensor(crow_indices=tensor([0, 2, 3]),\n",
      " |                 col_indices=tensor([0, 2, 1]),\n",
      " |                 values=tensor([[[[1.]],\n",
      " |\n",
      " |                                 [[1.]]],\n",
      " |\n",
      " |\n",
      " |                                [[[1.]],\n",
      " |\n",
      " |                                 [[1.]]],\n",
      " |\n",
      " |\n",
      " |                                [[[1.]],\n",
      " |\n",
      " |                                 [[1.]]]]), size=(4, 3, 1), nnz=3,\n",
      " |                 layout=torch.sparse_bsr)\n",
      " |\n",
      " |  to_sparse_csc(...)\n",
      " |      to_sparse_csc() -> Tensor\n",
      " |\n",
      " |      Convert a tensor to compressed column storage (CSC) format.  Except\n",
      " |      for strided tensors, only works with 2D tensors.  If the :attr:`self`\n",
      " |      is strided, then the number of dense dimensions could be specified,\n",
      " |      and a hybrid CSC tensor will be created, with `dense_dim` dense\n",
      " |      dimensions and `self.dim() - 2 - dense_dim` batch dimension.\n",
      " |\n",
      " |      Args:\n",
      " |\n",
      " |          dense_dim (int, optional): Number of dense dimensions of the\n",
      " |            resulting CSC tensor.  This argument should be used only if\n",
      " |            :attr:`self` is a strided tensor, and must be a value between 0\n",
      " |            and dimension of :attr:`self` tensor minus two.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> dense = torch.randn(5, 5)\n",
      " |          >>> sparse = dense.to_sparse_csc()\n",
      " |          >>> sparse._nnz()\n",
      " |          25\n",
      " |\n",
      " |          >>> dense = torch.zeros(3, 3, 1, 1)\n",
      " |          >>> dense[0, 0] = dense[1, 2] = dense[2, 1] = 1\n",
      " |          >>> dense.to_sparse_csc(dense_dim=2)\n",
      " |          tensor(ccol_indices=tensor([0, 1, 2, 3]),\n",
      " |                 row_indices=tensor([0, 2, 1]),\n",
      " |                 values=tensor([[[1.]],\n",
      " |\n",
      " |                                [[1.]],\n",
      " |\n",
      " |                                [[1.]]]), size=(3, 3, 1, 1), nnz=3,\n",
      " |                 layout=torch.sparse_csc)\n",
      " |\n",
      " |  to_sparse_csr(...)\n",
      " |      to_sparse_csr(dense_dim=None) -> Tensor\n",
      " |\n",
      " |      Convert a tensor to compressed row storage format (CSR).  Except for\n",
      " |      strided tensors, only works with 2D tensors.  If the :attr:`self` is\n",
      " |      strided, then the number of dense dimensions could be specified, and a\n",
      " |      hybrid CSR tensor will be created, with `dense_dim` dense dimensions\n",
      " |      and `self.dim() - 2 - dense_dim` batch dimension.\n",
      " |\n",
      " |      Args:\n",
      " |\n",
      " |          dense_dim (int, optional): Number of dense dimensions of the\n",
      " |            resulting CSR tensor.  This argument should be used only if\n",
      " |            :attr:`self` is a strided tensor, and must be a value between 0\n",
      " |            and dimension of :attr:`self` tensor minus two.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> dense = torch.randn(5, 5)\n",
      " |          >>> sparse = dense.to_sparse_csr()\n",
      " |          >>> sparse._nnz()\n",
      " |          25\n",
      " |\n",
      " |          >>> dense = torch.zeros(3, 3, 1, 1)\n",
      " |          >>> dense[0, 0] = dense[1, 2] = dense[2, 1] = 1\n",
      " |          >>> dense.to_sparse_csr(dense_dim=2)\n",
      " |          tensor(crow_indices=tensor([0, 1, 2, 3]),\n",
      " |                 col_indices=tensor([0, 2, 1]),\n",
      " |                 values=tensor([[[1.]],\n",
      " |\n",
      " |                                [[1.]],\n",
      " |\n",
      " |                                [[1.]]]), size=(3, 3, 1, 1), nnz=3,\n",
      " |                 layout=torch.sparse_csr)\n",
      " |\n",
      " |  tolist(...)\n",
      " |      tolist() -> list or number\n",
      " |\n",
      " |      Returns the tensor as a (nested) list. For scalars, a standard\n",
      " |      Python number is returned, just like with :meth:`~Tensor.item`.\n",
      " |      Tensors are automatically moved to the CPU first if necessary.\n",
      " |\n",
      " |      This operation is not differentiable.\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |          >>> a = torch.randn(2, 2)\n",
      " |          >>> a.tolist()\n",
      " |          [[0.012766935862600803, 0.5415473580360413],\n",
      " |           [-0.08909505605697632, 0.7729271650314331]]\n",
      " |          >>> a[0,0].tolist()\n",
      " |          0.012766935862600803\n",
      " |\n",
      " |  topk(...)\n",
      " |      topk(k, dim=None, largest=True, sorted=True) -> (Tensor, LongTensor)\n",
      " |\n",
      " |      See :func:`torch.topk`\n",
      " |\n",
      " |  trace(...)\n",
      " |      trace() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.trace`\n",
      " |\n",
      " |  transpose(...)\n",
      " |      transpose(dim0, dim1) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.transpose`\n",
      " |\n",
      " |  transpose_(...)\n",
      " |      transpose_(dim0, dim1) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.transpose`\n",
      " |\n",
      " |  triangular_solve(...)\n",
      " |      triangular_solve(A, upper=True, transpose=False, unitriangular=False) -> (Tensor, Tensor)\n",
      " |\n",
      " |      See :func:`torch.triangular_solve`\n",
      " |\n",
      " |  tril(...)\n",
      " |      tril(diagonal=0) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.tril`\n",
      " |\n",
      " |  tril_(...)\n",
      " |      tril_(diagonal=0) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.tril`\n",
      " |\n",
      " |  triu(...)\n",
      " |      triu(diagonal=0) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.triu`\n",
      " |\n",
      " |  triu_(...)\n",
      " |      triu_(diagonal=0) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.triu`\n",
      " |\n",
      " |  true_divide(...)\n",
      " |      true_divide(value) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.true_divide`\n",
      " |\n",
      " |  true_divide_(...)\n",
      " |      true_divide_(value) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.true_divide_`\n",
      " |\n",
      " |  trunc(...)\n",
      " |      trunc() -> Tensor\n",
      " |\n",
      " |      See :func:`torch.trunc`\n",
      " |\n",
      " |  trunc_(...)\n",
      " |      trunc_() -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.trunc`\n",
      " |\n",
      " |  type(...)\n",
      " |      type(dtype=None, non_blocking=False, **kwargs) -> str or Tensor\n",
      " |      Returns the type if `dtype` is not provided, else casts this object to\n",
      " |      the specified type.\n",
      " |\n",
      " |      If this is already of the correct type, no copy is performed and the\n",
      " |      original object is returned.\n",
      " |\n",
      " |      Args:\n",
      " |          dtype (dtype or string): The desired type\n",
      " |          non_blocking (bool): If ``True``, and the source is in pinned memory\n",
      " |              and destination is on the GPU or vice versa, the copy is performed\n",
      " |              asynchronously with respect to the host. Otherwise, the argument\n",
      " |              has no effect.\n",
      " |          **kwargs: For compatibility, may contain the key ``async`` in place of\n",
      " |              the ``non_blocking`` argument. The ``async`` arg is deprecated.\n",
      " |\n",
      " |  type_as(...)\n",
      " |      type_as(tensor) -> Tensor\n",
      " |\n",
      " |      Returns this tensor cast to the type of the given tensor.\n",
      " |\n",
      " |      This is a no-op if the tensor is already of the correct type. This is\n",
      " |      equivalent to ``self.type(tensor.type())``\n",
      " |\n",
      " |      Args:\n",
      " |          tensor (Tensor): the tensor which has the desired type\n",
      " |\n",
      " |  unbind(...)\n",
      " |      unbind(dim=0) -> seq\n",
      " |\n",
      " |      See :func:`torch.unbind`\n",
      " |\n",
      " |  unfold(...)\n",
      " |      unfold(dimension, size, step) -> Tensor\n",
      " |\n",
      " |      Returns a view of the original tensor which contains all slices of size :attr:`size` from\n",
      " |      :attr:`self` tensor in the dimension :attr:`dimension`.\n",
      " |\n",
      " |      Step between two slices is given by :attr:`step`.\n",
      " |\n",
      " |      If `sizedim` is the size of dimension :attr:`dimension` for :attr:`self`, the size of\n",
      " |      dimension :attr:`dimension` in the returned tensor will be\n",
      " |      `(sizedim - size) / step + 1`.\n",
      " |\n",
      " |      An additional dimension of size :attr:`size` is appended in the returned tensor.\n",
      " |\n",
      " |      Args:\n",
      " |          dimension (int): dimension in which unfolding happens\n",
      " |          size (int): the size of each slice that is unfolded\n",
      " |          step (int): the step between each slice\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.arange(1., 8)\n",
      " |          >>> x\n",
      " |          tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])\n",
      " |          >>> x.unfold(0, 2, 1)\n",
      " |          tensor([[ 1.,  2.],\n",
      " |                  [ 2.,  3.],\n",
      " |                  [ 3.,  4.],\n",
      " |                  [ 4.,  5.],\n",
      " |                  [ 5.,  6.],\n",
      " |                  [ 6.,  7.]])\n",
      " |          >>> x.unfold(0, 2, 2)\n",
      " |          tensor([[ 1.,  2.],\n",
      " |                  [ 3.,  4.],\n",
      " |                  [ 5.,  6.]])\n",
      " |\n",
      " |  uniform_(...)\n",
      " |      uniform_(from=0, to=1, *, generator=None) -> Tensor\n",
      " |\n",
      " |      Fills :attr:`self` tensor with numbers sampled from the continuous uniform\n",
      " |      distribution:\n",
      " |\n",
      " |      .. math::\n",
      " |          f(x) = \\dfrac{1}{\\text{to} - \\text{from}}\n",
      " |\n",
      " |  unsafe_chunk(...)\n",
      " |      unsafe_chunk(chunks, dim=0) -> List of Tensors\n",
      " |\n",
      " |      See :func:`torch.unsafe_chunk`\n",
      " |\n",
      " |  unsafe_split(...)\n",
      " |      unsafe_split(split_size, dim=0) -> List of Tensors\n",
      " |\n",
      " |      See :func:`torch.unsafe_split`\n",
      " |\n",
      " |  unsafe_split_with_sizes(...)\n",
      " |\n",
      " |  unsqueeze(...)\n",
      " |      unsqueeze(dim) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.unsqueeze`\n",
      " |\n",
      " |  unsqueeze_(...)\n",
      " |      unsqueeze_(dim) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.unsqueeze`\n",
      " |\n",
      " |  untyped_storage(...)\n",
      " |      untyped_storage() -> torch.UntypedStorage\n",
      " |\n",
      " |      Returns the underlying :class:`UntypedStorage`.\n",
      " |\n",
      " |  values(...)\n",
      " |      values() -> Tensor\n",
      " |\n",
      " |      Return the values tensor of a :ref:`sparse COO tensor <sparse-coo-docs>`.\n",
      " |\n",
      " |      .. warning::\n",
      " |        Throws an error if :attr:`self` is not a sparse COO tensor.\n",
      " |\n",
      " |      See also :meth:`Tensor.indices`.\n",
      " |\n",
      " |      .. note::\n",
      " |        This method can only be called on a coalesced sparse tensor. See\n",
      " |        :meth:`Tensor.coalesce` for details.\n",
      " |\n",
      " |  var(...)\n",
      " |      var(dim=None, *, correction=1, keepdim=False) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.var`\n",
      " |\n",
      " |  vdot(...)\n",
      " |      vdot(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.vdot`\n",
      " |\n",
      " |  view(...)\n",
      " |      view(*shape) -> Tensor\n",
      " |\n",
      " |      Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      " |      different :attr:`shape`.\n",
      " |\n",
      " |      The returned tensor shares the same data and must have the same number\n",
      " |      of elements, but may have a different size. For a tensor to be viewed, the new\n",
      " |      view size must be compatible with its original size and stride, i.e., each new\n",
      " |      view dimension must either be a subspace of an original dimension, or only span\n",
      " |      across original dimensions :math:`d, d+1, \\dots, d+k` that satisfy the following\n",
      " |      contiguity-like condition that :math:`\\forall i = d, \\dots, d+k-1`,\n",
      " |\n",
      " |      .. math::\n",
      " |\n",
      " |        \\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]\n",
      " |\n",
      " |      Otherwise, it will not be possible to view :attr:`self` tensor as :attr:`shape`\n",
      " |      without copying it (e.g., via :meth:`contiguous`). When it is unclear whether a\n",
      " |      :meth:`view` can be performed, it is advisable to use :meth:`reshape`, which\n",
      " |      returns a view if the shapes are compatible, and copies (equivalent to calling\n",
      " |      :meth:`contiguous`) otherwise.\n",
      " |\n",
      " |      Args:\n",
      " |          shape (torch.Size or int...): the desired size\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.randn(4, 4)\n",
      " |          >>> x.size()\n",
      " |          torch.Size([4, 4])\n",
      " |          >>> y = x.view(16)\n",
      " |          >>> y.size()\n",
      " |          torch.Size([16])\n",
      " |          >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
      " |          >>> z.size()\n",
      " |          torch.Size([2, 8])\n",
      " |\n",
      " |          >>> a = torch.randn(1, 2, 3, 4)\n",
      " |          >>> a.size()\n",
      " |          torch.Size([1, 2, 3, 4])\n",
      " |          >>> b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n",
      " |          >>> b.size()\n",
      " |          torch.Size([1, 3, 2, 4])\n",
      " |          >>> c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n",
      " |          >>> c.size()\n",
      " |          torch.Size([1, 3, 2, 4])\n",
      " |          >>> torch.equal(b, c)\n",
      " |          False\n",
      " |\n",
      " |\n",
      " |      .. method:: view(dtype) -> Tensor\n",
      " |         :noindex:\n",
      " |\n",
      " |      Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      " |      different :attr:`dtype`.\n",
      " |\n",
      " |      If the element size of :attr:`dtype` is different than that of ``self.dtype``,\n",
      " |      then the size of the last dimension of the output will be scaled\n",
      " |      proportionally.  For instance, if :attr:`dtype` element size is twice that of\n",
      " |      ``self.dtype``, then each pair of elements in the last dimension of\n",
      " |      :attr:`self` will be combined, and the size of the last dimension of the output\n",
      " |      will be half that of :attr:`self`. If :attr:`dtype` element size is half that\n",
      " |      of ``self.dtype``, then each element in the last dimension of :attr:`self` will\n",
      " |      be split in two, and the size of the last dimension of the output will be\n",
      " |      double that of :attr:`self`. For this to be possible, the following conditions\n",
      " |      must be true:\n",
      " |\n",
      " |          * ``self.dim()`` must be greater than 0.\n",
      " |          * ``self.stride(-1)`` must be 1.\n",
      " |\n",
      " |      Additionally, if the element size of :attr:`dtype` is greater than that of\n",
      " |      ``self.dtype``, the following conditions must be true as well:\n",
      " |\n",
      " |          * ``self.size(-1)`` must be divisible by the ratio between the element\n",
      " |            sizes of the dtypes.\n",
      " |          * ``self.storage_offset()`` must be divisible by the ratio between the\n",
      " |            element sizes of the dtypes.\n",
      " |          * The strides of all dimensions, except the last dimension, must be\n",
      " |            divisible by the ratio between the element sizes of the dtypes.\n",
      " |\n",
      " |      If any of the above conditions are not met, an error is thrown.\n",
      " |\n",
      " |      .. warning::\n",
      " |\n",
      " |          This overload is not supported by TorchScript, and using it in a Torchscript\n",
      " |          program will cause undefined behavior.\n",
      " |\n",
      " |\n",
      " |      Args:\n",
      " |          dtype (:class:`torch.dtype`): the desired dtype\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> x = torch.randn(4, 4)\n",
      " |          >>> x\n",
      " |          tensor([[ 0.9482, -0.0310,  1.4999, -0.5316],\n",
      " |                  [-0.1520,  0.7472,  0.5617, -0.8649],\n",
      " |                  [-2.4724, -0.0334, -0.2976, -0.8499],\n",
      " |                  [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
      " |          >>> x.dtype\n",
      " |          torch.float32\n",
      " |\n",
      " |          >>> y = x.view(torch.int32)\n",
      " |          >>> y\n",
      " |          tensor([[ 1064483442, -1124191867,  1069546515, -1089989247],\n",
      " |                  [-1105482831,  1061112040,  1057999968, -1084397505],\n",
      " |                  [-1071760287, -1123489973, -1097310419, -1084649136],\n",
      " |                  [-1101533110,  1073668768, -1082790149, -1088634448]],\n",
      " |              dtype=torch.int32)\n",
      " |          >>> y[0, 0] = 1000000000\n",
      " |          >>> x\n",
      " |          tensor([[ 0.0047, -0.0310,  1.4999, -0.5316],\n",
      " |                  [-0.1520,  0.7472,  0.5617, -0.8649],\n",
      " |                  [-2.4724, -0.0334, -0.2976, -0.8499],\n",
      " |                  [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
      " |\n",
      " |          >>> x.view(torch.cfloat)\n",
      " |          tensor([[ 0.0047-0.0310j,  1.4999-0.5316j],\n",
      " |                  [-0.1520+0.7472j,  0.5617-0.8649j],\n",
      " |                  [-2.4724-0.0334j, -0.2976-0.8499j],\n",
      " |                  [-0.2109+1.9913j, -0.9607-0.6123j]])\n",
      " |          >>> x.view(torch.cfloat).size()\n",
      " |          torch.Size([4, 2])\n",
      " |\n",
      " |          >>> x.view(torch.uint8)\n",
      " |          tensor([[  0, 202, 154,  59, 182, 243, 253, 188, 185, 252, 191,  63, 240,  22,\n",
      " |                     8, 191],\n",
      " |                  [227, 165,  27, 190, 128,  72,  63,  63, 146, 203,  15,  63,  22, 106,\n",
      " |                    93, 191],\n",
      " |                  [205,  59,  30, 192, 112, 206,   8, 189,   7,  95, 152, 190,  12, 147,\n",
      " |                    89, 191],\n",
      " |                  [ 43, 246,  87, 190, 235, 226, 254,  63, 111, 240, 117, 191, 177, 191,\n",
      " |                    28, 191]], dtype=torch.uint8)\n",
      " |          >>> x.view(torch.uint8).size()\n",
      " |          torch.Size([4, 16])\n",
      " |\n",
      " |  view_as(...)\n",
      " |      view_as(other) -> Tensor\n",
      " |\n",
      " |      View this tensor as the same size as :attr:`other`.\n",
      " |      ``self.view_as(other)`` is equivalent to ``self.view(other.size())``.\n",
      " |\n",
      " |      Please see :meth:`~Tensor.view` for more information about ``view``.\n",
      " |\n",
      " |      Args:\n",
      " |          other (:class:`torch.Tensor`): The result tensor has the same size\n",
      " |              as :attr:`other`.\n",
      " |\n",
      " |  vsplit(...)\n",
      " |      vsplit(split_size_or_sections) -> List of Tensors\n",
      " |\n",
      " |      See :func:`torch.vsplit`\n",
      " |\n",
      " |  where(...)\n",
      " |      where(condition, y) -> Tensor\n",
      " |\n",
      " |      ``self.where(condition, y)`` is equivalent to ``torch.where(condition, self, y)``.\n",
      " |      See :func:`torch.where`\n",
      " |\n",
      " |  xlogy(...)\n",
      " |      xlogy(other) -> Tensor\n",
      " |\n",
      " |      See :func:`torch.xlogy`\n",
      " |\n",
      " |  xlogy_(...)\n",
      " |      xlogy_(other) -> Tensor\n",
      " |\n",
      " |      In-place version of :meth:`~Tensor.xlogy`\n",
      " |\n",
      " |  xpu(...)\n",
      " |      xpu(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |\n",
      " |      Returns a copy of this object in XPU memory.\n",
      " |\n",
      " |      If this object is already in XPU memory and on the correct device,\n",
      " |      then no copy is performed and the original object is returned.\n",
      " |\n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The destination XPU device.\n",
      " |              Defaults to the current XPU device.\n",
      " |          non_blocking (bool): If ``True`` and the source is in pinned memory,\n",
      " |              the copy will be asynchronous with respect to the host.\n",
      " |              Otherwise, the argument has no effect. Default: ``False``.\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |\n",
      " |  zero_(...)\n",
      " |      zero_() -> Tensor\n",
      " |\n",
      " |      Fills :attr:`self` tensor with zeros.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from torch._C.TensorBase:\n",
      " |\n",
      " |  __new__(*args, **kwargs) class method of torch._C.TensorBase\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch._C.TensorBase:\n",
      " |\n",
      " |  H\n",
      " |      Returns a view of a matrix (2-D tensor) conjugated and transposed.\n",
      " |\n",
      " |      ``x.H`` is equivalent to ``x.transpose(0, 1).conj()`` for complex matrices and\n",
      " |      ``x.transpose(0, 1)`` for real matrices.\n",
      " |\n",
      " |      .. seealso::\n",
      " |\n",
      " |              :attr:`~.Tensor.mH`: An attribute that also works on batches of matrices.\n",
      " |\n",
      " |  T\n",
      " |      Returns a view of this tensor with its dimensions reversed.\n",
      " |\n",
      " |      If ``n`` is the number of dimensions in ``x``,\n",
      " |      ``x.T`` is equivalent to ``x.permute(n-1, n-2, ..., 0)``.\n",
      " |\n",
      " |      .. warning::\n",
      " |          The use of :func:`Tensor.T` on tensors of dimension other than 2 to reverse their shape\n",
      " |          is deprecated and it will throw an error in a future release. Consider :attr:`~.Tensor.mT`\n",
      " |          to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse\n",
      " |          the dimensions of a tensor.\n",
      " |\n",
      " |  data\n",
      " |\n",
      " |  device\n",
      " |      Is the :class:`torch.device` where this Tensor is.\n",
      " |\n",
      " |  dtype\n",
      " |\n",
      " |  grad\n",
      " |      This attribute is ``None`` by default and becomes a Tensor the first time a call to\n",
      " |      :func:`backward` computes gradients for ``self``.\n",
      " |      The attribute will then contain the gradients computed and future calls to\n",
      " |      :func:`backward` will accumulate (add) gradients into it.\n",
      " |\n",
      " |  grad_fn\n",
      " |\n",
      " |  imag\n",
      " |      Returns a new tensor containing imaginary values of the :attr:`self` tensor.\n",
      " |      The returned tensor and :attr:`self` share the same underlying storage.\n",
      " |\n",
      " |      .. warning::\n",
      " |          :func:`imag` is only supported for tensors with complex dtypes.\n",
      " |\n",
      " |      Example::\n",
      " |          >>> x=torch.randn(4, dtype=torch.cfloat)\n",
      " |          >>> x\n",
      " |          tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n",
      " |          >>> x.imag\n",
      " |          tensor([ 0.3553, -0.7896, -0.0633, -0.8119])\n",
      " |\n",
      " |  is_cpu\n",
      " |      Is ``True`` if the Tensor is stored on the CPU, ``False`` otherwise.\n",
      " |\n",
      " |  is_cuda\n",
      " |      Is ``True`` if the Tensor is stored on the GPU, ``False`` otherwise.\n",
      " |\n",
      " |  is_ipu\n",
      " |      Is ``True`` if the Tensor is stored on the IPU, ``False`` otherwise.\n",
      " |\n",
      " |  is_leaf\n",
      " |      All Tensors that have :attr:`requires_grad` which is ``False`` will be leaf Tensors by convention.\n",
      " |\n",
      " |      For Tensors that have :attr:`requires_grad` which is ``True``, they will be leaf Tensors if they were\n",
      " |      created by the user. This means that they are not the result of an operation and so\n",
      " |      :attr:`grad_fn` is None.\n",
      " |\n",
      " |      Only leaf Tensors will have their :attr:`grad` populated during a call to :func:`backward`.\n",
      " |      To get :attr:`grad` populated for non-leaf Tensors, you can use :func:`retain_grad`.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> a = torch.rand(10, requires_grad=True)\n",
      " |          >>> a.is_leaf\n",
      " |          True\n",
      " |          >>> b = torch.rand(10, requires_grad=True).cuda()\n",
      " |          >>> b.is_leaf\n",
      " |          False\n",
      " |          # b was created by the operation that cast a cpu Tensor into a cuda Tensor\n",
      " |          >>> c = torch.rand(10, requires_grad=True) + 2\n",
      " |          >>> c.is_leaf\n",
      " |          False\n",
      " |          # c was created by the addition operation\n",
      " |          >>> d = torch.rand(10).cuda()\n",
      " |          >>> d.is_leaf\n",
      " |          True\n",
      " |          # d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)\n",
      " |          >>> e = torch.rand(10).cuda().requires_grad_()\n",
      " |          >>> e.is_leaf\n",
      " |          True\n",
      " |          # e requires gradients and has no operations creating it\n",
      " |          >>> f = torch.rand(10, requires_grad=True, device=\"cuda\")\n",
      " |          >>> f.is_leaf\n",
      " |          True\n",
      " |          # f requires grad, has no operation creating it\n",
      " |\n",
      " |  is_maia\n",
      " |\n",
      " |  is_meta\n",
      " |      Is ``True`` if the Tensor is a meta tensor, ``False`` otherwise.  Meta tensors\n",
      " |      are like normal tensors, but they carry no data.\n",
      " |\n",
      " |  is_mkldnn\n",
      " |\n",
      " |  is_mps\n",
      " |      Is ``True`` if the Tensor is stored on the MPS device, ``False`` otherwise.\n",
      " |\n",
      " |  is_mtia\n",
      " |\n",
      " |  is_nested\n",
      " |\n",
      " |  is_quantized\n",
      " |      Is ``True`` if the Tensor is quantized, ``False`` otherwise.\n",
      " |\n",
      " |  is_sparse\n",
      " |      Is ``True`` if the Tensor uses sparse COO storage layout, ``False`` otherwise.\n",
      " |\n",
      " |  is_sparse_csr\n",
      " |      Is ``True`` if the Tensor uses sparse CSR storage layout, ``False`` otherwise.\n",
      " |\n",
      " |  is_vulkan\n",
      " |\n",
      " |  is_xla\n",
      " |      Is ``True`` if the Tensor is stored on an XLA device, ``False`` otherwise.\n",
      " |\n",
      " |  is_xpu\n",
      " |      Is ``True`` if the Tensor is stored on the XPU, ``False`` otherwise.\n",
      " |\n",
      " |  itemsize\n",
      " |      Alias for :meth:`~Tensor.element_size()`\n",
      " |\n",
      " |  layout\n",
      " |\n",
      " |  mH\n",
      " |      Accessing this property is equivalent to calling :func:`adjoint`.\n",
      " |\n",
      " |  mT\n",
      " |      Returns a view of this tensor with the last two dimensions transposed.\n",
      " |\n",
      " |      ``x.mT`` is equivalent to ``x.transpose(-2, -1)``.\n",
      " |\n",
      " |  name\n",
      " |\n",
      " |  names\n",
      " |      Stores names for each of this tensor's dimensions.\n",
      " |\n",
      " |      ``names[idx]`` corresponds to the name of tensor dimension ``idx``.\n",
      " |      Names are either a string if the dimension is named or ``None`` if the\n",
      " |      dimension is unnamed.\n",
      " |\n",
      " |      Dimension names may contain characters or underscore. Furthermore, a dimension\n",
      " |      name must be a valid Python variable name (i.e., does not start with underscore).\n",
      " |\n",
      " |      Tensors may not have two named dimensions with the same name.\n",
      " |\n",
      " |      .. warning::\n",
      " |          The named tensor API is experimental and subject to change.\n",
      " |\n",
      " |  nbytes\n",
      " |      Returns the number of bytes consumed by the \"view\" of elements of the Tensor\n",
      " |      if the Tensor does not use sparse storage layout.\n",
      " |      Defined to be :meth:`~Tensor.numel()` * :meth:`~Tensor.element_size()`\n",
      " |\n",
      " |  ndim\n",
      " |      Alias for :meth:`~Tensor.dim()`\n",
      " |\n",
      " |  output_nr\n",
      " |\n",
      " |  real\n",
      " |      Returns a new tensor containing real values of the :attr:`self` tensor for a complex-valued input tensor.\n",
      " |      The returned tensor and :attr:`self` share the same underlying storage.\n",
      " |\n",
      " |      Returns :attr:`self` if :attr:`self` is a real-valued tensor tensor.\n",
      " |\n",
      " |      Example::\n",
      " |          >>> x=torch.randn(4, dtype=torch.cfloat)\n",
      " |          >>> x\n",
      " |          tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n",
      " |          >>> x.real\n",
      " |          tensor([ 0.3100, -0.5445, -1.6492, -0.0638])\n",
      " |\n",
      " |  requires_grad\n",
      " |      Is ``True`` if gradients need to be computed for this Tensor, ``False`` otherwise.\n",
      " |\n",
      " |      .. note::\n",
      " |\n",
      " |          The fact that gradients need to be computed for a Tensor do not mean that the :attr:`grad`\n",
      " |          attribute will be populated, see :attr:`is_leaf` for more details.\n",
      " |\n",
      " |  retains_grad\n",
      " |      Is ``True`` if this Tensor is non-leaf and its :attr:`grad` is enabled to be\n",
      " |      populated during :func:`backward`, ``False`` otherwise.\n",
      " |\n",
      " |  shape\n",
      " |      shape() -> torch.Size\n",
      " |\n",
      " |      Returns the size of the :attr:`self` tensor. Alias for :attr:`size`.\n",
      " |\n",
      " |      See also :meth:`Tensor.size`.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> t = torch.empty(3, 4, 5)\n",
      " |          >>> t.size()\n",
      " |          torch.Size([3, 4, 5])\n",
      " |          >>> t.shape\n",
      " |          torch.Size([3, 4, 5])\n",
      " |\n",
      " |  volatile\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(param_store[\"AutoDiagonalNormal.scale\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc91586f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyro.params.param_store.ParamStoreDict at 0x27ecfea9b80>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1651c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loc (works directly)\n",
    "#with torch.no_grad():\n",
    "#    param_store[\"AutoDiagonalNormal.loc\"].data.copy_(modified_loc_tensor)\n",
    "\n",
    "# For scale (need to work with unconstrained parameter)\n",
    "with torch.no_grad():\n",
    "    # Get the unconstrained parameter directly\n",
    "    scale_param = pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")\n",
    "    \n",
    "    # If you want to set the constrained value to X, you need:\n",
    "    # unconstrained_value = inverse_softplus(X) = log(exp(X) - 1)\n",
    "    desired_constrained_value = 10.0\n",
    "    unconstrained_value = torch.log(torch.exp(torch.tensor(desired_constrained_value, device=scale_param.device)) - 1)\n",
    "    \n",
    "    # Now modify the unconstrained parameter\n",
    "    scale_param.data[0] = unconstrained_value\n",
    "\n",
    "# Or if you want to modify the unconstrained value directly:\n",
    "#with torch.no_grad():\n",
    "#    scale_param = pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")\n",
    "#    scale_param.data[0] = your_unconstrained_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9ae6278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.0000,  0.0385,  0.0440,  ...,  7.7091,  6.1614,  6.6950],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4e2d6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0454, 0.0385, 0.0440,  ..., 7.7091, 6.1614, 6.6950], device='cuda:0',\n",
       "       grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a566fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57f1403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of storing a reference\n",
    "scale_param = pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")\n",
    "\n",
    "# Always access directly\n",
    "with torch.no_grad():\n",
    "    current_scale = pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")\n",
    "    current_scale.data[0] = 10\n",
    "\n",
    "# Or if you must store a reference, make sure to update it after any param store changes\n",
    "def update_scale_reference():\n",
    "    global scale_param\n",
    "    scale_param = pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "007ca9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_scale_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c08e788a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same object? False\n",
      "Same values? True\n"
     ]
    }
   ],
   "source": [
    "scale_param = pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")\n",
    "current_param = pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")\n",
    "\n",
    "print(\"Same object?\", scale_param is current_param)\n",
    "print(\"Same values?\", torch.equal(scale_param, current_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7d96564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_item(param_store, location_index, new_value):\n",
    "    pyro.get_param_store()[param_store][location_index] = new_value\n",
    "\n",
    "    return pyro.get_param_store()[param_store]\n",
    "\n",
    "with torch.no_grad():\n",
    "    pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\").data.copy_(change_item(\"AutoDiagonalNormal.scale\", 0, 88))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d69e03de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0454, 0.0385, 0.0440,  ..., 7.7091, 6.1614, 6.6950], device='cuda:0',\n",
       "       grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ea83126",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pyro.get_param_store().get_param(\"AutoDiagonalNormal.loc\").data.copy_(change_item(\"AutoDiagonalNormal.loc\", 0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17a550cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([10.0000, -2.4763, -1.0711,  ..., -2.4452,  4.6454,  1.5156],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.get_param_store().get_param(\"AutoDiagonalNormal.loc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58e07e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: tensor([0.0454, 0.0385, 0.0440,  ..., 7.7091, 6.1614, 6.6950], device='cuda:0',\n",
      "       grad_fn=<CloneBackward0>)\n",
      "After: tensor([0.9000, 0.0385, 0.0440,  ..., 7.7091, 6.1614, 6.6950], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n"
     ]
    }
   ],
   "source": [
    "import pyro\n",
    "\n",
    "# Access the scale parameter\n",
    "scale_param = pyro.get_param_store()[\"AutoDiagonalNormal.scale\"]\n",
    "\n",
    "# Print value before\n",
    "print(\"Before:\", scale_param.clone())  # clone to avoid referencing\n",
    "\n",
    "# Modify it (example)\n",
    "with torch.no_grad():\n",
    "    scale_param[0] = 0.9\n",
    "\n",
    "# Print value after\n",
    "print(\"After:\", pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\").data.copy_(scale_param))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "149ab14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0454, 0.0385, 0.0440,  ..., 7.7091, 6.1614, 6.6950], device='cuda:0',\n",
       "       grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.get_param_store()[\"AutoDiagonalNormal.scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a18ceb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0454, 0.0385, 0.0440,  ..., 7.7091, 6.1614, 6.6950], device='cuda:0',\n",
       "       grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b428893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import torch\n",
    "\n",
    "# Get the parameter\n",
    "param = pyro.get_param_store().get_param(\"AutoDiagonalNormal.loc\")\n",
    "\n",
    "# Modify it safely by creating a new tensor\n",
    "new_param = param.clone()\n",
    "new_param[0] = 10  # Your new value\n",
    "\n",
    "# Update the parameter store\n",
    "pyro.get_param_store().replace_param(\"AutoDiagonalNormal.loc\", new_param, param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6c5a123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.0000, -2.4763, -1.0711,  ..., -2.4452,  4.6454,  1.5156],\n",
       "       device='cuda:0', grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.get_param_store().get_param(\"AutoDiagonalNormal.loc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfca4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def change_first_item(param_store, new_value):\n",
    "#    pyro.get_param_store()[param_store][0] = new_value\n",
    "#\n",
    "#    return pyro.get_param_store()[param_store]\n",
    "\n",
    "# Change the first item with no gradient tracking\n",
    "#with torch.no_grad():\n",
    "#    param_store[\"AutoDiagonalNormal.loc\"].data.copy_(change_first_item(\"AutoDiagonalNormal.loc\", 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3501075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(param_store.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4740dcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(param_store.keys()))\n",
    "#print(param_store[\"AutoDiagonalNormal.scale\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "96f74c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_item(param_store, location_index, new_value):\n",
    "    pyro.get_param_store()[param_store][location_index] = new_value\n",
    "\n",
    "    return pyro.get_param_store()[param_store]\n",
    "\n",
    "def run_seu_autodiagonal_normal(location_index: int, bit_i: int, parameter_name: str=\"loc\"):\n",
    "    \"\"\"Perform a bitflip at index i across every variable in the AutoDiagonalNormal guide\"\"\"\n",
    "\n",
    "    assert bit_i in range(0, 33)\n",
    "    assert parameter_name in [\"loc\", \"scale\"]\n",
    "    assert location_index in range(0, len(pyro.get_param_store()[f\"AutoDiagonalNormal.{parameter_name}\"]))\n",
    "\n",
    "    if parameter_name == \"loc\":\n",
    "        param_store_name = \"AutoDiagonalNormal.loc\"\n",
    "    elif parameter_name == \"scale\":\n",
    "        param_store_name = \"AutoDiagonalNormal.scale\"\n",
    "\n",
    "    bayesian_model.to(device)\n",
    "    bayesian_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        param_dict = {}\n",
    "\n",
    "        for name, value in pyro.get_param_store().items():\n",
    "            #print(f\"{name}: {value.shape}\")\n",
    "            #print(value)\n",
    "            param_dict[name] = value.cpu().detach().numpy()\n",
    "\n",
    "        tensor_cpu = param_dict[param_store_name]\n",
    "\n",
    "        original_val = tensor_cpu[0]\n",
    "        seu_val = bitflip_float32(original_val, bit_i)\n",
    "\n",
    "        #pyro.get_param_store()[param_store][0] = seu_val\n",
    "\n",
    "        print(f\"Original value: {original_val}, SEU value: {seu_val}\")\n",
    "        #pyro.get_param_store()[\"AutoDiagonalNormal.loc\"]\n",
    "\n",
    "        #if param_store_name == \"AutoDiagonalNormal.loc\":\n",
    "        #    param_store[param_store_name].data.copy_(change_item(param_store_name, location_index, seu_val))\n",
    "        #elif param_store_name == \"AutoDiagonalNormal.scale\":\n",
    "        #    param_store[param_store_name].data.copy_(change_item(param_store_name, location_index, seu_val))\n",
    "        #print(f\"Changed {param_store_name} at index {location_index} to {seu_val}\")\n",
    "\n",
    "        # Get the parameter\n",
    "        param = pyro.get_param_store().get_param(param_store_name)\n",
    "\n",
    "        # Modify it safely by creating a new tensor\n",
    "        new_param = param.clone()\n",
    "        new_param[location_index] = seu_val  # Your new value\n",
    "\n",
    "        # Update the parameter store\n",
    "        if parameter_name == \"loc\":\n",
    "            pyro.get_param_store().replace_param(param_store_name, new_param, param)\n",
    "        elif parameter_name == \"scale\":\n",
    "            pyro.get_param_store().__setitem__(param_store_name, new_param)\n",
    "        #pyro.get_param_store()[param_store_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4bb36bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0454, 0.0385, 0.0440,  ..., 7.7091, 6.1614, 6.6950], device='cuda:0',\n",
       "       grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_store[\"AutoDiagonalNormal.scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ec416c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 3.1483, -2.4763, -1.0711,  ..., -2.4452,  4.6454,  1.5156],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_store[\"AutoDiagonalNormal.loc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec0c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original value: 3.148340940475464, SEU value: 5.807663958573292e+19\n"
     ]
    }
   ],
   "source": [
    "#run_seu_autodiagonal_normal(location_index= 0, bit_i=2, parameter_name=\"loc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a2f3da53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 3.1483, -2.4763, -1.0711,  ..., -2.4452,  4.6454,  1.5156],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_store[\"AutoDiagonalNormal.loc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "859bc798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0454, 0.0385, 0.0440,  ..., 7.7091, 6.1614, 6.6950], device='cuda:0',\n",
       "       grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_store[\"AutoDiagonalNormal.scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "28365b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original value: 0.04540996998548508, SEU value: 1.5452212068469636e+37\n"
     ]
    }
   ],
   "source": [
    "run_seu_autodiagonal_normal(location_index= 0, bit_i=1, parameter_name=\"scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fb14cf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5452e+37, 3.8508e-02, 4.3955e-02,  ..., 7.7091e+00, 6.1614e+00,\n",
       "        6.6950e+00], device='cuda:0', grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_store[\"AutoDiagonalNormal.scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f2ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a418d7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0736abbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the unconstrained scale\n",
    "scale_param = pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")\n",
    "\n",
    "# Change scale[0] to 10.0\n",
    "with torch.no_grad():\n",
    "    scale_param.data[0] = torch.log(torch.exp(torch.tensor(10.0, device=scale_param.device)) - 1)\n",
    "\n",
    "# Confirm the change\n",
    "print(\"New scale[0]:\", F.softplus(scale_param)[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73af65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39695580",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afe90c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b2548",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    scale_param = pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")\n",
    "    # Remember that scale is stored in unconstrained form (inverse softplus)\n",
    "    scale_param.data[0] = torch.log(torch.exp(torch.tensor(10.0, device=scale_param.device)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73642c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76efbdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3798aeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_store[\"AutoDiagonalNormal.scale\"].data.copy_(scale_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df6c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9283c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original constrained value: {F.softplus(pyro.get_param_store()[\"AutoDiagonalNormal.scale\"][0]).item()}\")\n",
    "print(f\"SEU constrained value: {F.softplus(torch.tensor(bitflip_float32(pyro.get_param_store()[\"AutoDiagonalNormal.scale\"].cpu().detach().numpy(), \n",
    "                                                                        0))[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b297dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softplus(torch.tensor(10.0, device=scale_param.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f10bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store()[\"AutoDiagonalNormal.scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31887ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store()[\"AutoDiagonalNormal.scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5292522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with torch.no_grad():\n",
    "#    guide._scale_unconstrained.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad4130",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store()[\"AutoDiagonalNormal.scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31bcbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_store = pyro.get_param_store()\n",
    "run_seu_autodiagonal_normal(location_index= 0, bit_i=2, parameter_name=\"loc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55384d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store()[\"AutoDiagonalNormal.scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ba0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "guide.loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f02921",
   "metadata": {},
   "outputs": [],
   "source": [
    "guide.scale[0] = F.softplus(scale_param)[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "guide.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84261c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fba4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softplus(scale_param)[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f0e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\n",
    "def run_seu_autodiagonal_normal(location_index: int, bit_i: int, parameter_name: str=\"loc\"):\n",
    "    \"\"\"Perform a bitflip at index i across every variable in the AutoDiagonalNormal guide\"\"\"\n",
    "\n",
    "    assert bit_i in range(0, 33)\n",
    "    assert parameter_name in [\"loc\", \"scale\"]\n",
    "    assert location_index in range(0, len(pyro.get_param_store()[f\"AutoDiagonalNormal.{parameter_name}\"]))\n",
    "\n",
    "    if parameter_name == \"loc\":\n",
    "        param_store_name = \"AutoDiagonalNormal.loc\"\n",
    "    elif parameter_name == \"scale\":\n",
    "        param_store_name = \"AutoDiagonalNormal.scale\"\n",
    "\n",
    "    bayesian_model.to(device)\n",
    "    bayesian_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get the unconstrained parameter directly\n",
    "        param_tensor = pyro.get_param_store()[param_store_name]\n",
    "        \n",
    "        # Get the original unconstrained value\n",
    "        original_val = param_tensor[location_index].cpu().detach().numpy()\n",
    "        print(f\"Original unconstrained value: {original_val}\")\n",
    "        seu_val = bitflip_float32(original_val, bit_i)\n",
    "\n",
    "        print(f\"Original unconstrained value: {original_val}, SEU unconstrained value: {seu_val}\")\n",
    "        \n",
    "        if parameter_name == \"scale\":\n",
    "            print(f\"Original constrained value: {F.softplus(param_tensor[location_index]).item()}\")\n",
    "            print(f\"SEU constrained value: {F.softplus(torch.tensor(seu_val)).item()}\")\n",
    "            #guide()\n",
    "\n",
    "        # Set the unconstrained value directly\n",
    "        param_tensor[location_index] = torch.tensor(seu_val, device=param_tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3f57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_item(param_store, location_index, new_value):\n",
    "    pyro.get_param_store()[param_store][location_index] = new_value\n",
    "\n",
    "    return pyro.get_param_store()[param_store]\n",
    "\n",
    "def run_seu_autodiagonal_normal(location_index: int, bit_i: int, parameter_name: str=\"loc\"):\n",
    "    \"\"\"Perform a bitflip at index i across every variable in the AutoDiagonalNormal guide\"\"\"\n",
    "\n",
    "    assert bit_i in range(0, 33)\n",
    "    assert parameter_name in [\"loc\", \"scale\"]\n",
    "    assert location_index in range(0, len(pyro.get_param_store()[f\"AutoDiagonalNormal.{parameter_name}\"]))\n",
    "\n",
    "    if parameter_name == \"loc\":\n",
    "        param_store_name = \"AutoDiagonalNormal.loc\"\n",
    "    elif parameter_name == \"scale\":\n",
    "        param_store_name = \"AutoDiagonalNormal.scale\"\n",
    "\n",
    "    bayesian_model.to(device)\n",
    "    bayesian_model.eval()\n",
    "\n",
    "    if parameter_name == \"loc\":\n",
    "        with torch.no_grad():\n",
    "            param_dict = {}\n",
    "\n",
    "            for name, value in pyro.get_param_store().items():\n",
    "                #print(f\"{name}: {value.shape}\")\n",
    "                #print(value)\n",
    "                param_dict[name] = value.cpu().detach().numpy()\n",
    "\n",
    "            tensor_cpu = param_dict[param_store_name]\n",
    "\n",
    "            original_val = tensor_cpu[0]\n",
    "            seu_val = bitflip_float32(original_val, bit_i)\n",
    "\n",
    "            #pyro.get_param_store()[param_store][0] = seu_val\n",
    "\n",
    "            print(f\"Original value: {original_val}, SEU value: {seu_val}\")\n",
    "            #pyro.get_param_store()[\"AutoDiagonalNormal.loc\"]\n",
    "\n",
    "            if parameter_name == \"scale\":\n",
    "                param_tensor = pyro.get_param_store()[param_store_name]\n",
    "                print(f\"Original constrained value: {F.softplus(param_tensor[location_index]).item()}\")\n",
    "                print(f\"SEU constrained value: {F.softplus(torch.tensor(seu_val)).item()}\")\n",
    "\n",
    "            if param_store_name == \"AutoDiagonalNormal.loc\":\n",
    "                param_store[param_store_name].data.copy_(change_item(param_store_name, location_index, seu_val))\n",
    "            elif param_store_name == \"AutoDiagonalNormal.scale\":\n",
    "                param_store[param_store_name].data.copy_(change_item(param_store_name, location_index, seu_val))\n",
    "\n",
    "    elif parameter_name == \"scale\":\n",
    "        with torch.no_grad():\n",
    "            scale_param = pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")\n",
    "            param_dict = {}\n",
    "\n",
    "            for name, value in pyro.get_param_store().items():\n",
    "                #print(f\"{name}: {value.shape}\")\n",
    "                #print(value)\n",
    "                param_dict[name] = value.cpu().detach().numpy()\n",
    "\n",
    "            tensor_cpu = param_dict[param_store_name]\n",
    "\n",
    "            original_val = tensor_cpu[0]\n",
    "            seu_val = bitflip_float32(original_val, bit_i)\n",
    "            print(f\"Original value: {original_val}, SEU value: {seu_val}\")\n",
    "            scale_param.data.copy_(change_item(param_store_name, location_index, seu_val))\n",
    "        #pyro.get_param_store()[param_store_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#override first item of guide.scale, focus on changing the item directly in the param store, without using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e842b2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06045425",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store().get_param(\"AutoDiagonalNormal.loc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133e497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_seu_autodiagonal_normal(location_index= 0, bit_i=1, parameter_name=\"loc\")\n",
    "run_seu_autodiagonal_normal(location_index= 0, bit_i=1, parameter_name=\"loc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e49fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale_param = pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")\n",
    "#scale_param[0] += 10.0  # In-place modification\n",
    "\n",
    "# Now run the guide — it will use the modified scale\n",
    "#guide = AutoDiagonalNormal(bayesian_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad33f3",
   "metadata": {},
   "source": [
    "## After Bitflip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "bb9f4b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights changed: True\n"
     ]
    }
   ],
   "source": [
    "changed = not torch.equal(pyro.get_param_store()[\"AutoDiagonalNormal.scale\"], #AFTER \n",
    "                          original_param_store[\"AutoDiagonalNormal.scale\"], #BEFORE\n",
    "                          )\n",
    "print(\"Weights changed:\", changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a79f79bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:09<00:00, 11.08it/s]\n"
     ]
    }
   ],
   "source": [
    "after_all_labels, after_all_predictions, after_all_logits, after_all_probs = predict_data_probs(bayesian_model, test_loader, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1c0f0448",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_cm = confusion_matrix(after_all_labels, after_all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d06cfa09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from confusion matrix: 11.111111%\n"
     ]
    }
   ],
   "source": [
    "after_accuracy = np.trace(after_cm) / np.sum(after_cm)\n",
    "print(f\"Accuracy from confusion matrix: {after_accuracy * 100:.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0c7129f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy difference: -63.796296%\n"
     ]
    }
   ],
   "source": [
    "#print the difference in accuracy\n",
    "print(f\"Accuracy difference: {(after_accuracy - accuracy)*100:.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dc6635",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store().get_param(\"AutoDiagonalNormal.loc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6440c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store().get_param(\"AutoDiagonalNormal.scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0840e33",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44b13e40",
   "metadata": {},
   "source": [
    "## END OF Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f06103",
   "metadata": {},
   "outputs": [],
   "source": [
    "mendingdiemdeh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27109e6d",
   "metadata": {},
   "source": [
    "## Real Bitflip Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2163c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store()[\"AutoDiagonalNormal.loc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c12088b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822bc70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b33635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean pyro param store\n",
    "model_path = 'results_eurosat/bayesian_cnn_model_std10_100_epoch.pth'\n",
    "guide_path = 'results_eurosat/bayesian_cnn_guide_std10_100_epoch_guide.pth'\n",
    "pyro_param_store_path = 'results_eurosat/pyro_param_store_std10_100_epoch.pkl'\n",
    "\n",
    "guide = AutoDiagonalNormal(bayesian_model).to(device)\n",
    "pyro.get_param_store().set_state(torch.load(pyro_param_store_path,weights_only=False))\n",
    "\n",
    "original_param_store = {}\n",
    "\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print(f\"{name}: {value.shape}\")\n",
    "    original_param_store[name] = torch.tensor(value.data, requires_grad=value.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b343f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ab1697",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store()[\"AutoDiagonalNormal.loc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_seu_autodiagonal_normal(location_index= 0, bit_i=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store()[\"AutoDiagonalNormal.loc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9986a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42583770",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_shapes = {\n",
    "    \"conv1.weight\": (32, 3, 5, 5),\n",
    "    \"conv1.bias\": (32,),\n",
    "    \"conv2.weight\": (64, 32, 5, 5),\n",
    "    \"conv2.bias\": (64,),\n",
    "    \"fc1.weight\": (num_classes, 64 * 16 * 16),\n",
    "    \"fc1.bias\": (num_classes,)\n",
    "}\n",
    "\n",
    "def _unpack(vector):\n",
    "    \"\"\"Unpacks flat vector into a dict of shaped tensors\"\"\"\n",
    "    params = {}\n",
    "    offset = 0\n",
    "    for name, shape in param_shapes.items():\n",
    "        size = torch.tensor(shape).prod().item()\n",
    "        flat_param = vector[offset:offset + size]\n",
    "        params[name] = flat_param.view(shape)\n",
    "        offset += size\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14f2bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# access the value of generator pyro.get_param_store().items()\n",
    "#pyro_params = pyro.get_param_store().items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb92dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict = {}\n",
    "\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    #print(f\"{name}: {value.shape}\")\n",
    "    #print(value)\n",
    "    weight_dict[name] = value.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a3c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a44608",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_shapes = {\n",
    "    \"conv1.weight\": (32, 3, 5, 5),\n",
    "    \"conv1.bias\": (32,),\n",
    "    \"conv2.weight\": (64, 32, 5, 5),\n",
    "    \"conv2.bias\": (64,),\n",
    "    \"fc1.weight\": (num_classes, 64 * 16 * 16),\n",
    "    \"fc1.bias\": (num_classes,)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf70882",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_params_loc = {}\n",
    "offset_loc = 0\n",
    "\n",
    "for name, shape in param_shapes.items():\n",
    "    size = torch.tensor(shape).prod().item()\n",
    "    flat_param = weight_dict[\"AutoDiagonalNormal.loc\"][offset_loc:offset_loc + size]\n",
    "    unpacked_params_loc[name] = torch.tensor(flat_param).view(shape)\n",
    "    offset_loc += size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27416e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_params_scale = {}\n",
    "offset_scale = 0\n",
    "\n",
    "for name, shape in param_shapes.items():\n",
    "    size = torch.tensor(shape).prod().item()\n",
    "    flat_param = weight_dict[\"AutoDiagonalNormal.scale\"][offset_scale:offset_scale + size]\n",
    "    unpacked_params_scale[name] = torch.tensor(flat_param).view(shape)\n",
    "    offset_scale += size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cab4b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([param.view(-1) for param in unpacked_params_loc.values()]).shape\n",
    "torch.cat([param.view(-1) for param in unpacked_params_scale.values()]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a3893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the first item of unpacked_params_weights[\"conv1.weight\"][0, 0, 0, 0]\n",
    "unpacked_params_loc[\"conv1.weight\"][0, 0, 0, 0] = 100.0\n",
    "print(unpacked_params_loc[\"conv1.weight\"][0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53ac99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return unpacked_params back to the param store, into a flattened form\n",
    "# put unpacked_params_weights to pyro param store autodiagonalnormal.loc\n",
    "# put unpacked_params_bias to pyro param store autodiagonalnormal.scale\n",
    "unpacked_params_loc_flat = torch.cat([param.view(-1) for param in unpacked_params_loc.values()])\n",
    "unpacked_params_scale_flat = torch.cat([param.view(-1) for param in unpacked_params_scale.values()])\n",
    "\n",
    "#for name, param in unpacked_params_loc.items():\n",
    "#    pyro.get_param_store().set_param(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d836c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_params_scale_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79f1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(f\"{name}: {value.shape}\")\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d929fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_store = pyro.get_param_store()\n",
    "weight_loc = param_store[\"AutoDiagonalNormal.loc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c8455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bitflip_tensor(tensor, bit_index=0):\n",
    "    flat_tensor = tensor.flatten()\n",
    "    raw_bytes = flat_tensor.view(torch.uint8)\n",
    "\n",
    "    # Choose a random byte and bit to flip\n",
    "    byte_idx = torch.randint(0, raw_bytes.numel(), (1,)).item()\n",
    "    mask = 1 << bit_index\n",
    "    raw_bytes[byte_idx] ^= mask  # Flip bit\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a553aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_bit_at(tensor, index, bit_position=0):\n",
    "    val = tensor.view(torch.uint8)\n",
    "    val[index] ^= (1 << bit_position)\n",
    "    return val.view(tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949320e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean pyro param store\n",
    "model_path = 'results_eurosat/bayesian_cnn_model_std10_100_epoch.pth'\n",
    "guide_path = 'results_eurosat/bayesian_cnn_guide_std10_100_epoch_guide.pth'\n",
    "pyro_param_store_path = 'results_eurosat/pyro_param_store_std10_100_epoch.pkl'\n",
    "\n",
    "guide = AutoDiagonalNormal(bayesian_model).to(device)\n",
    "#guide.load_state_dict(torch.load(guide_path))\n",
    "\n",
    "pyro.get_param_store().set_state(torch.load(pyro_param_store_path,weights_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3235d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip bit at index 0, bit position 0 (least significant bit)\n",
    "original_tensor = param_store[\"AutoDiagonalNormal.loc\"]\n",
    "flipped_tensor = flip_bit_at(original_tensor, index=0, bit_position=7)\n",
    "\n",
    "# Update the parameter store with the modified tensor\n",
    "param_store[\"AutoDiagonalNormal.loc\"].data.copy_(flipped_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579fad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store()[\"AutoDiagonalNormal.loc\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19eb5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_store[\"AutoDiagonalNormal.loc\"].data.copy_(flip_bit_at(param_store[\"AutoDiagonalNormal.loc\"], 30, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b777311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_store[\"AutoDiagonalNormal.loc\"].data.copy_(bitflip_tensor(param_store[\"AutoDiagonalNormal.loc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b02dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_bit_in_tensor(tensor, bit_position=1, flip_count=1):\n",
    "    flat = tensor.view(-1)\n",
    "    idx = torch.randint(0, flat.numel(), (flip_count,))\n",
    "    for i in idx:\n",
    "        val = flat[i].item()\n",
    "        int_val = np.frombuffer(np.float32(val).tobytes(), dtype=np.uint32)[0]\n",
    "        flipped = int_val ^ (1 << bit_position)\n",
    "        flipped_val = np.frombuffer(np.uint32(flipped).tobytes(), dtype=np.float32)[0]\n",
    "        flat[i] = torch.tensor(flipped_val)\n",
    "    return tensor\n",
    "\n",
    "def inject_seu_conv_layer(layer, bit_position=10, flip_count=1):\n",
    "    with torch.no_grad():\n",
    "        layer.weight.data = flip_bit_in_tensor(layer.weight.data.clone(), bit_position, flip_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c3e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    flip_bit_in_tensor(pyro.get_param_store()[\"AutoDiagonalNormal.loc\"], bit_position=3, flip_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03011fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store()[\"AutoDiagonalNormal.loc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59a99cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a894c65c",
   "metadata": {},
   "source": [
    "## BEFORE BITFLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653ec307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a873ad1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4894b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6114ce02",
   "metadata": {},
   "source": [
    "## BITFLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de521308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with torch.no_grad():\n",
    "#    flip_bit_in_tensor(pyro.get_param_store()[\"AutoDiagonalNormal.loc\"], bit_position=3, flip_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2933ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2122f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the first item of pyro.get_param_store()[\"AutoDiagonalNormal.loc\"] to 100\n",
    "# pyro.get_param_store()[\"AutoDiagonalNormal.loc\"][0] = 100.0\n",
    "\n",
    "def change_first_item(param_store, new_value):\n",
    "    pyro.get_param_store()[param_store][0] = new_value\n",
    "\n",
    "    return pyro.get_param_store()[param_store]\n",
    "\n",
    "# Change the first item with no gradient tracking\n",
    "with torch.no_grad():\n",
    "    param_store[\"AutoDiagonalNormal.loc\"].data.copy_(change_first_item(\"AutoDiagonalNormal.loc\", 100.0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37911b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store()[\"AutoDiagonalNormal.loc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5789a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_param_store[\"AutoDiagonalNormal.loc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4abec3",
   "metadata": {},
   "source": [
    "## AFTER BITFLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "changed = not torch.equal(pyro.get_param_store()[\"AutoDiagonalNormal.loc\"], #AFTER \n",
    "                          original_param_store[\"AutoDiagonalNormal.loc\"], #BEFORE\n",
    "                          )\n",
    "print(\"Weights changed:\", changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad94c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed = not torch.equal(model_cnn.conv1.weight.data, original_weights)\n",
    "#print(\"Weights changed:\", changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a04ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate roc auc score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calculate_roc_auc_score(y_true, y_pred, num_classes):\n",
    "    # Convert to one-hot encoding\n",
    "    y_true_one_hot = np.eye(num_classes)[y_true]\n",
    "    y_pred_one_hot = np.eye(num_classes)[y_pred]\n",
    "    \n",
    "    # Calculate ROC AUC for each class\n",
    "    roc_auc_scores = []\n",
    "    for i in range(num_classes):\n",
    "        if np.sum(y_true_one_hot[:, i]) > 0:  # Check if the class is present\n",
    "            roc_auc = roc_auc_score(y_true_one_hot[:, i], y_pred[:, i])\n",
    "            roc_auc_scores.append(roc_auc)\n",
    "        else:\n",
    "            roc_auc_scores.append(np.nan)  # Class not present in the test set\n",
    "\n",
    "    return np.array(roc_auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6f7010",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_scores = calculate_roc_auc_score(all_labels, all_predictions, num_classes)\n",
    "print(f\"ROC AUC scores: {roc_auc_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d08b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "y_true = [0, 1, 2, 2, 1]  # true labels\n",
    "y_score = [\n",
    "    [0.8, 0.1, 0.1],\n",
    "    [0.2, 0.6, 0.2],\n",
    "    [0.1, 0.2, 0.7],\n",
    "    [0.1, 0.3, 0.6],\n",
    "    [0.2, 0.7, 0.1]\n",
    "]  # predicted probabilities from model\n",
    "\n",
    "y_true = np.array(all_labels)\n",
    "y_score = np.array(all_probs)\n",
    "\n",
    "# Binarize labels\n",
    "n_classes = y_score.shape[1]\n",
    "y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "\n",
    "# Compute ROC curve and AUC for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot all 10 ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = [\n",
    "    'tab:blue',      # 1\n",
    "    'tab:orange',    # 2\n",
    "    'tab:green',     # 3\n",
    "    'tab:red',       # 4\n",
    "    'tab:purple',    # 5\n",
    "    'tab:brown',     # 6\n",
    "    'tab:pink',      # 7\n",
    "    'tab:gray',      # 8\n",
    "    'tab:olive',     # 9\n",
    "    'tab:cyan'       # 10\n",
    "]\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], color=colors[i % len(colors)],\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                   ''.format(i, roc_auc[i]))\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curves for Each Class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3d690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnntest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
