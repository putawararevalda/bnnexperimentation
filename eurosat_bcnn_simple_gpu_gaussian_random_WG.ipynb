{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b1af2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import torch\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5965d3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Revalda Putawara\\.conda\\envs\\bnntest\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f9ba9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "853a93c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0d753eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import BayesianCNNSingleFCCustomWG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_load_data(batch_size=54):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.3444, 0.3809, 0.4082], std=[0.1809, 0.1331, 0.1137])\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.EuroSAT(root='./data', transform=transform, download=False)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    with open('datasplit/split_indices.pkl', 'rb') as f:\n",
    "        split = pickle.load(f)\n",
    "        train_dataset = Subset(dataset, split['train'])\n",
    "        test_dataset = Subset(dataset, split['test'])\n",
    "\n",
    "    # Add num_workers and pin_memory for faster data loading\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                             num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                            num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2af6fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=54):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.3444, 0.3809, 0.4082], std=[0.1809, 0.1331, 0.1137])\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.EuroSAT(root='./data', transform=transform, download=False)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    #with open('datasplit/split_indices.pkl', 'rb') as f:\n",
    "    #    split = pickle.load(f)\n",
    "    #    train_dataset = Subset(dataset, split['train'])\n",
    "    #    test_dataset = Subset(dataset, split['test'])\n",
    "\n",
    "    # Add num_workers and pin_memory for faster data loading\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                             num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                            num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6bc1293",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "bayesian_model = BayesianCNNSingleFCCustomWG(num_classes=num_classes, \n",
    "                                             mu=0,\n",
    "                                             sigma=10.,\n",
    "                                             device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba2346c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianCNNSingleFCCustomWG(\n",
       "  (conv1): PyroConv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): PyroConv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): PyroLinear(in_features=16384, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3553741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "#from pyro.infer.autoguide import AutoLowRankMultivariateNormal\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1420314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "guide = AutoDiagonalNormal(bayesian_model)\n",
    "#guide = AutoLowRankMultivariateNormal(bayesian_model, rank=10)\n",
    "\n",
    "# 2. Optimizer and SVI - increase learning rate for better convergence\n",
    "optimizer = Adam({\"lr\": 1e-3})  # Increased from 1e-4 to 1e-3\n",
    "svi = pyro.infer.SVI(model=bayesian_model,\n",
    "                     guide=guide,\n",
    "                     optim=optimizer,\n",
    "                     loss=pyro.infer.Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a372470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "439efc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svi(model, guide, svi, train_loader, num_epochs=10):\n",
    "    # Clear parameter store only ONCE at the beginning\n",
    "    pyro.clear_param_store()\n",
    "    model.train()\n",
    "    \n",
    "    # Ensure model is on the correct device\n",
    "    model.to(device)\n",
    "    #guide.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            loss = svi.step(images, labels)\n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "            \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1} - ELBO Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9addc81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svi(model, guide, svi, train_loader, num_epochs=10):\n",
    "    # Clear parameter store only ONCE at the beginning\n",
    "    pyro.clear_param_store()\n",
    "    model.train()\n",
    "    \n",
    "    # Ensure model is on the correct device\n",
    "    model.to(device)\n",
    "    #guide.to(device)\n",
    "    \n",
    "    # Lists to store losses and accuracies\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    accuracy_epochs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            loss = svi.step(images, labels)\n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "            \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        epoch_losses.append(avg_loss)\n",
    "        \n",
    "        # Calculate accuracy every 10 epochs (and on the first and last epoch)\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or epoch == num_epochs - 1:\n",
    "            model.eval()\n",
    "            guide.eval()\n",
    "            \n",
    "            correct_predictions = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in tqdm(train_loader, desc=f\"Calculating accuracy for epoch {epoch+1}\"):\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    \n",
    "                    # Sample from the guide to get model parameters\n",
    "                    guide_trace = pyro.poutine.trace(guide).get_trace(images)\n",
    "                    replayed_model = pyro.poutine.replay(model, trace=guide_trace)\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    logits = replayed_model(images)\n",
    "                    predictions = torch.argmax(logits, dim=1)\n",
    "                    \n",
    "                    # Count correct predictions\n",
    "                    correct_predictions += (predictions == labels).sum().item()\n",
    "                    total_samples += labels.size(0)\n",
    "            \n",
    "            epoch_accuracy = correct_predictions / total_samples\n",
    "            epoch_accuracies.append(epoch_accuracy)\n",
    "            accuracy_epochs.append(epoch + 1)\n",
    "            \n",
    "            model.train()  # Set back to training mode\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} - ELBO Loss: {avg_loss:.4f}, Train Accuracy: {epoch_accuracy*100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1} - ELBO Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return epoch_losses, epoch_accuracies, accuracy_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8dcfd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svi_with_annealing(model, guide, svi, train_loader, num_epochs=10):\n",
    "    pyro.clear_param_store()\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    accuracy_epochs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # KL annealing - gradually increase KL weight\n",
    "        kl_weight = min(1.0, (epoch + 1) / (num_epochs * 0.5))  # Reach full weight at 50% of training\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Use weighted ELBO\n",
    "            def weighted_model(images, labels):\n",
    "                with pyro.poutine.scale(scale=kl_weight):\n",
    "                    return model(images, labels)\n",
    "            \n",
    "            loss = svi.step(images, labels)\n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "            \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        epoch_losses.append(avg_loss)\n",
    "\n",
    "        # Calculate accuracy every 10 epochs (and on the first and last epoch)\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or epoch == num_epochs - 1:\n",
    "            model.eval()\n",
    "            guide.eval()\n",
    "            \n",
    "            correct_predictions = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in tqdm(train_loader, desc=f\"Calculating accuracy for epoch {epoch+1}\"):\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    \n",
    "                    # Sample from the guide to get model parameters\n",
    "                    guide_trace = pyro.poutine.trace(guide).get_trace(images)\n",
    "                    replayed_model = pyro.poutine.replay(model, trace=guide_trace)\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    logits = replayed_model(images)\n",
    "                    predictions = torch.argmax(logits, dim=1)\n",
    "                    \n",
    "                    # Count correct predictions\n",
    "                    correct_predictions += (predictions == labels).sum().item()\n",
    "                    total_samples += labels.size(0)\n",
    "            \n",
    "            epoch_accuracy = correct_predictions / total_samples\n",
    "            epoch_accuracies.append(epoch_accuracy)\n",
    "            accuracy_epochs.append(epoch + 1)\n",
    "            \n",
    "            model.train()  # Set back to training mode\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} - ELBO Loss: {avg_loss:.4f}, Train Accuracy: {epoch_accuracy*100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1} - ELBO Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return epoch_losses, epoch_accuracies, accuracy_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cba82446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npyro.clear_param_store()\\n\\n# Ensure model and guide are on the correct device\\nbayesian_model.to(device)\\nguide.to(device)\\n\\ntrain_loader, test_loader = load_data(batch_size=54)\\nlosses, accuracies, accuracy_epochs = train_svi(bayesian_model, guide, svi, train_loader, num_epochs=100)\\n\\n# Plot training curves\\nplt.figure(figsize=(12, 4))\\n\\nplt.subplot(1, 2, 1)\\nplt.plot(range(1, len(losses) + 1), losses)\\nplt.title('Training Loss')\\nplt.xlabel('Epoch')\\nplt.ylabel('ELBO Loss')\\nplt.grid(True)\\n\\nplt.subplot(1, 2, 2)\\nplt.plot(accuracy_epochs, accuracies, 'o-')\\nplt.title('Training Accuracy (Every 10 Epochs)')\\nplt.xlabel('Epoch')\\nplt.ylabel('Accuracy')\\nplt.grid(True)\\n\\nplt.tight_layout()\\nplt.show()\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Ensure model and guide are on the correct device\n",
    "bayesian_model.to(device)\n",
    "guide.to(device)\n",
    "\n",
    "train_loader, test_loader = load_data(batch_size=54)\n",
    "losses, accuracies, accuracy_epochs = train_svi(bayesian_model, guide, svi, train_loader, num_epochs=100)\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(losses) + 1), losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('ELBO Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracy_epochs, accuracies, 'o-')\n",
    "plt.title('Training Accuracy (Every 10 Epochs)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0aae2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svi_with_stats(model, guide, svi, train_loader, num_epochs=10):\n",
    "    # Clear parameter store only ONCE at the beginning\n",
    "    pyro.clear_param_store()\n",
    "    model.train()\n",
    "    \n",
    "    # Ensure model is on the correct device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Lists to store losses and accuracies\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    accuracy_epochs = []\n",
    "    \n",
    "    # Lists to store weight and bias statistics\n",
    "    weight_stats = {'epochs': [], 'means': [], 'stds': []}\n",
    "    bias_stats = {'epochs': [], 'means': [], 'stds': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            loss = svi.step(images, labels)\n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "            \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        epoch_losses.append(avg_loss)\n",
    "        \n",
    "        # Calculate accuracy every 10 epochs (and on the first and last epoch)\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or epoch == num_epochs - 1:\n",
    "            model.eval()\n",
    "            guide.eval()\n",
    "            \n",
    "            correct_predictions = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in tqdm(train_loader, desc=f\"Calculating accuracy for epoch {epoch+1}\"):\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    \n",
    "                    # Sample from the guide to get model parameters\n",
    "                    guide_trace = pyro.poutine.trace(guide).get_trace(images)\n",
    "                    replayed_model = pyro.poutine.replay(model, trace=guide_trace)\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    logits = replayed_model(images)\n",
    "                    predictions = torch.argmax(logits, dim=1)\n",
    "                    \n",
    "                    # Count correct predictions\n",
    "                    correct_predictions += (predictions == labels).sum().item()\n",
    "                    total_samples += labels.size(0)\n",
    "            \n",
    "            epoch_accuracy = correct_predictions / total_samples\n",
    "            epoch_accuracies.append(epoch_accuracy)\n",
    "            accuracy_epochs.append(epoch + 1)\n",
    "            \n",
    "            # Record weight and bias statistics\n",
    "            weight_means = []   # loc means\n",
    "            weight_stds = []    # loc stds\n",
    "            bias_means = []     # scale means\n",
    "            bias_stds = []      # scale stds\n",
    "            \n",
    "            for name, param in pyro.get_param_store().items():\n",
    "                if 'AutoDiagonalNormal.loc' in name:\n",
    "                    weight_means.append(param.mean().item())\n",
    "                    weight_stds.append(param.std().item())\n",
    "                elif 'AutoDiagonalNormal.scale' in name:\n",
    "                    bias_means.append(param.mean().item())\n",
    "                    bias_stds.append(param.std().item())\n",
    "            \n",
    "            # Store statistics for this epoch\n",
    "            weight_stats['epochs'].append(epoch + 1)\n",
    "            weight_stats['means'].append(weight_means)\n",
    "            weight_stats['stds'].append(weight_stds)\n",
    "            \n",
    "            bias_stats['epochs'].append(epoch + 1)\n",
    "            bias_stats['means'].append(bias_means)\n",
    "            bias_stats['stds'].append(bias_stds)\n",
    "            \n",
    "            model.train()  # Set back to training mode\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} - ELBO Loss: {avg_loss:.4f}, Train Accuracy: {epoch_accuracy*100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1} - ELBO Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return epoch_losses, epoch_accuracies, accuracy_epochs, weight_stats, bias_stats\n",
    "\n",
    "def plot_training_results_with_stats(losses, accuracies, accuracy_epochs, weight_stats, bias_stats):\n",
    "    \"\"\"Plot training results with weight and bias statistics\"\"\"\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Training Loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, len(losses) + 1), losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('ELBO Loss')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot 2: Training Accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(accuracy_epochs, accuracies, 'o-')\n",
    "    plt.title('Training Accuracy (Every 10 Epochs)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot 3: Weight Statistics Boxplot\n",
    "    plt.subplot(2, 2, 3)\n",
    "    weight_data = []\n",
    "    weight_labels = []\n",
    "    \n",
    "    for i, epoch in enumerate(weight_stats['epochs']):\n",
    "        # Combine means and stds for this epoch\n",
    "        epoch_data = weight_stats['means'][i] + weight_stats['stds'][i]\n",
    "        weight_data.append(epoch_data)\n",
    "        weight_labels.append(f'Epoch {epoch}')\n",
    "    \n",
    "    if weight_data:\n",
    "        bp1 = plt.boxplot(weight_data, labels=weight_labels, patch_artist=True)\n",
    "        for patch in bp1['boxes']:\n",
    "            patch.set_facecolor('lightblue')\n",
    "    \n",
    "    plt.title('LOC Statistics Distribution')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('LOC Values')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Bias Statistics Boxplot\n",
    "    plt.subplot(2, 2, 4)\n",
    "    bias_data = []\n",
    "    bias_labels = []\n",
    "    \n",
    "    for i, epoch in enumerate(bias_stats['epochs']):\n",
    "        # Combine means and stds for this epoch\n",
    "        epoch_data = bias_stats['means'][i] + bias_stats['stds'][i]\n",
    "        bias_data.append(epoch_data)\n",
    "        bias_labels.append(f'Epoch {epoch}')\n",
    "    \n",
    "    if bias_data:\n",
    "        bp2 = plt.boxplot(bias_data, tick_labels=bias_labels, patch_artist=True)\n",
    "        for patch in bp2['boxes']:\n",
    "            patch.set_facecolor('lightcoral')\n",
    "    \n",
    "    plt.title('SCALE Statistics Distribution')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('SCALE Values')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac8333a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 400/400 [00:50<00:00,  7.87it/s]\n",
      "Calculating accuracy for epoch 1: 100%|██████████| 400/400 [00:34<00:00, 11.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - ELBO Loss: 879611.9734, Train Accuracy: 13.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 400/400 [00:22<00:00, 17.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - ELBO Loss: 812692.3976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 400/400 [00:21<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - ELBO Loss: 749191.7969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 400/400 [00:21<00:00, 18.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - ELBO Loss: 690004.2978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 400/400 [00:23<00:00, 17.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - ELBO Loss: 635815.0776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 400/400 [00:23<00:00, 16.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - ELBO Loss: 587200.6685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 400/400 [00:24<00:00, 16.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - ELBO Loss: 543921.8312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 400/400 [00:21<00:00, 18.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - ELBO Loss: 506332.8077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 400/400 [00:22<00:00, 18.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - ELBO Loss: 473820.3193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 400/400 [00:22<00:00, 18.09it/s]\n",
      "Calculating accuracy for epoch 10: 100%|██████████| 400/400 [00:06<00:00, 61.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - ELBO Loss: 445811.8398, Train Accuracy: 24.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 400/400 [00:22<00:00, 18.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - ELBO Loss: 421812.2692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 400/400 [00:22<00:00, 18.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - ELBO Loss: 401259.5880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 400/400 [00:22<00:00, 18.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - ELBO Loss: 382971.3948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 400/400 [00:22<00:00, 18.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - ELBO Loss: 367053.4920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 400/400 [00:21<00:00, 18.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - ELBO Loss: 353202.7687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 400/400 [00:22<00:00, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - ELBO Loss: 340655.5123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 400/400 [00:22<00:00, 18.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - ELBO Loss: 329414.8210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 400/400 [00:22<00:00, 17.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - ELBO Loss: 319294.9046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 400/400 [00:22<00:00, 18.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - ELBO Loss: 309992.8095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████| 400/400 [00:22<00:00, 17.94it/s]\n",
      "Calculating accuracy for epoch 20: 100%|██████████| 400/400 [00:06<00:00, 60.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - ELBO Loss: 301440.4635, Train Accuracy: 25.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 400/400 [00:22<00:00, 17.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - ELBO Loss: 293645.5574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 400/400 [00:22<00:00, 17.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - ELBO Loss: 286665.6185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|██████████| 400/400 [00:22<00:00, 17.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - ELBO Loss: 279836.7715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|██████████| 400/400 [00:22<00:00, 18.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - ELBO Loss: 273539.7267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|██████████| 400/400 [00:22<00:00, 17.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - ELBO Loss: 267899.4401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|██████████| 400/400 [00:22<00:00, 17.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - ELBO Loss: 262482.3923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|██████████| 400/400 [00:19<00:00, 21.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - ELBO Loss: 257062.5763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|██████████| 400/400 [00:17<00:00, 23.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - ELBO Loss: 252127.3713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|██████████| 400/400 [00:18<00:00, 21.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - ELBO Loss: 247196.2026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|██████████| 400/400 [00:20<00:00, 19.95it/s]\n",
      "Calculating accuracy for epoch 30: 100%|██████████| 400/400 [00:06<00:00, 60.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - ELBO Loss: 242945.2948, Train Accuracy: 26.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|██████████| 400/400 [00:16<00:00, 24.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 - ELBO Loss: 238869.8112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|██████████| 400/400 [00:15<00:00, 25.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 - ELBO Loss: 234788.9208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: 100%|██████████| 400/400 [00:15<00:00, 25.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 - ELBO Loss: 231220.6146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: 100%|██████████| 400/400 [00:15<00:00, 25.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 - ELBO Loss: 227414.1276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100: 100%|██████████| 400/400 [00:15<00:00, 25.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 - ELBO Loss: 224019.7665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100: 100%|██████████| 400/400 [00:15<00:00, 25.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 - ELBO Loss: 220327.0863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100: 100%|██████████| 400/400 [00:15<00:00, 25.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 - ELBO Loss: 217067.4683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100: 100%|██████████| 400/400 [00:15<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 - ELBO Loss: 214052.7557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100: 100%|██████████| 400/400 [00:15<00:00, 25.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 - ELBO Loss: 211088.1951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100: 100%|██████████| 400/400 [00:15<00:00, 25.52it/s]\n",
      "Calculating accuracy for epoch 40: 100%|██████████| 400/400 [00:05<00:00, 79.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 - ELBO Loss: 208352.5653, Train Accuracy: 26.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100: 100%|██████████| 400/400 [00:15<00:00, 25.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 - ELBO Loss: 205588.6150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100: 100%|██████████| 400/400 [00:15<00:00, 26.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 - ELBO Loss: 202671.8353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100: 100%|██████████| 400/400 [00:15<00:00, 25.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 - ELBO Loss: 200417.5043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100: 100%|██████████| 400/400 [00:15<00:00, 25.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 - ELBO Loss: 197966.6900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100: 100%|██████████| 400/400 [00:15<00:00, 25.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 - ELBO Loss: 195390.6190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100: 100%|██████████| 400/400 [00:16<00:00, 24.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 - ELBO Loss: 193044.8233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100: 100%|██████████| 400/400 [00:15<00:00, 25.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 - ELBO Loss: 190988.9963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/100: 100%|██████████| 400/400 [00:15<00:00, 25.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 - ELBO Loss: 188440.7155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100: 100%|██████████| 400/400 [00:15<00:00, 25.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 - ELBO Loss: 186171.4094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/100: 100%|██████████| 400/400 [00:15<00:00, 25.23it/s]\n",
      "Calculating accuracy for epoch 50: 100%|██████████| 400/400 [00:04<00:00, 87.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 - ELBO Loss: 184118.6397, Train Accuracy: 27.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/100: 100%|██████████| 400/400 [00:16<00:00, 24.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 - ELBO Loss: 182207.0772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/100: 100%|██████████| 400/400 [00:15<00:00, 26.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 - ELBO Loss: 179877.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/100: 100%|██████████| 400/400 [00:53<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 - ELBO Loss: 178010.0658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/100: 100%|██████████| 400/400 [00:51<00:00,  7.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 - ELBO Loss: 176215.1924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/100:  92%|█████████▏| 369/400 [01:07<00:05,  5.45it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m train_loader, test_loader = load_data(batch_size=\u001b[32m54\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Train with statistics recording\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m losses, accuracies, accuracy_epochs, weight_stats, bias_stats = \u001b[43mtrain_svi_with_stats\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbayesian_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msvi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain_svi_with_stats\u001b[39m\u001b[34m(model, guide, svi, train_loader, num_epochs)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m     22\u001b[39m     images, labels = images.to(device), labels.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     loss = \u001b[43msvi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     epoch_loss += loss\n\u001b[32m     26\u001b[39m     num_batches += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Revalda Putawara\\.conda\\envs\\bnntest\\Lib\\site-packages\\pyro\\infer\\svi.py:145\u001b[39m, in \u001b[36mSVI.step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# get loss and compute gradients\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m poutine.trace(param_only=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m param_capture:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_and_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m params = \u001b[38;5;28mset\u001b[39m(\n\u001b[32m    148\u001b[39m     site[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m].unconstrained() \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture.trace.nodes.values()\n\u001b[32m    149\u001b[39m )\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Revalda Putawara\\.conda\\envs\\bnntest\\Lib\\site-packages\\pyro\\infer\\trace_elbo.py:157\u001b[39m, in \u001b[36mTrace_ELBO.loss_and_grads\u001b[39m\u001b[34m(self, model, guide, *args, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainable_params \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m    154\u001b[39m         surrogate_loss_particle, \u001b[33m\"\u001b[39m\u001b[33mrequires_grad\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    155\u001b[39m     ):\n\u001b[32m    156\u001b[39m         surrogate_loss_particle = surrogate_loss_particle / \u001b[38;5;28mself\u001b[39m.num_particles\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         \u001b[43msurrogate_loss_particle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m warn_if_nan(loss, \u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Revalda Putawara\\.conda\\envs\\bnntest\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Revalda Putawara\\.conda\\envs\\bnntest\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Revalda Putawara\\.conda\\envs\\bnntest\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "# Ensure model and guide are on the correct device\n",
    "bayesian_model.to(device)\n",
    "guide.to(device)\n",
    "\n",
    "train_loader, test_loader = load_data(batch_size=54)\n",
    "\n",
    "# Train with statistics recording\n",
    "losses, accuracies, accuracy_epochs, weight_stats, bias_stats = train_svi_with_stats(\n",
    "    bayesian_model, guide, svi, train_loader, num_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a75a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all results including weight and bias statistics\n",
    "plot_training_results_with_stats(losses, accuracies, accuracy_epochs, weight_stats, bias_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaea70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopdeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f6e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e680bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_means = []\n",
    "weight_stds = []\n",
    "bias_means = []\n",
    "bias_stds = []\n",
    "\n",
    "for name, param in pyro.get_param_store().items():\n",
    "    print(f\"Parameter: {name}, Mean: {param.mean().item()}, Std: {param.std().item()}\")\n",
    "    if 'AutoDiagonalNormal.loc' in name:\n",
    "        weight_means.append(param.mean().item())\n",
    "        weight_stds.append(param.std().item())\n",
    "    elif 'AutoDiagonalNormal.scale' in name:\n",
    "        bias_means.append(param.mean().item())\n",
    "        bias_stds.append(param.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587cc9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8163303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841e2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36776e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create boxplot for weight means and stds\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.boxplot([weight_means, weight_stds], labels=['Weight Means', 'Weight Stds'])\n",
    "plt.title('Weight Statistics')\n",
    "plt.xlabel('Statistic')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot([bias_means, bias_stds], labels=['Bias Means', 'Bias Stds'])\n",
    "plt.title('Bias Statistics')\n",
    "plt.xlabel('Statistic')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc68b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pyro.clear_param_store()\n",
    "\n",
    "# Ensure model and guide are on the correct device\n",
    "#bayesian_model.to(device)\n",
    "#guide.to(device)\n",
    "\n",
    "#train_loader, test_loader = load_data(batch_size=54)\n",
    "#train_svi(bayesian_model, guide, svi, train_loader, num_epochs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0086a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_path = 'results_eurosat/bayesian_cnn_model_std10_100_epoch_WG.pth'\n",
    "torch.save(bayesian_model.state_dict(), model_path)\n",
    "\n",
    "# save the guide\n",
    "guide_path = 'results_eurosat/bayesian_cnn_guide_std10_100_epoch_guide_WG.pth'\n",
    "torch.save(guide.state_dict(), guide_path)\n",
    "\n",
    "# save the pyro parameter store\n",
    "pyro_param_store_path = 'results_eurosat/pyro_param_store_std10_100_epoch_WG.pkl'\n",
    "pyro.get_param_store().save(pyro_param_store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8edc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae041e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print confusion matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def predict_data(model, loader_of_interest, num_samples=10):\n",
    "    model.eval()\n",
    "    guide.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader_of_interest, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            logits_mc = torch.zeros(num_samples, images.size(0), model.fc1.out_features).to(device)\n",
    "\n",
    "            for i in range(num_samples):\n",
    "                guide_trace = pyro.poutine.trace(guide).get_trace(images)\n",
    "                replayed_model = pyro.poutine.replay(model, trace=guide_trace)\n",
    "                logits = replayed_model(images)\n",
    "                logits_mc[i] = logits\n",
    "\n",
    "            avg_logits = logits_mc.mean(dim=0)\n",
    "            predictions = torch.argmax(avg_logits, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e41ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_predictions = predict_data(bayesian_model, train_loader, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ae529",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cm = confusion_matrix(train_labels, train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e69bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print accuracy from confusion matrix\n",
    "train_accuracy = np.trace(train_cm) / np.sum(train_cm)\n",
    "print(f\"Train accuracy from confusion matrix: {train_accuracy * 100:.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1a7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels, all_predictions = predict_data(bayesian_model, test_loader, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a00a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df3ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print accuracy from confusion matrix\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(f\"Accuracy from confusion matrix: {accuracy * 100:.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69439a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print pyro parameters\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print(f\"{name}: {value.shape} - {value.mean().item():.4f} ± {value.std().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de99aab6",
   "metadata": {},
   "source": [
    "60.092593% for the 10 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb36f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "            \n",
    "    # make a mark to the diagonal\n",
    "    plt.plot([0, cm.shape[1]-1], [0, cm.shape[0]-1], color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the confusion matrix\n",
    "class_names = ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial',\n",
    "               'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
    "plot_confusion_matrix(cm, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb4511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "#model_path = 'results_eurosat/bayesian_cnn_model_std10_100_epoch.pth'\n",
    "#torch.save(bayesian_model.state_dict(), model_path)\n",
    "\n",
    "# save the guide\n",
    "#guide_path = 'results_eurosat/bayesian_cnn_guide_std10_100_epoch_guide.pth'\n",
    "#torch.save(guide.state_dict(), guide_path)\n",
    "\n",
    "# save the pyro parameter store\n",
    "#pyro_param_store_path = 'results_eurosat/pyro_param_store_std10_100_epoch.pkl'\n",
    "#pyro.get_param_store().save(pyro_param_store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3dd508",
   "metadata": {},
   "outputs": [],
   "source": [
    "kataguediemdeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a06416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svi_early_save(model, guide, svi, train_loader, num_epochs=10, patience=3, min_delta=0.001):\n",
    "    # Clear parameter store only ONCE at the beginning\n",
    "    pyro.clear_param_store()\n",
    "    model.train()\n",
    "    \n",
    "    # Ensure model is on the correct device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Lists to store losses and accuracies\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    accuracy_epochs = []\n",
    "    \n",
    "    # Early stopping variables\n",
    "    best_accuracy = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    best_guide_state = None\n",
    "    best_pyro_params = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            loss = svi.step(images, labels)\n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "            \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        epoch_losses.append(avg_loss)\n",
    "        \n",
    "        # Calculate accuracy every 10 epochs (and on the first and last epoch)\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or epoch == num_epochs - 1:\n",
    "            model.eval()\n",
    "            guide.eval()\n",
    "            \n",
    "            correct_predictions = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in tqdm(train_loader, desc=f\"Calculating accuracy for epoch {epoch+1}\"):\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    \n",
    "                    # Sample from the guide to get model parameters\n",
    "                    guide_trace = pyro.poutine.trace(guide).get_trace(images)\n",
    "                    replayed_model = pyro.poutine.replay(model, trace=guide_trace)\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    logits = replayed_model(images)\n",
    "                    predictions = torch.argmax(logits, dim=1)\n",
    "                    \n",
    "                    # Count correct predictions\n",
    "                    correct_predictions += (predictions == labels).sum().item()\n",
    "                    total_samples += labels.size(0)\n",
    "            \n",
    "            epoch_accuracy = correct_predictions / total_samples\n",
    "            epoch_accuracies.append(epoch_accuracy)\n",
    "            accuracy_epochs.append(epoch + 1)\n",
    "            \n",
    "            # Check for improvement\n",
    "            if epoch_accuracy > best_accuracy + min_delta:\n",
    "                best_accuracy = epoch_accuracy\n",
    "                best_epoch = epoch + 1\n",
    "                patience_counter = 0\n",
    "                \n",
    "                # Save best model states\n",
    "                best_model_state = model.state_dict().copy()\n",
    "                best_guide_state = guide.state_dict().copy()\n",
    "                best_pyro_params = pyro.get_param_store().get_state().copy()\n",
    "                \n",
    "                print(f\"Epoch {epoch+1} - ELBO Loss: {avg_loss:.4f}, Train Accuracy: {epoch_accuracy*100:.2f}% *** NEW BEST ***\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"Epoch {epoch+1} - ELBO Loss: {avg_loss:.4f}, Train Accuracy: {epoch_accuracy*100:.2f}% (Best: {best_accuracy*100:.2f}% at epoch {best_epoch})\")\n",
    "                \n",
    "                # Early stopping check\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"\\nEarly stopping triggered! No improvement for {patience} evaluations.\")\n",
    "                    print(f\"Best accuracy: {best_accuracy*100:.2f}% at epoch {best_epoch}\")\n",
    "                    \n",
    "                    # Restore best model\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                    guide.load_state_dict(best_guide_state)\n",
    "                    pyro.get_param_store().set_state(best_pyro_params)\n",
    "                    \n",
    "                    break\n",
    "            \n",
    "            model.train()  # Set back to training mode\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1} - ELBO Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # If training completed without early stopping, still restore best model\n",
    "    if patience_counter < patience and best_model_state is not None:\n",
    "        print(f\"\\nTraining completed. Restoring best model from epoch {best_epoch} (accuracy: {best_accuracy*100:.2f}%)\")\n",
    "        model.load_state_dict(best_model_state)\n",
    "        guide.load_state_dict(best_guide_state)\n",
    "        pyro.get_param_store().set_state(best_pyro_params)\n",
    "    \n",
    "    return epoch_losses, epoch_accuracies, accuracy_epochs, best_epoch, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e877f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "# Ensure model and guide are on the correct device\n",
    "bayesian_model.to(device)\n",
    "guide.to(device)\n",
    "\n",
    "train_loader, test_loader = load_data(batch_size=54)\n",
    "\n",
    "# Train with early stopping\n",
    "losses, accuracies, accuracy_epochs, best_epoch, best_accuracy = train_svi(\n",
    "    bayesian_model, guide, svi, train_loader, \n",
    "    num_epochs=100, \n",
    "    patience=3,  # Stop if no improvement for 3 accuracy evaluations (30 epochs)\n",
    "    min_delta=0.001  # Minimum improvement threshold (0.1%)\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Best training accuracy: {best_accuracy*100:.2f}% at epoch {best_epoch}\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(losses) + 1), losses)\n",
    "plt.axvline(x=best_epoch, color='red', linestyle='--', label=f'Best Model (Epoch {best_epoch})')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('ELBO Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracy_epochs, accuracies, 'o-')\n",
    "plt.axvline(x=best_epoch, color='red', linestyle='--', label=f'Best Model (Epoch {best_epoch})')\n",
    "plt.axhline(y=best_accuracy, color='red', linestyle=':', alpha=0.7)\n",
    "plt.title('Training Accuracy (Every 10 Epochs)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c997e",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76b851",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8865b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svi_with_tensorboard(model, guide, svi, train_loader, num_epochs=10, log_dir='runs/bayesian_cnn'):\n",
    "    # Clear parameter store only ONCE at the beginning\n",
    "    pyro.clear_param_store()\n",
    "    model.train()\n",
    "    \n",
    "    # Ensure model is on the correct device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir)\n",
    "    \n",
    "    # Lists to store losses and accuracies\n",
    "    epoch_losses = []\n",
    "    epoch_accuracies = []\n",
    "    accuracy_epochs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            loss = svi.step(images, labels)\n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "            \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        epoch_losses.append(avg_loss)\n",
    "        \n",
    "        # Log loss to TensorBoard every epoch\n",
    "        writer.add_scalar('Loss/ELBO', avg_loss, epoch + 1)\n",
    "        \n",
    "        # Calculate accuracy every 10 epochs (and on the first and last epoch)\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0 or epoch == num_epochs - 1:\n",
    "            model.eval()\n",
    "            guide.eval()\n",
    "            \n",
    "            correct_predictions = 0\n",
    "            total_samples = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in tqdm(train_loader, desc=f\"Calculating accuracy for epoch {epoch+1}\"):\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    \n",
    "                    # Sample from the guide to get model parameters\n",
    "                    guide_trace = pyro.poutine.trace(guide).get_trace(images)\n",
    "                    replayed_model = pyro.poutine.replay(model, trace=guide_trace)\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    logits = replayed_model(images)\n",
    "                    predictions = torch.argmax(logits, dim=1)\n",
    "                    \n",
    "                    # Count correct predictions\n",
    "                    correct_predictions += (predictions == labels).sum().item()\n",
    "                    total_samples += labels.size(0)\n",
    "            \n",
    "            epoch_accuracy = correct_predictions / total_samples\n",
    "            epoch_accuracies.append(epoch_accuracy)\n",
    "            accuracy_epochs.append(epoch + 1)\n",
    "            \n",
    "            # Log accuracy to TensorBoard\n",
    "            writer.add_scalar('Accuracy/Train', epoch_accuracy, epoch + 1)\n",
    "            \n",
    "            model.train()  # Set back to training mode\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} - ELBO Loss: {avg_loss:.4f}, Train Accuracy: {epoch_accuracy*100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1} - ELBO Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Close the writer\n",
    "    writer.close()\n",
    "    \n",
    "    return epoch_losses, epoch_accuracies, accuracy_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d58a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with TensorBoard logging\n",
    "losses, accuracies, accuracy_epochs = train_svi_with_tensorboard(\n",
    "    bayesian_model, guide, svi, train_loader, \n",
    "    num_epochs=100,\n",
    "    log_dir='runs/eurosat_bayesian_cnn_experiment'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc06dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with TensorBoard logging\n",
    "losses, accuracies, accuracy_epochs = train_svi_with_tensorboard(\n",
    "    bayesian_model, guide, svi, train_loader, \n",
    "    num_epochs=100,\n",
    "    log_dir='runs/eurosat_bayesian_cnn_experiment'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c59226",
   "metadata": {},
   "source": [
    "Feature TODO:\n",
    "1. Record loss after each epoch\n",
    "2. Send result to GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnntest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
