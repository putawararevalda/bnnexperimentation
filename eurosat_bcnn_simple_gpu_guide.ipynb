{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b1af2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import torch\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5965d3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Revalda Putawara\\.conda\\envs\\bnntest\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "853a93c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "#device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5a1b485",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianCNNSingleFC(PyroModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        prior_mu = 0.\n",
    "        #prior_sigma = 0.1 #accuracy 13.203704% 2 epochs\n",
    "        #prior_sigma = 1. #accuracy 31% 2 epochs\n",
    "        prior_sigma = torch.tensor(10., device=device) #accuracy 45% 10 epochs\n",
    "        #prior_sigma = 100 #accuracy 21% 10 epochs\n",
    "\n",
    "        self.conv1 = PyroModule[nn.Conv2d](3, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv1.weight = PyroSample(dist.Normal(prior_mu, prior_sigma).expand([32, 3, 5, 5]).to_event(4))\n",
    "        self.conv1.bias = PyroSample(dist.Normal(prior_mu, prior_sigma).expand([32]).to_event(1))\n",
    "\n",
    "        self.conv2 = PyroModule[nn.Conv2d](32, 64, kernel_size=5, stride=1, padding=2) #initially padding=1 kernel_size=3, without stride\n",
    "        self.conv2.weight = PyroSample(dist.Normal(prior_mu, prior_sigma).expand([64, 32, 5, 5]).to_event(4))\n",
    "        self.conv2.bias = PyroSample(dist.Normal(prior_mu, prior_sigma).expand([64]).to_event(1))\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = PyroModule[nn.Linear](64 * 16 * 16, num_classes)\n",
    "        self.fc1.weight = PyroSample(dist.Normal(prior_mu, prior_sigma).expand([num_classes, 64 * 16 * 16]).to_event(2))\n",
    "        self.fc1.bias = PyroSample(dist.Normal(prior_mu, prior_sigma).expand([num_classes]).to_event(1))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc1(x)\n",
    "        \n",
    "        # THIS IS THE MISSING PIECE: Define the likelihood\n",
    "        if y is not None:\n",
    "            with pyro.plate(\"data\", x.shape[0]):\n",
    "                pyro.sample(\"obs\", dist.Categorical(logits=logits), obs=y)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd0e0735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def old_load_data(batch_size=54): \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.3444, 0.3809, 0.4082], std=[0.1809, 0.1331, 0.1137])\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.EuroSAT(root='./data', transform=transform, download=False)\n",
    "\n",
    "    # Use fixed random seed for reproducible splits\n",
    "    torch.manual_seed(42)\n",
    "    #train_size = int(0.8 * len(dataset))\n",
    "    #test_size = len(dataset) - train_size\n",
    "    #train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    with open('datasplit/split_indices.pkl', 'rb') as f:\n",
    "        split = pickle.load(f)\n",
    "        train_dataset = Subset(dataset, split['train'])\n",
    "        test_dataset = Subset(dataset, split['test'])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22bf6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=54):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.3444, 0.3809, 0.4082], std=[0.1809, 0.1331, 0.1137])\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.EuroSAT(root='./data', transform=transform, download=False)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    with open('datasplit/split_indices.pkl', 'rb') as f:\n",
    "        split = pickle.load(f)\n",
    "        train_dataset = Subset(dataset, split['train'])\n",
    "        test_dataset = Subset(dataset, split['test'])\n",
    "\n",
    "    # Add num_workers and pin_memory for faster data loading\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                             num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                            num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6bc1293",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "bayesian_model = BayesianCNNSingleFC(num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af70a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.nn\n",
    "from pyro.nn import PyroParam, PyroModule\n",
    "#from torch.distributions import constraints\n",
    "from pyro.distributions import constraints\n",
    "import torch\n",
    "\n",
    "class CustomGuide(PyroModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # conv1 weights and bias\n",
    "        self.conv1_weight_loc = PyroParam(torch.zeros(32, 3, 5, 5))\n",
    "        self.conv1_weight_scale = PyroParam(torch.ones(32, 3, 5, 5) * 0.05, constraint=constraints.positive)\n",
    "        self.conv1_bias_loc = PyroParam(torch.zeros(32))\n",
    "        self.conv1_bias_scale = PyroParam(torch.ones(32) * 0.05, constraint=constraints.positive)\n",
    "\n",
    "        # conv2 weights and bias\n",
    "        self.conv2_weight_loc = PyroParam(torch.zeros(64, 32, 5, 5))\n",
    "        self.conv2_weight_scale = PyroParam(torch.ones(64, 32, 5, 5) * 0.05, constraint=constraints.positive)\n",
    "        self.conv2_bias_loc = PyroParam(torch.zeros(64))\n",
    "        self.conv2_bias_scale = PyroParam(torch.ones(64) * 0.05, constraint=constraints.positive)\n",
    "\n",
    "        # fc1 weights and bias\n",
    "        self.fc1_weight_loc = PyroParam(torch.zeros(num_classes, 64 * 16 * 16))\n",
    "        self.fc1_weight_scale = PyroParam(torch.ones(num_classes, 64 * 16 * 16) * 0.05, constraint=constraints.positive)\n",
    "        self.fc1_bias_loc = PyroParam(torch.zeros(num_classes))\n",
    "        self.fc1_bias_scale = PyroParam(torch.ones(num_classes) * 0.05, constraint=constraints.positive)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        pyro.sample(\"conv1.weight\", dist.Normal(self.conv1_weight_loc, self.conv1_weight_scale).to_event(4))\n",
    "        pyro.sample(\"conv1.bias\", dist.Normal(self.conv1_bias_loc, self.conv1_bias_scale).to_event(1))\n",
    "        pyro.sample(\"conv2.weight\", dist.Normal(self.conv2_weight_loc, self.conv2_weight_scale).to_event(4))\n",
    "        pyro.sample(\"conv2.bias\", dist.Normal(self.conv2_bias_loc, self.conv2_bias_scale).to_event(1))\n",
    "        pyro.sample(\"fc1.weight\", dist.Normal(self.fc1_weight_loc, self.fc1_weight_scale).to_event(2))\n",
    "        pyro.sample(\"fc1.bias\", dist.Normal(self.fc1_bias_loc, self.fc1_bias_scale).to_event(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f39c1cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn import PyroParam, PyroModule\n",
    "#from torch.distributions import constraints\n",
    "from pyro.distributions import constraints\n",
    "import torch\n",
    "\n",
    "class CustomGuide(PyroModule):\n",
    "    def __init__(self, num_classes, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize means and scales similarly to AutoDiagonalNormal default (mean=0, scale=0.1)\n",
    "        init_loc = 0.0\n",
    "        init_scale = 0.1  # AutoDiagonalNormal usually init scale ~0.1, but you can adjust\n",
    "\n",
    "        self.conv1_weight_loc = PyroParam(torch.full((32, 3, 5, 5), init_loc, device=device))\n",
    "        self.conv1_weight_scale = PyroParam(torch.full((32, 3, 5, 5), init_scale, device=device),\n",
    "                                            constraint=constraints.softplus_positive)\n",
    "\n",
    "        self.conv1_bias_loc = PyroParam(torch.full((32,), init_loc, device=device))\n",
    "        self.conv1_bias_scale = PyroParam(torch.full((32,), init_scale, device=device),\n",
    "                                         constraint=constraints.softplus_positive)\n",
    "\n",
    "        self.conv2_weight_loc = PyroParam(torch.full((64, 32, 5, 5), init_loc, device=device))\n",
    "        self.conv2_weight_scale = PyroParam(torch.full((64, 32, 5, 5), init_scale, device=device),\n",
    "                                            constraint=constraints.softplus_positive)\n",
    "\n",
    "        self.conv2_bias_loc = PyroParam(torch.full((64,), init_loc, device=device))\n",
    "        self.conv2_bias_scale = PyroParam(torch.full((64,), init_scale, device=device),\n",
    "                                         constraint=constraints.softplus_positive)\n",
    "\n",
    "        self.fc1_weight_loc = PyroParam(torch.full((num_classes, 64 * 16 * 16), init_loc, device=device))\n",
    "        self.fc1_weight_scale = PyroParam(torch.full((num_classes, 64 * 16 * 16), init_scale, device=device),\n",
    "                                         constraint=constraints.softplus_positive)\n",
    "\n",
    "        self.fc1_bias_loc = PyroParam(torch.full((num_classes,), init_loc, device=device))\n",
    "        self.fc1_bias_scale = PyroParam(torch.full((num_classes,), init_scale, device=device),\n",
    "                                       constraint=constraints.softplus_positive)\n",
    "\n",
    "    def forward(self, x=None, y=None):\n",
    "        # Sample latent variables in *exact same order* as model's latent variables\n",
    "        pyro.sample(\"conv1.weight\", dist.Normal(self.conv1_weight_loc, self.conv1_weight_scale).to_event(4))\n",
    "        pyro.sample(\"conv1.bias\", dist.Normal(self.conv1_bias_loc, self.conv1_bias_scale).to_event(1))\n",
    "\n",
    "        pyro.sample(\"conv2.weight\", dist.Normal(self.conv2_weight_loc, self.conv2_weight_scale).to_event(4))\n",
    "        pyro.sample(\"conv2.bias\", dist.Normal(self.conv2_bias_loc, self.conv2_bias_scale).to_event(1))\n",
    "\n",
    "        pyro.sample(\"fc1.weight\", dist.Normal(self.fc1_weight_loc, self.fc1_weight_scale).to_event(2))\n",
    "        pyro.sample(\"fc1.bias\", dist.Normal(self.fc1_bias_loc, self.fc1_bias_scale).to_event(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e7df546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroParam\n",
    "from torch.distributions import constraints\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomVectorizedGuide(PyroModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Total parameter size\n",
    "        self.param_shapes = {\n",
    "            \"conv1.weight\": (32, 3, 5, 5),\n",
    "            \"conv1.bias\": (32,),\n",
    "            \"conv2.weight\": (64, 32, 5, 5),\n",
    "            \"conv2.bias\": (64,),\n",
    "            \"fc1.weight\": (num_classes, 64 * 16 * 16),\n",
    "            \"fc1.bias\": (num_classes,)\n",
    "        }\n",
    "\n",
    "        total_params = sum(torch.tensor(shape).prod() for shape in self.param_shapes.values())\n",
    "        self.total_size = total_params.item()\n",
    "\n",
    "        # Vectorized parameters\n",
    "        self.loc = PyroParam(torch.zeros(self.total_size))\n",
    "        self.scale_unconstrained = PyroParam(torch.full((self.total_size,), -2.0))  # softplus(-2) ≈ 0.12\n",
    "\n",
    "    def _unpack(self, vector):\n",
    "        \"\"\"Unpacks flat vector into a dict of shaped tensors\"\"\"\n",
    "        params = {}\n",
    "        offset = 0\n",
    "        for name, shape in self.param_shapes.items():\n",
    "            size = torch.tensor(shape).prod().item()\n",
    "            flat_param = vector[offset:offset + size]\n",
    "            params[name] = flat_param.view(shape)\n",
    "            offset += size\n",
    "        return params\n",
    "\n",
    "    def forward(self, x=None, y=None):\n",
    "        scale = F.softplus(self.scale_unconstrained)\n",
    "        # Sample a single Normal(loc, scale)\n",
    "        guide_dist = dist.Normal(self.loc, scale).to_event(1)\n",
    "        sample = pyro.sample(\"_auto_latent\", guide_dist)\n",
    "\n",
    "        # Unpack the flat sample into model parameter shapes\n",
    "        unpacked = self._unpack(sample)\n",
    "\n",
    "        # Feed these into Pyro sample statements so they match the model\n",
    "        pyro.sample(\"conv1.weight\", dist.Delta(unpacked[\"conv1.weight\"]).to_event(4))\n",
    "        pyro.sample(\"conv1.bias\", dist.Delta(unpacked[\"conv1.bias\"]).to_event(1))\n",
    "        pyro.sample(\"conv2.weight\", dist.Delta(unpacked[\"conv2.weight\"]).to_event(4))\n",
    "        pyro.sample(\"conv2.bias\", dist.Delta(unpacked[\"conv2.bias\"]).to_event(1))\n",
    "        pyro.sample(\"fc1.weight\", dist.Delta(unpacked[\"fc1.weight\"]).to_event(2))\n",
    "        pyro.sample(\"fc1.bias\", dist.Delta(unpacked[\"fc1.bias\"]).to_event(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3553741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "from pyro.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1420314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#guide = AutoDiagonalNormal(bayesian_model)\n",
    "#guide = CustomGuide(num_classes=num_classes)\n",
    "guide = CustomVectorizedGuide(num_classes=num_classes)\n",
    "\n",
    "# 2. Optimizer and SVI - increase learning rate for better convergence\n",
    "optimizer = Adam({\"lr\": 1e-3})  # Increased from 1e-4 to 1e-3\n",
    "svi = pyro.infer.SVI(model=bayesian_model,\n",
    "                     guide=guide,\n",
    "                     optim=optimizer,\n",
    "                     loss=pyro.infer.Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a372470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "845d3400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svi(model, guide, svi, train_loader, num_epochs=10):\n",
    "    # Clear parameter store only ONCE at the beginning\n",
    "    pyro.clear_param_store()\n",
    "    model.train()\n",
    "    \n",
    "    # Ensure model is on the correct device\n",
    "    model.to(device)\n",
    "    #guide.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            loss = svi.step(images, labels)\n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "            \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1} - ELBO Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8321bc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/400 [00:00<?, ?it/s]c:\\Users\\Revalda Putawara\\.conda\\envs\\bnntest\\Lib\\site-packages\\pyro\\util.py:288: UserWarning: Found non-auxiliary vars in guide but not model, consider marking these infer={'is_auxiliary': True}:\n",
      "{'_auto_latent'}\n",
      "  warnings.warn(\n",
      "Epoch 1/10: 100%|██████████| 400/400 [00:36<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - ELBO Loss: 816830.5121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 400/400 [00:14<00:00, 27.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - ELBO Loss: 755250.8649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 400/400 [00:13<00:00, 30.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - ELBO Loss: 689854.6885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 400/400 [00:14<00:00, 28.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - ELBO Loss: 625978.2338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 400/400 [00:13<00:00, 30.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - ELBO Loss: 565972.3778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 400/400 [00:15<00:00, 25.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - ELBO Loss: 511394.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 400/400 [00:13<00:00, 29.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - ELBO Loss: 462949.3297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 400/400 [00:13<00:00, 29.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - ELBO Loss: 420367.4260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 400/400 [00:13<00:00, 30.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - ELBO Loss: 383153.0426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 400/400 [00:13<00:00, 30.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - ELBO Loss: 350525.5477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "# Ensure model and guide are on the correct device\n",
    "bayesian_model.to(device)\n",
    "guide.to(device)\n",
    "\n",
    "train_loader, test_loader = load_data(batch_size=54)\n",
    "train_svi(bayesian_model, guide, svi, train_loader, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2e7cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_path = 'results_eurosat/bayesian_cnn_model_std10_cust10_epoch.pth'\n",
    "torch.save(bayesian_model.state_dict(), model_path)\n",
    "\n",
    "# save the guide\n",
    "guide_path = 'results_eurosat/bayesian_cnn_guide_std10_cust10_epoch_guide.pth'\n",
    "torch.save(guide.state_dict(), guide_path)\n",
    "\n",
    "# save the pyro parameter store\n",
    "pyro_param_store_path = 'results_eurosat/pyro_param_store_std10_cust10_epoch.pkl'\n",
    "pyro.get_param_store().save(pyro_param_store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb338cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_unconstrained torch.Size([217546])\n",
      "loc torch.Size([217546])\n",
      "Total number of parameters in the model: 435092\n"
     ]
    }
   ],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, value.shape)\n",
    "\n",
    "# print the total number of parameters in the model, from the value.shape\n",
    "# by converting the value.shape into scalar\n",
    "total_params = sum(value.numel() for value in pyro.get_param_store().values())\n",
    "print(f\"Total number of parameters in the model: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f049f656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_unconstrained: torch.Size([217546])\n",
      "loc: torch.Size([217546])\n"
     ]
    }
   ],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(f\"{name}: {value.shape}\")\n",
    "    #print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eae041e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print confusion matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def predict_data(model, test_loader, num_samples=10):\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            logits_mc = torch.zeros(num_samples, images.size(0), model.fc1.out_features).to(device)\n",
    "\n",
    "            for i in range(num_samples):\n",
    "                guide_trace = pyro.poutine.trace(guide).get_trace()\n",
    "                replayed_model = pyro.poutine.replay(model, trace=guide_trace)\n",
    "                logits = replayed_model(images)\n",
    "                logits_mc[i] = logits\n",
    "\n",
    "            avg_logits = logits_mc.mean(dim=0)\n",
    "            predictions = torch.argmax(avg_logits, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e1a7997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:40<00:00,  2.50it/s]\n"
     ]
    }
   ],
   "source": [
    "all_labels, all_predictions = predict_data(bayesian_model, test_loader, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f83a00a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64df3ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy from confusion matrix: 8.981481%\n"
     ]
    }
   ],
   "source": [
    "#print accuracy from confusion matrix\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(f\"Accuracy from confusion matrix: {accuracy * 100:.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb36f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "            \n",
    "    # make a mark to the diagonal\n",
    "    plt.plot([0, cm.shape[1]-1], [0, cm.shape[0]-1], color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the confusion matrix\n",
    "class_names = ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial',\n",
    "               'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
    "plot_confusion_matrix(cm, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb4511a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5c59226",
   "metadata": {},
   "source": [
    "Feature TODO:\n",
    "1. Record loss after each epoch\n",
    "2. Send result to GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnntest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
