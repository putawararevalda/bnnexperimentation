{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0c695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import torch\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3ece6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Revalda Putawara\\.conda\\envs\\bnntest\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f28e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c28915aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_weight_dict = {'conv1.weight': (-0.0002266301744384691, 0.07788292318582535), \n",
    "                    'conv1.bias': (-0.06533964723348618, 0.11781848967075348), \n",
    "                    'conv2.weight': (-0.0124916797503829, 0.044476691633462906), \n",
    "                    'conv2.bias': (-0.010315056890249252, 0.06136268749833107), \n",
    "                    'fc1.weight': (-0.004733316134661436, 0.04221488535404205), \n",
    "                    'fc1.bias': (-0.004896007478237152, 0.11825607717037201)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2498c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the CNN class to have 1 fc layer only\n",
    "\n",
    "class CNNSingleFC(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, num_classes)  # Single FC layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))   # -> (32, 32, 32)\n",
    "        x = self.pool(F.relu(self.conv2(x)))   # -> (64, 16, 16)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)                         # Single FC layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0dcd99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the CNN class to a Bayesian CNN with 1 fc layer only, with the weight initialization from init_weight_dict\n",
    "class BayesianCNNSingleFC(PyroModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        #self.conv1 = PyroModule[nn.Conv2d](3, 32, kernel_size=5, stride=1, padding=2)\n",
    "        #self.conv2 = PyroModule[nn.Conv2d](32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        #self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        #self.fc1 = PyroModule[nn.Linear](64 * 16 * 16, num_classes)\n",
    "\n",
    "        self.conv1 = PyroModule[nn.Conv2d](3, 32, kernel_size=3, padding=1)\n",
    "        self.conv1.weight = PyroSample(dist.Normal(init_weight_dict['conv1.weight'][0], init_weight_dict['conv1.weight'][1]).expand([32, 3, 3, 3]).to_event(4))\n",
    "        self.conv1.bias = PyroSample(dist.Normal(init_weight_dict['conv1.bias'][0], init_weight_dict['conv1.bias'][1]).expand([32]).to_event(1))\n",
    "\n",
    "        self.conv2 = PyroModule[nn.Conv2d](32, 64, kernel_size=3, padding=1)\n",
    "        self.conv2.weight = PyroSample(dist.Normal(init_weight_dict['conv2.weight'][0], init_weight_dict['conv2.weight'][1]).expand([64, 32, 3, 3]).to_event(4))\n",
    "        self.conv2.bias = PyroSample(dist.Normal(init_weight_dict['conv2.bias'][0], init_weight_dict['conv2.bias'][1]).expand([64]).to_event(1))\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = PyroModule[nn.Linear](64 * 16 * 16, num_classes)\n",
    "        self.fc1.weight = PyroSample(dist.Normal(init_weight_dict['fc1.weight'][0], init_weight_dict['fc1.weight'][1]).expand([num_classes, 64 * 16 * 16]).to_event(2))\n",
    "        self.fc1.bias = PyroSample(dist.Normal(init_weight_dict['fc1.bias'][0], init_weight_dict['fc1.bias'][1]).expand([num_classes]).to_event(1))\n",
    "\n",
    "    #def forward(self, x, y=None):\n",
    "    #    x = self.pool(F.relu(self.conv1(x)))\n",
    "    #    x = self.pool(F.relu(self.conv2(x)))\n",
    "    #    x = x.view(x.size(0), -1)\n",
    "    #    x = self.fc1(x)\n",
    "\n",
    "    #    if y is not None:\n",
    "    #        with pyro.plate(\"data\", x.shape[0]):\n",
    "    #            pyro.sample(\"obs\", dist.Categorical(logits=logits), obs=y)\n",
    "\n",
    "    #    return x\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc1(x)\n",
    "        \n",
    "        # THIS IS THE MISSING PIECE: Define the likelihood\n",
    "        if y is not None:\n",
    "            with pyro.plate(\"data\", x.shape[0]):\n",
    "                pyro.sample(\"obs\", dist.Categorical(logits=logits), obs=y)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a61760b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=64):  # Changed from 54 to 64 to match deterministic CNN\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.3444, 0.3809, 0.4082], std=[0.1809, 0.1331, 0.1137])\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.EuroSAT(root='./data', transform=transform, download=False)\n",
    "\n",
    "    # Use fixed random seed for reproducible splits\n",
    "    torch.manual_seed(42)\n",
    "    #train_size = int(0.8 * len(dataset))\n",
    "    #test_size = len(dataset) - train_size\n",
    "    #train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    with open('datasplit/split_indices.pkl', 'rb') as f:\n",
    "        split = pickle.load(f)\n",
    "        train_dataset = Subset(dataset, split['train'])\n",
    "        test_dataset = Subset(dataset, split['test'])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cca9cde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.infer\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "from pyro.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Instantiate model and guide - fix device consistency\n",
    "num_classes = 10\n",
    "bayesian_model = BayesianCNNSingleFC(num_classes=num_classes).to(device)  # Use .to(device) instead of .cuda()\n",
    "guide = AutoDiagonalNormal(bayesian_model)\n",
    "\n",
    "# 2. Optimizer and SVI - increase learning rate for better convergence\n",
    "optimizer = Adam({\"lr\": 1e-3})  # Increased from 1e-4 to 1e-3\n",
    "svi = pyro.infer.SVI(model=bayesian_model,\n",
    "                     guide=guide,\n",
    "                     optim=optimizer,\n",
    "                     loss=pyro.infer.Trace_ELBO())\n",
    "\n",
    "# 3. Training function\n",
    "def train_svi(model, guide, svi, train_loader, num_epochs=10):\n",
    "    # Clear parameter store only ONCE at the beginning\n",
    "    pyro.clear_param_store()\n",
    "    model.train()\n",
    "    \n",
    "    # Ensure model is on the correct device\n",
    "    model.to(device)\n",
    "    guide.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            loss = svi.step(images, labels)\n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "            \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"Epoch {epoch+1} - ELBO Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "959a92e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 400/400 [02:02<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - ELBO Loss: 197077.5633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 400/400 [00:47<00:00,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - ELBO Loss: 110739.0545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "# Ensure model and guide are on the correct device\n",
    "bayesian_model.to(device)\n",
    "guide.to(device)\n",
    "\n",
    "train_loader, test_loader = load_data(batch_size=54)\n",
    "train_svi(bayesian_model, guide, svi, train_loader, num_epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d61342ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_svi(model, guide, test_loader, num_samples=10):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    #device = next(model.parameters()).device  # Get the device where the model is allocated\n",
    "\n",
    "    with torch.no_grad():  # Add no_grad for efficiency\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)  # Fix: move to device\n",
    "\n",
    "            # Accumulate logits from multiple samples\n",
    "            logits_mc = torch.zeros(num_samples, images.size(0), model.fc1.out_features).to(device)\n",
    "\n",
    "            for i in range(num_samples):\n",
    "                # Sample from the guide (posterior) and replay through model\n",
    "                guide_trace = pyro.poutine.trace(guide).get_trace()\n",
    "                replayed_model = pyro.poutine.replay(model, trace=guide_trace)\n",
    "                logits = replayed_model(images)\n",
    "                logits_mc[i] = logits\n",
    "\n",
    "            # Average the logits across samples\n",
    "            avg_logits = logits_mc.mean(dim=0)\n",
    "            predictions = torch.argmax(avg_logits, dim=1)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy over {num_samples} MC samples: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39b3ecad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 100/100 [00:39<00:00,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy over 10 MC samples: 10.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10648148148148148"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_svi(bayesian_model, guide, test_loader, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a795f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(f\"{name}: {value.shape}\")\n",
    "    print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print confusion matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def predict_data(model, test_loader, num_samples=10):\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            logits_mc = torch.zeros(num_samples, images.size(0), model.fc1.out_features).to(device)\n",
    "\n",
    "            for i in range(num_samples):\n",
    "                guide_trace = pyro.poutine.trace(guide).get_trace()\n",
    "                replayed_model = pyro.poutine.replay(model, trace=guide_trace)\n",
    "                logits = replayed_model(images)\n",
    "                logits_mc[i] = logits\n",
    "\n",
    "            avg_logits = logits_mc.mean(dim=0)\n",
    "            predictions = torch.argmax(avg_logits, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99abe9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels, all_predictions = predict_data(bayesian_model, test_loader, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95b393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8673b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print accuracy from confusion matrix\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(f\"Accuracy from confusion matrix: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b901f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "            \n",
    "    # make a mark to the diagonal\n",
    "    plt.plot([0, cm.shape[1]-1], [0, cm.shape[0]-1], color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the confusion matrix\n",
    "class_names = ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial',\n",
    "               'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\n",
    "plot_confusion_matrix(cm, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43595f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_prediction_check_batch(model, guide, test_loader, batch_num=0, num_samples=10):\n",
    "    model.eval()\n",
    "    \n",
    "    # Check multiple samples from posterior\n",
    "    all_preds_single = []\n",
    "    all_preds_mc = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            if i == batch_num:\n",
    "                images = images.to(device)\n",
    "                \n",
    "                # Single sample prediction (your current method)\n",
    "                guide_trace = pyro.poutine.trace(guide).get_trace()\n",
    "                replayed_model = pyro.poutine.replay(model, trace=guide_trace)\n",
    "                logits_single = replayed_model(images)\n",
    "                preds_single = torch.argmax(logits_single, dim=1)\n",
    "                all_preds_single.extend(preds_single.cpu().numpy())\n",
    "                \n",
    "                # Multiple samples (Monte Carlo)\n",
    "                logits_mc = torch.zeros(num_samples, images.size(0), model.fc1.out_features).to(device)\n",
    "                for j in range(num_samples):\n",
    "                    guide_trace = pyro.poutine.trace(guide).get_trace()\n",
    "                    replayed_model = pyro.poutine.replay(model, trace=guide_trace)\n",
    "                    logits_mc[j] = replayed_model(images)\n",
    "                \n",
    "                avg_logits = logits_mc.mean(dim=0)\n",
    "                preds_mc = torch.argmax(avg_logits, dim=1)\n",
    "                all_preds_mc.extend(preds_mc.cpu().numpy())\n",
    "\n",
    "\n",
    "                \n",
    "                break  # Just check the specified batch\n",
    "    \n",
    "    print(f\"Batch {batch_num} - Single sample prediction distribution:\", np.bincount(all_preds_single, minlength=num_classes))\n",
    "    print(f\"Batch {batch_num} - MC average prediction distribution:\\t\", np.bincount(all_preds_mc, minlength=num_classes))\n",
    "    true_labels = labels.cpu().numpy()\n",
    "    print(\"True labels distribution:\\t\\t\\t\", np.bincount(true_labels, minlength=num_classes))\n",
    "\n",
    "    #print the confusion matrix for this batch\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(true_labels, all_preds_mc)\n",
    "    print(\"Confusion Matrix for Batch\", batch_num)\n",
    "    print(cm)\n",
    "    \n",
    "    # Check if guide has learned meaningful parameters\n",
    "    print(\"\\nGuide parameter statistics:\")\n",
    "    for name, param in pyro.get_param_store().items():\n",
    "        if 'loc' in name:\n",
    "            print(f\"{name}: mean={param.mean().item():.4f}, std={param.std().item():.4f}\")\n",
    "        elif 'scale' in name:\n",
    "            print(f\"{name}: mean={param.mean().item():.4f}, min={param.min().item():.4f}, max={param.max().item():.4f}\")\n",
    "    \n",
    "    #plot the confusion matrix\n",
    "    plot_confusion_matrix(cm, class_names)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e936af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehensive_prediction_check_batch(bayesian_model, \n",
    "                                     guide, \n",
    "                                     test_loader, \n",
    "                                     batch_num=4, \n",
    "                                     num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a71f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show 1 sample prediction and put the label on the image in matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_sample_prediction(model, test_loader, num_samples=10, image_idx=0):\n",
    "    model.eval()\n",
    "    images, labels = next(iter(test_loader))\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    logits_mc = torch.zeros(num_samples, images.size(0), model.fc1.out_features).to(device)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        guide_trace = pyro.poutine.trace(guide).get_trace()\n",
    "        replayed_model = pyro.poutine.replay(model, trace=guide_trace)\n",
    "        logits = replayed_model(images)\n",
    "        logits_mc[i] = logits\n",
    "\n",
    "    avg_logits = logits_mc.mean(dim=0)\n",
    "    predictions = torch.argmax(avg_logits, dim=1)\n",
    "\n",
    "    #print the logits and its label in descending order\n",
    "    sorted_logits, sorted_indices = torch.sort(avg_logits[0], descending=True)\n",
    "\n",
    "    # for the correct label, print in which rank is the label in the sorted logits\n",
    "    correct_label = labels[0].item()\n",
    "    correct_rank = (sorted_indices == correct_label).nonzero(as_tuple=True)[0].item()\n",
    "    #print(f\"Correct label {correct_label} is at rank {correct_rank + 1} in the sorted logits.\")\n",
    "\n",
    "    # Show the first image and its prediction\n",
    "    plt.imshow(images[image_idx].cpu().permute(1, 2, 0)* 0.1137 + 0.4082)\n",
    "    #plt.title(f\"Predicted: {predictions[0]}, True: {labels[0]} \\nRank: {correct_rank + 1}\")\n",
    "    plt.title(f\"Rank: {correct_rank + 1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    #converts the logits to probabilities\n",
    "    probabilities = F.softmax(avg_logits, dim=1)\n",
    "\n",
    "    print(\"Logits (sorted):\")\n",
    "    for i in range(len(sorted_logits)):\n",
    "        print(f\"Class {sorted_indices[i].item()}: {sorted_logits[i].item()} ({probabilities[0][sorted_indices[i]].item()*100:.2f}%)\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca4abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample_prediction(bayesian_model, test_loader, num_samples=10, image_idx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e18fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnntest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
