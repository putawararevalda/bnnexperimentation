{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8f77272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import torch\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfad9b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/bnntest/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f88cba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3af76a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "583407a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_star = torch.load(\"w_star.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb8ceab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicCNNSingleFCRelu(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc1 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 3, 64, 64]\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # → [B, 32, 32, 32]\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # → [B, 64, 16, 16]\n",
    "        x = self.gap(x)                       # → [B, 64, 1, 1]\n",
    "        x = x.view(x.size(0), -1)             # → [B, 64]\n",
    "        logits = self.fc1(x)                  # → [B, num_classes]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b3193e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic_cnn = DeterministicCNNSingleFCRelu(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeb9b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to invert softplus\n",
    "def inv_softplus(x):\n",
    "    return x + torch.log(-torch.expm1(-x))\n",
    "\n",
    "# small initial posterior std\n",
    "init_eps = 0.05\n",
    "raw_init = inv_softplus(torch.tensor(init_eps))\n",
    "\n",
    "# --- 2) Define your guide using warm-started variational params ---\n",
    "def guide(x, y=None):\n",
    "    # Plate over parameters\n",
    "    for name, param in deterministic_cnn.named_parameters():\n",
    "        # variational mean = pretrained weight\n",
    "        loc = pyro.param(f\"{name}_loc\",\n",
    "                         lambda: w_star[name])\n",
    "        \n",
    "        # variational raw scale so that softplus(raw_scale) ≈ init_eps\n",
    "        scale_unconstrained = pyro.param(f\"{name}_scale_unconstrained\",\n",
    "                                         lambda: torch.full_like(w_star[name], raw_init))\n",
    "        scale = F.softplus(scale_unconstrained)\n",
    "        \n",
    "        # sample a weight tensor from Normal(loc, scale)\n",
    "        # to_event(param.dim()) ensures we treat all dims as batch dims\n",
    "        pyro.sample(name,\n",
    "                    dist.Normal(loc, scale)\n",
    "                        .to_event(param.dim()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c03393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample\n",
    "\n",
    "class BayesianCNNSingleFC(PyroModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        device,\n",
    "        activation='wg',\n",
    "        prior_dist='gaussian',\n",
    "        mu=0,\n",
    "        b=10.0,\n",
    "        prior_params=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Store device\n",
    "        self.device = device\n",
    "\n",
    "        # Activation setup: accept string or callable\n",
    "        if isinstance(activation, str):\n",
    "            act_map = {\n",
    "                'relu': F.relu,\n",
    "                'tanh': F.tanh,\n",
    "                'wg': self.actWG,\n",
    "                'rwg': self.actRWG,\n",
    "                'sigmoid': F.sigmoid,\n",
    "                'sinusoidal': torch.sin,\n",
    "            }\n",
    "            try:\n",
    "                self.activation_fn = act_map[activation]\n",
    "            except KeyError:\n",
    "                raise ValueError(f\"Unsupported activation: {activation}\")\n",
    "        elif callable(activation):\n",
    "            self.activation_fn = activation\n",
    "        else:\n",
    "            raise ValueError(\"activation must be a string or callable\")\n",
    "\n",
    "        # Prior distribution setup\n",
    "        self.prior_dist = prior_dist\n",
    "        # Set default prior parameters if not provided\n",
    "        default_params = {'mu': 0.0, 'b': 10.0}\n",
    "        params = default_params if prior_params is None else prior_params\n",
    "        self.prior_mu = torch.tensor(params.get('mu', params['mu']), device=device)\n",
    "        self.prior_b  = torch.tensor(params.get('b', params['b']), device=device)\n",
    "\n",
    "        print(f\"Using prior distribution: {self.prior_dist} with mu={self.prior_mu.item()} and b={self.prior_b.item()}\")\n",
    "\n",
    "        # Layer definitions with priors\n",
    "        self.conv1 = PyroModule[nn.Conv2d](3, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv1.weight = PyroSample(self._make_prior([32, 3, 5, 5]))\n",
    "        self.conv1.bias   = PyroSample(self._make_prior([32]))\n",
    "\n",
    "        self.conv2 = PyroModule[nn.Conv2d](32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2.weight = PyroSample(self._make_prior([64, 32, 5, 5]))\n",
    "        self.conv2.bias   = PyroSample(self._make_prior([64]))\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.gap  = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc1 = PyroModule[nn.Linear](64, num_classes)\n",
    "        self.fc1.weight = PyroSample(self._make_prior([num_classes, 64]))\n",
    "        self.fc1.bias   = PyroSample(self._make_prior([num_classes]))\n",
    "\n",
    "    def actWG(self, x, alpha=1.0):\n",
    "        # Weight-gradient activation\n",
    "        return x * torch.exp(-alpha * x ** 2)\n",
    "    \n",
    "    def actRWG(self, x, alpha=1.0):\n",
    "        # Weight-gradient activation\n",
    "        return max(0,x * torch.exp(-alpha * x ** 2))\n",
    "\n",
    "    def _make_prior(self, shape):\n",
    "        \"\"\"\n",
    "        Construct a prior distribution based on self.prior_dist and parameters.\n",
    "        \"\"\"\n",
    "        if self.prior_dist == 'gaussian':\n",
    "            base = dist.Normal(self.prior_mu, self.prior_b)\n",
    "        elif self.prior_dist == 'laplace':\n",
    "            base = dist.Laplace(self.prior_mu, self.prior_b)\n",
    "        elif self.prior_dist == 'uniform':\n",
    "            # uniform over [-b, b]\n",
    "            base = dist.Uniform(-self.prior_b, self.prior_b)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported prior distribution: {self.prior_dist}\")\n",
    "        return base.expand(shape).to_event(len(shape))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # x: [B, 3, 64, 64]\n",
    "        x = self.activation_fn(self.conv1(x).to(self.device))\n",
    "        x = self.pool(x)\n",
    "        x = self.activation_fn(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.gap(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc1(x)\n",
    "\n",
    "        if y is not None:\n",
    "            with pyro.plate(\"data\", x.size(0)):\n",
    "                pyro.sample(\"obs\", dist.Categorical(logits=logits), obs=y)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ff75157",
   "metadata": {},
   "outputs": [],
   "source": [
    "eurosat_mean = [0.344, 0.380, 0.408]\n",
    "eurosat_std  = [0.190, 0.137, 0.115]\n",
    "\n",
    "old_mean = [0.3444, 0.3803, 0.4078]\n",
    "old_std = [0.0914, 0.0651, 0.0552]\n",
    "\n",
    "\n",
    "def load_data(batch_size=54):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=eurosat_mean, \n",
    "                             std=eurosat_std)\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.EuroSAT(root='./data', transform=transform, download=True)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    #train_size = int(0.8 * len(dataset))\n",
    "    #test_size = len(dataset) - train_size\n",
    "    #train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    with open('datasplit/split_indices.pkl', 'rb') as f:\n",
    "        split = pickle.load(f)\n",
    "        train_dataset = Subset(dataset, split['train'])\n",
    "        test_dataset = Subset(dataset, split['test'])\n",
    "\n",
    "    # Add num_workers and pin_memory for faster data loading\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                             num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                            num_workers=4, pin_memory=True, persistent_workers=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f77d5606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training SVI function\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pyro\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def train_svi_with_stats(\n",
    "    model,\n",
    "    guide,\n",
    "    svi,\n",
    "    train_loader,\n",
    "    device,\n",
    "    num_epochs=10,\n",
    "    save_epochs=None,\n",
    "    save_dir='results',\n",
    "    model_filename_pattern='model_{activation}_{prior}_epoch_{epoch}_{timestamp}.pth',\n",
    "    guide_filename_pattern='guide_{activation}_{prior}_epoch_{epoch}_{timestamp}.pth',\n",
    "    param_store_filename_pattern='param_store_{activation}_{prior}_epoch_{epoch}_{timestamp}.pkl',\n",
    "    accuracies_filename_pattern='accuracy_results_{activation}_{prior}_{timestamp}.csv',\n",
    "    losses_filename_pattern='losses_{activation}_{prior}_{timestamp}.csv',\n",
    "    model_config_filename_pattern='config_{activation}_{prior}_{timestamp}.json'\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the SVI model, track losses/accuracies, and\n",
    "    save artifacts only when accuracy improves, naming files\n",
    "    like `model_relu_gaussian_epoch_3.pth`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pull names off the model if available, else fall back\n",
    "    #act_name  = getattr(model, 'activation', getattr(model, 'activation_name', 'act'))\n",
    "    act_name = model.activation_fn.__name__ if hasattr(model.activation_fn, '__name__') else str(model.activation_fn)\n",
    "    prior_name = getattr(model, 'prior_dist', 'prior')\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_epochs = set(save_epochs or range(1, num_epochs+1))\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "    model.to(device)\n",
    "\n",
    "    epoch_losses, epoch_accuracies, accuracy_epochs = [], [], []\n",
    "    weight_stats = {'epochs': [], 'means': [], 'stds': []}\n",
    "    bias_stats   = {'epochs': [], 'means': [], 'stds': []}\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        batches = 0\n",
    "\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            total_loss += svi.step(images, labels)\n",
    "            batches += 1\n",
    "\n",
    "        avg_loss = total_loss / batches\n",
    "        epoch_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch} - ELBO Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == num_epochs:\n",
    "            model.eval(); #guide.eval()\n",
    "            correct, total = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for images, labels in tqdm(train_loader, desc=f\"Acc check epoch {epoch}\"):\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    trace = pyro.poutine.trace(guide).get_trace(images)\n",
    "                    replayed = pyro.poutine.replay(model, trace=trace)\n",
    "                    logits = replayed(images)\n",
    "                    preds = torch.argmax(logits, dim=1)\n",
    "                    correct += (preds == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "\n",
    "            acc = correct/total\n",
    "            epoch_accuracies.append(acc); accuracy_epochs.append(epoch)\n",
    "            print(f\"Epoch {epoch} - Train Acc: {acc*100:.2f}%\")\n",
    "\n",
    "            # record stats...\n",
    "            w_means, w_stds, b_means, b_stds = [], [], [], []\n",
    "            for name, param in pyro.get_param_store().items():\n",
    "                if 'loc' in name:\n",
    "                    w_means.append(param.mean().item()); w_stds.append(param.std().item())\n",
    "                elif 'scale' in name:\n",
    "                    b_means.append(param.mean().item()); b_stds.append(param.std().item())\n",
    "            weight_stats['epochs'].append(epoch)\n",
    "            weight_stats['means'].append(w_means)\n",
    "            weight_stats['stds'].append(w_stds)\n",
    "            bias_stats['epochs'].append(epoch)\n",
    "            bias_stats['means'].append(b_means)\n",
    "            bias_stats['stds'].append(b_stds)\n",
    "\n",
    "            # only save when accuracy improves\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                fname_model = model_filename_pattern.format(activation=act_name, prior=prior_name, epoch=\"best\", timestamp=timestamp)\n",
    "                fname_guide = guide_filename_pattern.format(activation=act_name, prior=prior_name, epoch=\"best\", timestamp=timestamp)\n",
    "                fname_ps    = param_store_filename_pattern.format(activation=act_name, prior=prior_name, epoch=\"best\", timestamp=timestamp)\n",
    "\n",
    "                torch.save(model.state_dict(), os.path.join(save_dir, fname_model))\n",
    "                #torch.save(guide.state_dict(), os.path.join(save_dir, fname_guide))\n",
    "                pyro.get_param_store().save(os.path.join(save_dir, fname_ps))\n",
    "                print(f\"  ↳ Saved: {fname_model}, {fname_guide}, {fname_ps}\")\n",
    "\n",
    "    # save losses per epoch in a csv file, with consistent file naming\n",
    "    accuracies_df = pd.DataFrame({\n",
    "        'epoch': accuracy_epochs,\n",
    "        'accuracy': epoch_accuracies\n",
    "    })\n",
    "    accuracies_df.to_csv(os.path.join(save_dir,accuracies_filename_pattern.format(activation=act_name, prior=prior_name, timestamp=timestamp)), index=False)\n",
    "\n",
    "    loss_df = pd.DataFrame({\n",
    "        'epoch': list(range(1, epoch + 1)),\n",
    "        'loss': epoch_losses\n",
    "    })\n",
    "    loss_df.to_csv(os.path.join(save_dir,losses_filename_pattern.format(activation=act_name, prior=prior_name, timestamp=timestamp)), index=False)\n",
    "            \n",
    "    # save model configuration in a json file\n",
    "    config = {\n",
    "        'activation': act_name,\n",
    "        'prior': prior_name,\n",
    "        'num_epochs': num_epochs,\n",
    "        'best_accuracy_at_epoch': accuracy_epochs[np.argmax(epoch_accuracies)],\n",
    "        'best_accuracy': best_acc,\n",
    "        'batch_size': train_loader.batch_size,\n",
    "        'train_size': len(train_loader.dataset),\n",
    "        'prior_params': {\n",
    "            'mu': model.prior_mu.item(),\n",
    "            'b': model.prior_b.item()\n",
    "        },\n",
    "    }\n",
    "    config_filename = model_config_filename_pattern.format(activation=act_name, prior=prior_name, timestamp=timestamp)\n",
    "\n",
    "    with open(os.path.join(save_dir, config_filename), 'w') as f:\n",
    "        import json\n",
    "        json.dump(config, f, indent=4)\n",
    "        print(f\"Configuration saved to {config_filename}\")\n",
    "\n",
    "    return epoch_losses, epoch_accuracies, accuracy_epochs, weight_stats, bias_stats, os.path.join(save_dir, fname_model), os.path.join(save_dir, fname_guide), os.path.join(save_dir, fname_ps), timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bbab083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "from pyro.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ed012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using prior distribution: gaussian with mu=0.0 and b=1\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "bayesian_model = BayesianCNNSingleFC(num_classes,\n",
    "        device,\n",
    "        activation='relu',\n",
    "        prior_dist='gaussian',\n",
    "        prior_params={'mu': 0.0, 'b': 10.0})\n",
    "\n",
    "#act_name  = getattr(bayesian_model, 'activation', getattr(bayesian_model, 'activation', 'act'))\n",
    "#prior_name = getattr(bayesian_model, 'prior_dist', 'prior')\n",
    "\n",
    "#guide = AutoDiagonalNormal(bayesian_model)\n",
    "\n",
    "optimizer = Adam({\"lr\": 1e-3})  # Increased from 1e-4 to 1e-3\n",
    "svi = pyro.infer.SVI(model=bayesian_model,\n",
    "                     guide=guide,\n",
    "                     optim=optimizer,\n",
    "                     loss=pyro.infer.Trace_ELBO(num_particles=1,\n",
    "                                                )) #TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b5b1763",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "# Ensure model and guide are on the correct device\n",
    "bayesian_model.to(device)\n",
    "#guide.to(device)\n",
    "\n",
    "train_loader, test_loader = load_data(batch_size=54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d578695b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/360 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 360/360 [00:08<00:00, 40.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - ELBO Loss: 126417.0818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc check epoch 1: 100%|██████████| 360/360 [00:04<00:00, 82.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Acc: 62.63%\n",
      "  ↳ Saved: model_relu_gaussian_epoch_best_20250714_181853.pth, guide_relu_gaussian_epoch_best_20250714_181853.pth, param_store_relu_gaussian_epoch_best_20250714_181853.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 360/360 [00:08<00:00, 41.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - ELBO Loss: 107729.7695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 360/360 [00:08<00:00, 42.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - ELBO Loss: 89683.3380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 360/360 [00:08<00:00, 41.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - ELBO Loss: 72428.2660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 360/360 [00:08<00:00, 41.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - ELBO Loss: 56381.6261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 360/360 [00:08<00:00, 41.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - ELBO Loss: 42017.6050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 360/360 [00:08<00:00, 41.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - ELBO Loss: 29938.2612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 360/360 [00:08<00:00, 41.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - ELBO Loss: 20575.4683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 360/360 [00:08<00:00, 41.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - ELBO Loss: 13937.2326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 360/360 [00:08<00:00, 42.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - ELBO Loss: 9635.0922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc check epoch 10: 100%|██████████| 360/360 [00:04<00:00, 82.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train Acc: 13.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 360/360 [00:08<00:00, 41.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - ELBO Loss: 6947.3098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 360/360 [00:08<00:00, 41.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - ELBO Loss: 5265.9073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 360/360 [00:08<00:00, 41.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - ELBO Loss: 4193.3404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 360/360 [00:08<00:00, 41.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - ELBO Loss: 3516.0140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 360/360 [00:08<00:00, 41.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - ELBO Loss: 3041.4008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 360/360 [00:08<00:00, 42.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - ELBO Loss: 2692.3384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 360/360 [00:08<00:00, 42.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - ELBO Loss: 2484.7237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 360/360 [00:08<00:00, 41.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - ELBO Loss: 2300.8506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 360/360 [00:08<00:00, 41.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - ELBO Loss: 2159.9345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████| 360/360 [00:08<00:00, 41.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - ELBO Loss: 2045.9729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc check epoch 20: 100%|██████████| 360/360 [00:04<00:00, 83.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Train Acc: 13.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 360/360 [00:08<00:00, 42.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - ELBO Loss: 2019.7566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 360/360 [00:08<00:00, 42.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - ELBO Loss: 1923.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|██████████| 360/360 [00:08<00:00, 41.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - ELBO Loss: 1885.1080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|██████████| 360/360 [00:08<00:00, 42.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 - ELBO Loss: 1846.9597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|██████████| 360/360 [00:08<00:00, 41.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 - ELBO Loss: 1795.3861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|██████████| 360/360 [00:08<00:00, 42.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 - ELBO Loss: 1769.7429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|██████████| 360/360 [00:08<00:00, 41.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 - ELBO Loss: 1729.6698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|██████████| 360/360 [00:08<00:00, 42.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 - ELBO Loss: 1708.6824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|██████████| 360/360 [00:08<00:00, 42.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 - ELBO Loss: 1678.1144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|██████████| 360/360 [00:08<00:00, 41.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - ELBO Loss: 1692.9943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Acc check epoch 30: 100%|██████████| 360/360 [00:04<00:00, 83.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 - Train Acc: 14.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100:  94%|█████████▎| 337/360 [00:08<00:00, 40.35it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m losses, accuracies, accuracy_epochs, weight_stats, bias_stats, best_model_path, best_guide_path, best_param_store_path, experiment_timestamp = \u001b[43mtrain_svi_with_stats\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbayesian_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguide\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43msvi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mresults_GP_eurosat\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mtrain_svi_with_stats\u001b[39m\u001b[34m(model, guide, svi, train_loader, device, num_epochs, save_epochs, save_dir, model_filename_pattern, guide_filename_pattern, param_store_filename_pattern, accuracies_filename_pattern, losses_filename_pattern, model_config_filename_pattern)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m     54\u001b[39m     images, labels = images.to(device), labels.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     total_loss += \u001b[43msvi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m     batches += \u001b[32m1\u001b[39m\n\u001b[32m     58\u001b[39m avg_loss = total_loss / batches\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/bnntest/lib/python3.11/site-packages/pyro/infer/svi.py:153\u001b[39m, in \u001b[36mSVI.step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    147\u001b[39m params = \u001b[38;5;28mset\u001b[39m(\n\u001b[32m    148\u001b[39m     site[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m].unconstrained() \u001b[38;5;28;01mfor\u001b[39;00m site \u001b[38;5;129;01min\u001b[39;00m param_capture.trace.nodes.values()\n\u001b[32m    149\u001b[39m )\n\u001b[32m    151\u001b[39m \u001b[38;5;66;03m# actually perform gradient steps\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# zero gradients\u001b[39;00m\n\u001b[32m    156\u001b[39m pyro.infer.util.zero_grads(params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/bnntest/lib/python3.11/site-packages/pyro/optim/optim.py:155\u001b[39m, in \u001b[36mPyroOptim.__call__\u001b[39m\u001b[34m(self, params, *args, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_objs[p].optimizer.step(*args, **kwargs)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptim_objs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/bnntest/lib/python3.11/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/bnntest/lib/python3.11/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/bnntest/lib/python3.11/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/bnntest/lib/python3.11/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/bnntest/lib/python3.11/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/bnntest/lib/python3.11/site-packages/torch/optim/adam.py:652\u001b[39m, in \u001b[36m_multi_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;66;03m# Update steps\u001b[39;00m\n\u001b[32m    647\u001b[39m \u001b[38;5;66;03m# If steps are on CPU, foreach will fall back to the slow path, which is a for-loop calling t.add(1) over\u001b[39;00m\n\u001b[32m    648\u001b[39m \u001b[38;5;66;03m# and over. 1 will then be wrapped into a Tensor over and over again, which is slower than if we just\u001b[39;00m\n\u001b[32m    649\u001b[39m \u001b[38;5;66;03m# wrapped it once now. The alpha is required to assure we go to the right overload.\u001b[39;00m\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.compiler.is_compiling() \u001b[38;5;129;01mand\u001b[39;00m device_state_steps[\u001b[32m0\u001b[39m].is_cpu:\n\u001b[32m    651\u001b[39m     torch._foreach_add_(\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m         device_state_steps, \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, alpha=\u001b[32m1.0\u001b[39m\n\u001b[32m    653\u001b[39m     )\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    655\u001b[39m     torch._foreach_add_(device_state_steps, \u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "losses, accuracies, accuracy_epochs, weight_stats, bias_stats, best_model_path, best_guide_path, best_param_store_path, experiment_timestamp = train_svi_with_stats(\n",
    "    bayesian_model,\n",
    "    guide,\n",
    "    svi,\n",
    "    train_loader,\n",
    "    device,\n",
    "    num_epochs=100,\n",
    "    save_epochs=None,\n",
    "    save_dir='results_GP_eurosat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93951306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results_with_stats(losses, accuracies, accuracy_epochs, weight_stats, bias_stats):\n",
    "    \"\"\"Plot training results with weight and bias statistics\"\"\"\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Training Loss\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, len(losses) + 1), losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('ELBO Loss')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot 2: Training Accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(accuracy_epochs, accuracies, 'o-')\n",
    "    plt.title('Training Accuracy (Every 10 Epochs)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot 3: Weight Statistics Boxplot\n",
    "    plt.subplot(2, 2, 3)\n",
    "    weight_data = []\n",
    "    weight_labels = []\n",
    "    \n",
    "    for i, epoch in enumerate(weight_stats['epochs']):\n",
    "        # Combine means and stds for this epoch\n",
    "        epoch_data = weight_stats['means'][i] + weight_stats['stds'][i]\n",
    "        weight_data.append(epoch_data)\n",
    "        weight_labels.append(f'Epoch {epoch}')\n",
    "    \n",
    "    if weight_data:\n",
    "        bp1 = plt.boxplot(weight_data, labels=weight_labels, patch_artist=True)\n",
    "        for patch in bp1['boxes']:\n",
    "            patch.set_facecolor('lightblue')\n",
    "    \n",
    "    plt.title('LOC Statistics Distribution')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('LOC Values')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Bias Statistics Boxplot\n",
    "    plt.subplot(2, 2, 4)\n",
    "    bias_data = []\n",
    "    bias_labels = []\n",
    "    \n",
    "    for i, epoch in enumerate(bias_stats['epochs']):\n",
    "        # Combine means and stds for this epoch\n",
    "        epoch_data = bias_stats['means'][i] + bias_stats['stds'][i]\n",
    "        bias_data.append(epoch_data)\n",
    "        bias_labels.append(f'Epoch {epoch}')\n",
    "    \n",
    "    if bias_data:\n",
    "        bp2 = plt.boxplot(bias_data, tick_labels=bias_labels, patch_artist=True)\n",
    "        for patch in bp2['boxes']:\n",
    "            patch.set_facecolor('lightcoral')\n",
    "    \n",
    "    plt.title('SCALE Statistics Distribution')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('SCALE Values')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b11ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_results_with_stats(losses, accuracies, accuracy_epochs, weight_stats, bias_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14131c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the parameter store and reload the parameter store from the best result\n",
    "pyro.clear_param_store()\n",
    "\n",
    "bayesian_model.load_state_dict(torch.load(best_model_path))\n",
    "guide.load_state_dict(torch.load(best_guide_path))\n",
    "pyro.get_param_store().set_state(torch.load(best_param_store_path,weights_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4cc82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print confusion matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def predict_data(model, loader_of_interest, num_samples=10):\n",
    "    model.eval()\n",
    "    #guide.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader_of_interest, desc=\"Evaluating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            logits_mc = torch.zeros(num_samples, images.size(0), model.fc1.out_features).to(device)\n",
    "\n",
    "            for i in range(num_samples):\n",
    "                guide_trace = pyro.poutine.trace(guide).get_trace(images)\n",
    "                replayed_model = pyro.poutine.replay(model, trace=guide_trace)\n",
    "                logits = replayed_model(images)\n",
    "                logits_mc[i] = logits\n",
    "\n",
    "            avg_logits = logits_mc.mean(dim=0)\n",
    "            predictions = torch.argmax(avg_logits, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e9843",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels, all_predictions = predict_data(bayesian_model, test_loader, num_samples=10)\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "#print accuracy from confusion matrix\n",
    "accuracy = np.trace(cm) / np.sum(cm)\n",
    "print(f\"Accuracy from confusion matrix: {accuracy * 100:.6f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955f506",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'True Label': all_labels, 'Predicted Label': all_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ebfcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the prediction label and true label in a csv file\n",
    "\n",
    "def save_predictions_to_csv(labels, predictions, filename='predictions.csv'):\n",
    "    df = pd.DataFrame({'True Label': labels, 'Predicted Label': predictions})\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Predictions saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67691b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_name = bayesian_model.activation_fn.__name__ if hasattr(bayesian_model.activation_fn, '__name__') else str(bayesian_model.activation_fn)\n",
    "prior_name = getattr(bayesian_model, 'prior_dist', 'prior')\n",
    "\n",
    "save_predictions_to_csv(all_labels, all_predictions, os.path.join('results_GP_eurosat', f'predictions_{act_name}_{prior_name}_{experiment_timestamp}_{accuracy * 100:.0f}.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0fddbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnntest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
